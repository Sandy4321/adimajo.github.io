{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hnbYXwoANPop"
   },
   "outputs": [],
   "source": [
    "!kill -9 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1213
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31455,
     "status": "ok",
     "timestamp": 1547118126819,
     "user": {
      "displayName": "Adrien Ehrhardt",
      "photoUrl": "https://lh6.googleusercontent.com/-YaHJkcitxbk/AAAAAAAAAAI/AAAAAAAAAC8/lVJxYfcygtE/s64/photo.jpg",
      "userId": "06833975693225147439"
     },
     "user_tz": -60
    },
    "id": "KlR3j8fk5IxP",
    "outputId": "240f1e4e-9e84-4a5a-e4c0-f22f29cfdcb5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import sklearn.metrics\n",
    "import scipy as sp\n",
    "#import google.colab\n",
    "#from google.colab import drive\n",
    "import sklearn.linear_model\n",
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "from keras import backend as K\n",
    "from keras import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import LambdaCallback, Callback, ReduceLROnPlateau, TensorBoard\n",
    "import sklearn.linear_model\n",
    "#!pip install hyperas\n",
    "from hyperas.distributions import uniform, choice\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "K.set_session(sess)\n",
    "#drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NwkqCjOI5N-2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Tue Nov  6 10:06:52 2018\n",
    "\n",
    "@author: yandexdataschool\n",
    "\n",
    "Original Code found in:\n",
    "https://github.com/yandexdataschool/roc_comparison\n",
    "\n",
    "updated: Raul Sanchez-Vazquez\n",
    "\"\"\"\n",
    "\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "\n",
    "# AUC comparison adapted from\n",
    "# https://github.com/Netflix/vmaf/\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    if sample_weight is None:\n",
    "        return fastDeLong_no_weights(predictions_sorted_transposed, label_1_count)\n",
    "    else:\n",
    "        return fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight)\n",
    "\n",
    "\n",
    "def fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank_weight(positive_examples[r, :], sample_weight[:m])\n",
    "        ty[r, :] = compute_midrank_weight(negative_examples[r, :], sample_weight[m:])\n",
    "        tz[r, :] = compute_midrank_weight(predictions_sorted_transposed[r, :], sample_weight)\n",
    "    total_positive_weights = sample_weight[:m].sum()\n",
    "    total_negative_weights = sample_weight[m:].sum()\n",
    "    pair_weights = np.dot(sample_weight[:m, np.newaxis], sample_weight[np.newaxis, m:])\n",
    "    total_pair_weights = pair_weights.sum()\n",
    "    aucs = (sample_weight[:m]*(tz[:, :m] - tx)).sum(axis=1) / total_pair_weights\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / total_negative_weights\n",
    "    v10 = 1. - (tz[:, m:] - ty[:, :]) / total_positive_weights\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def fastDeLong_no_weights(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating\n",
    "              Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count, ordered_sample_weight\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count, ordered_sample_weight)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "itvCJ-Sh5Evq"
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "\n",
    "    import pandas as pd\n",
    "    column_names = [\n",
    "        'timestamp', 'cylinder', 'customer', 'job', 'grain', 'ink', 'proof',\n",
    "        'blade', 'cylinder1', 'paper', 'ink2', 'direct', 'solvent', 'type_cyl',\n",
    "        'press', 'press2', 'unit', 'cylinder2', 'paper2', 'plating', 'proof2',\n",
    "        'viscosity', 'caliper', 'ink3', 'humifity', 'roughness', 'blade2',\n",
    "        'varnish', 'press3', 'ink4', 'solvent2', 'ESA', 'ESA2', 'wax', 'hardener',\n",
    "        'roller', 'current', 'anode', 'chrome', 'band'\n",
    "    ]\n",
    "\n",
    "    bands = pd.read_csv(\n",
    "        \"~/Google Drive/Discrétisation ICLR19/opendata/bands.data\",\n",
    "        sep=\",\",\n",
    "        names=column_names,\n",
    "        na_values=[\n",
    "            '-NaN', '-nan',\n",
    "            'N/A', 'NA', 'NULL', 'NaN', 'n/a', 'nan', 'null',\n",
    "            '.', '?'\n",
    "        ])\n",
    "    \n",
    "    bands.drop([\"timestamp\"],axis=1,inplace=True)\n",
    "    \n",
    "    for col in ['cylinder',\n",
    "                'customer',\n",
    "                'grain',\n",
    "                'ink',\n",
    "                'proof',\n",
    "                'blade',\n",
    "                'cylinder1',\n",
    "                'paper',\n",
    "                'ink2',\n",
    "                'direct',\n",
    "                'solvent',\n",
    "                'type_cyl',\n",
    "                'press',\n",
    "               'cylinder2',\n",
    "               'paper2',\n",
    "               'caliper',\n",
    "               'band']:\n",
    "    \n",
    "        bands[col] = bands[col].str.upper()\n",
    "\n",
    "    bands[[\n",
    "        'cylinder', 'customer', 'grain', 'ink', 'proof', 'blade', 'cylinder1',\n",
    "        'paper', 'ink2', 'direct', 'solvent', 'type_cyl', 'press', 'cylinder2',\n",
    "        'paper2', 'caliper'\n",
    "        ]] = bands[[\n",
    "        'cylinder', 'customer', 'grain', 'ink', 'proof', 'blade', 'cylinder1',\n",
    "        'paper', 'ink2', 'direct', 'solvent', 'type_cyl', 'press', 'cylinder2',\n",
    "        'paper2', 'caliper'\n",
    "        ]].fillna(value=\"missing\")\n",
    "    \n",
    "    bands.dropna(inplace=True)\n",
    "    \n",
    "    bands.band = sklearn.preprocessing.LabelEncoder().fit_transform(bands.band)\n",
    "    \n",
    "    \n",
    "    \n",
    "    bands[[\n",
    "        'press2', 'plating', 'proof2', 'viscosity', 'caliper', 'ink3', 'humifity',\n",
    "        'roughness', 'blade2', 'varnish', 'press3', 'ink4', 'solvent2', 'ESA',\n",
    "        'ESA2', 'wax', 'hardener', 'roller', 'current', 'anode', 'chrome'\n",
    "    ]] = bands[[    'press2', 'plating', 'proof2', 'viscosity', 'caliper', 'ink3', 'humifity',\n",
    "        'roughness', 'blade2', 'varnish', 'press3', 'ink4', 'solvent2', 'ESA',\n",
    "        'ESA2', 'wax', 'hardener', 'roller', 'current', 'anode', 'chrome'\n",
    "    ]].fillna(value=0.0)\n",
    "    \n",
    "    \n",
    "    bands.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    bands_label_encoders = []\n",
    "\n",
    "    bands_encoded = bands.copy()\n",
    "\n",
    "    for j in ['cylinder', 'customer', 'grain', 'ink', 'proof', 'blade', 'cylinder1',\n",
    "    'paper', 'ink2', 'direct', 'solvent', 'type_cyl', 'press', 'cylinder2',\n",
    "    'paper2', 'caliper']:\n",
    "        temp = sk.preprocessing.LabelEncoder()\n",
    "        temp.fit(bands[j].astype(str))\n",
    "        bands_label_encoders.append(temp)\n",
    "        bands_encoded[j] = temp.transform(bands[j].astype(str))\n",
    "        \n",
    "    bands_one_hot_encoder = sk.preprocessing.OneHotEncoder(categories='auto',sparse=False,handle_unknown=\"ignore\")\n",
    "    \n",
    "    bands_one_hot_encoder.fit(bands_encoded[[\n",
    "    'cylinder', 'customer', 'grain', 'ink', 'proof', 'blade', 'cylinder1',\n",
    "    'paper', 'ink2', 'direct', 'solvent', 'type_cyl', 'press', 'cylinder2',\n",
    "    'paper2', 'caliper'\n",
    "    ]])\n",
    "    \n",
    "    \n",
    "    bands_one_hot_encoded = bands_encoded.copy()\n",
    "    bands_one_hot_encoded.drop(\n",
    "    ['cylinder', 'customer', 'grain', 'ink', 'proof', 'blade', 'cylinder1',\n",
    "    'paper', 'ink2', 'direct', 'solvent', 'type_cyl', 'press', 'cylinder2',\n",
    "    'paper2', 'caliper'],\n",
    "    axis=1,\n",
    "    inplace=True)\n",
    "    \n",
    "    \n",
    "    bands_one_hot_encoded = pd.concat(\n",
    "    [\n",
    "    bands_one_hot_encoded,\n",
    "    pd.DataFrame(\n",
    "    bands_one_hot_encoder.transform(bands_encoded[[\n",
    "      'cylinder', 'customer', 'grain', 'ink', 'proof', 'blade', 'cylinder1',\n",
    "    'paper', 'ink2', 'direct', 'solvent', 'type_cyl', 'press', 'cylinder2',\n",
    "    'paper2', 'caliper'\n",
    "    ]]),\n",
    "    index=bands_one_hot_encoded.index)\n",
    "    ],\n",
    "    axis=1)\n",
    "    \n",
    "    \n",
    "    bands_features_train, bands_features_test, bands_perf_train, bands_perf_test = sk.model_selection.train_test_split(\n",
    "    bands_one_hot_encoded.drop('band', axis=1),\n",
    "    bands_one_hot_encoded.band,\n",
    "    test_size=0.33,\n",
    "    random_state=1)\n",
    "    \n",
    "    \n",
    "    bands_nn_features_train = bands_encoded.iloc[\n",
    "    bands_features_train.index, :].drop(\n",
    "    'band', axis=1)\n",
    "    \n",
    "    \n",
    "    bands_nn_features_test = bands_encoded.iloc[bands_features_test.index, :].drop(\n",
    "    'band', axis=1)\n",
    "    \n",
    "    \n",
    "    bands_nn_perf_train = bands_encoded.iloc[\n",
    "    bands_features_train.index, :].band\n",
    "    \n",
    "    bands_nn_perf_test = bands_encoded.iloc[bands_features_test.index, :].band\n",
    "\n",
    "    \n",
    "    global n\n",
    "    global n_test\n",
    "    n = bands_nn_features_train.shape[0]\n",
    "    n_test = bands_nn_features_test.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "    x_quant = bands_nn_features_train[[\n",
    "    'press2', 'plating', 'proof2', 'viscosity', 'caliper', 'ink3', 'humifity',\n",
    "    'roughness', 'blade2', 'varnish', 'press3', 'ink4', 'solvent2', 'ESA',\n",
    "    'ESA2', 'wax', 'hardener', 'roller', 'current', 'anode', 'chrome'\n",
    "    ]].values\n",
    "    \n",
    "    \n",
    "    x_qual = bands_nn_features_train[['cylinder', 'customer', 'grain', 'ink', 'proof', 'blade', 'cylinder1',\n",
    "    'paper', 'ink2', 'direct', 'solvent', 'type_cyl', 'press', 'cylinder2',\n",
    "    'paper2', 'caliper']].values\n",
    "    \n",
    "    \n",
    "    y = bands_nn_perf_train.values\n",
    "\n",
    "\n",
    "    d1 = x_quant.shape[1]\n",
    "    d2 = x_qual.shape[1]\n",
    "\n",
    "\n",
    "    x_quant_test = bands_nn_features_test[[\n",
    "    'press2', 'plating', 'proof2', 'viscosity', 'caliper', 'ink3', 'humifity',\n",
    "    'roughness', 'blade2', 'varnish', 'press3', 'ink4', 'solvent2', 'ESA',\n",
    "    'ESA2', 'wax', 'hardener', 'roller', 'current', 'anode', 'chrome'\n",
    "    ]].values\n",
    "    \n",
    "    \n",
    "    x_qual_test = bands_nn_features_test[['cylinder', 'customer', 'grain', 'ink', 'proof', 'blade', 'cylinder1',\n",
    "    'paper', 'ink2', 'direct', 'solvent', 'type_cyl', 'press', 'cylinder2',\n",
    "    'paper2', 'caliper']].values\n",
    "    \n",
    "    y_test = bands_nn_perf_test.values\n",
    "\n",
    "\n",
    "    for j in range(d2):\n",
    "        liste_absents = [\n",
    "        item for item in np.unique(x_qual_test[:, j])\n",
    "        if item not in np.unique(x_qual[:, j])\n",
    "        ]\n",
    "        liste_lignes = [\n",
    "        np.where(x_qual_test[:, j] == item) for item in liste_absents\n",
    "        ]\n",
    "        x_qual_test = np.delete(\n",
    "        x_qual_test,\n",
    "        list(chain.from_iterable(list(chain.from_iterable(liste_lignes)))),\n",
    "        axis=0)\n",
    "        x_quant_test = np.delete(\n",
    "        x_quant_test,\n",
    "        list(chain.from_iterable(list(chain.from_iterable(liste_lignes)))),\n",
    "        axis=0)\n",
    "        y_test = np.delete(\n",
    "        y_test,\n",
    "        list(chain.from_iterable(list(chain.from_iterable(liste_lignes)))),\n",
    "        axis=0)\n",
    "        \n",
    "        \n",
    "    x_qual_encoded = x_qual.copy()\n",
    "    x_qual_label_encoders = []\n",
    "    \n",
    "    for j in range(d2):\n",
    "        temp = sk.preprocessing.LabelEncoder()\n",
    "        temp.fit(x_qual[:,j].astype(str))\n",
    "        x_qual_label_encoders.append(temp)\n",
    "        x_qual_encoded[:,j] = temp.transform(x_qual[:,j].astype(str))\n",
    "        \n",
    "    x_qual_one_hot_encoder = sk.preprocessing.OneHotEncoder(categories='auto',sparse=False,handle_unknown=\"ignore\")\n",
    "    \n",
    "    x_qual_one_hot_encoder.fit(x_qual_encoded)\n",
    "    \n",
    "    x_qual_dummy = x_qual_one_hot_encoder.transform(x_qual_encoded)\n",
    "    \n",
    "    x_qual = x_qual_encoded\n",
    "    \n",
    "    x_qual_test_encoded = x_qual_test.copy()\n",
    "    \n",
    "    for j in range(d2):\n",
    "        x_qual_test_encoded[:,j] = x_qual_label_encoders[j].transform(x_qual_test[:,j].astype(str))\n",
    "        \n",
    "    x_qual_test = x_qual_test_encoded\n",
    "\n",
    "    x_qual_dummy_test = x_qual_one_hot_encoder.transform(x_qual_test)\n",
    "\n",
    "\n",
    "    return x_quant,x_qual,x_qual_dummy,y,x_quant_test,x_qual_test,x_qual_dummy_test,y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_quant,x_qual,x_qual_dummy,y,x_quant_test,x_qual_test,x_qual_dummy_test,y_test = data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"x_quant.csv\", x_quant, delimiter=\",\")\n",
    "np.savetxt(\"x_qual.csv\", x_qual, delimiter=\",\")\n",
    "np.savetxt(\"y.csv\", y, delimiter=\",\")\n",
    "np.savetxt(\"x_quant_test.csv\", x_quant_test, delimiter=\",\")\n",
    "np.savetxt(\"x_qual_test.csv\", x_qual_test, delimiter=\",\")\n",
    "np.savetxt(\"y_test.csv\", y_test, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.unique(x_qual[:,0])==305)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(x_qual_test[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_qual_dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QMDJV_yP5vK6"
   },
   "outputs": [],
   "source": [
    "n = 247\n",
    "n_test = 25\n",
    "d1 = 21\n",
    "d2 = 16\n",
    "\n",
    "liste_layers_quant = [None] * d1\n",
    "liste_layers_qual = [None] * d2\n",
    "liste_qual_arrays = [None] * d2\n",
    "liste_qual_arrays_test = [None] * d2\n",
    "\n",
    "def create_model(x_quant,x_qual,x_qual_dummy,y,x_quant_test,x_qual_test,x_qual_dummy_test,y_test):    \n",
    "    def initialize_neural_net(m_quant,m_qual):\n",
    "        liste_inputs_quant = [None] * d1\n",
    "        liste_inputs_qual = [None] * d2\n",
    "\n",
    "        liste_layers_quant = [None] * d1\n",
    "        liste_layers_qual = [None] * d2\n",
    "\n",
    "        liste_layers_quant_inputs = [None] * d1\n",
    "        liste_layers_qual_inputs = [None] * d2\n",
    "\n",
    "        for i in range(d1):\n",
    "            liste_inputs_quant[i] = Input((1, ))\n",
    "            liste_layers_quant[i] = Dense(m_quant[i], activation='softmax')\n",
    "            liste_layers_quant_inputs[i] = liste_layers_quant[i](\n",
    "                liste_inputs_quant[i])\n",
    "\n",
    "        for i in range(d2):\n",
    "            liste_inputs_qual[i] = Input((len(np.unique(x_qual[:, i])), ))\n",
    "            if (len(np.unique(x_qual[:, i])) > m_qual[i]):\n",
    "                liste_layers_qual[i] = Dense(\n",
    "                m_qual[i], activation='softmax', use_bias=False)\n",
    "            else:\n",
    "                liste_layers_qual[i] = Dense(\n",
    "                len(np.unique(x_qual[:, i])), activation='softmax', use_bias=False)\n",
    "\n",
    "            liste_layers_qual_inputs[i] = liste_layers_qual[i](\n",
    "                liste_inputs_qual[i])\n",
    "\n",
    "        return ([\n",
    "            liste_inputs_quant, liste_layers_quant, liste_layers_quant_inputs,\n",
    "            liste_inputs_qual, liste_layers_qual, liste_layers_qual_inputs\n",
    "        ])\n",
    "    \n",
    "    \n",
    "    def from_layers_to_proba_training(d1,d2,liste_layers_quant,liste_layers_qual):\n",
    "\n",
    "        results = [None] * (d1 + d2)\n",
    "\n",
    "        for j in range(d1):\n",
    "            results[j] = K.function([liste_layers_quant[j].input],\n",
    "                                    [liste_layers_quant[j].output])(\n",
    "                                        [x_quant[:, j, np.newaxis]])\n",
    "\n",
    "        for j in range(d2):\n",
    "            results[j + d1] = K.function([liste_layers_qual[j].input],\n",
    "                                         [liste_layers_qual[j].output])(\n",
    "                                             [liste_qual_arrays[j]])\n",
    "\n",
    "        return (results)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def from_weights_to_proba_test(d1,d2,m_quant,m_qual,history,x_quant_test,x_qual_test,n_test):\n",
    "\n",
    "        results = [None] * (d1 + d2)    \n",
    "\n",
    "        for j in range(d1):\n",
    "            results[j] = np.zeros((n_test,m_quant[j]))\n",
    "            for i in range(m_quant[j]):\n",
    "                results[j][:,i] = history.best_weights[j][1][i] + history.best_weights[j][0][0][i]*x_quant_test[:,j]\n",
    "\n",
    "\n",
    "        for j in range(d2):\n",
    "            results[j+d1] = np.zeros((n_test,history.best_weights[j+d1][0].shape[1]))\n",
    "            for i in range(history.best_weights[j+d1][0].shape[1]):\n",
    "                for k in range(n_test):\n",
    "                    results[j+d1][k,i] = history.best_weights[j+d1][0][x_qual_test[k,j],i]\n",
    "\n",
    "        return(results)\n",
    "    \n",
    "    \n",
    "    def evaluate_disc(type,d1,d2,misc):\n",
    "        if type==\"train\":\n",
    "            proba = from_layers_to_proba_training(d1,d2,misc[0],misc[1])\n",
    "        else:\n",
    "            proba = from_weights_to_proba_test(d1,d2,misc[0],misc[1],misc[2],misc[3],misc[4],misc[5])\n",
    "\n",
    "\n",
    "        results = [None] * (d1 + d2)\n",
    "\n",
    "        if type==\"train\":\n",
    "            X_transformed = np.ones((n, 1))\n",
    "        else:\n",
    "            X_transformed = np.ones((n_test, 1))\n",
    "\n",
    "        for j in range(d1 + d2):\n",
    "            if type==\"train\":\n",
    "                results[j] = np.argmax(proba[j][0], axis=1)\n",
    "            else:\n",
    "                results[j] = np.argmax(proba[j], axis=1)\n",
    "            X_transformed = np.concatenate(\n",
    "                (X_transformed, sk.preprocessing.OneHotEncoder(categories='auto',sparse=False,handle_unknown=\"ignore\").fit_transform(\n",
    "                    X=results[j].reshape(-1, 1))),\n",
    "                axis=1)\n",
    "\n",
    "        proposed_logistic_regression = sk.linear_model.LogisticRegression(\n",
    "            fit_intercept=False, solver = \"lbfgs\", C=1e20, tol=1e-8, max_iter=50)\n",
    "\n",
    "\n",
    "        if type==\"train\":\n",
    "            proposed_logistic_regression.fit(X=X_transformed, y=y.reshape((n, )))\n",
    "            performance = 2 * sk.metrics.log_loss(\n",
    "              y,\n",
    "              proposed_logistic_regression.predict_proba(X=X_transformed)[:, 1],\n",
    "              normalize=False\n",
    "          ) + proposed_logistic_regression.coef_.shape[1] * np.log(n)\n",
    "            predicted = proposed_logistic_regression.predict_proba(X_transformed)[:,1]\n",
    "\n",
    "        else:\n",
    "            proposed_logistic_regression.fit(X=X_transformed, y=y_test.reshape((n_test, )))\n",
    "            performance = 2*sk.metrics.roc_auc_score(y_test,proposed_logistic_regression.predict_proba(X_transformed)[:,1])-1\n",
    "            predicted = proposed_logistic_regression.predict_proba(X_transformed)[:,1]\n",
    "\n",
    "        return (performance, predicted)\n",
    "\n",
    "\n",
    "    \n",
    "    class LossHistory(Callback):\n",
    "        def on_train_begin(self, logs={}):\n",
    "            self.losses = []\n",
    "            self.best_criterion = float(\"inf\")\n",
    "            self.best_outputs = []\n",
    "\n",
    "        def on_epoch_end(self, batch, logs={}):\n",
    "            self.losses.append(evaluate_disc(\"train\",d1,d2,[liste_layers_quant,liste_layers_qual])[0])\n",
    "            if self.losses[-1] < self.best_criterion:\n",
    "                self.best_weights = []\n",
    "                self.best_outputs = []\n",
    "                self.best_criterion = self.losses[-1]\n",
    "                for j in range(d1):\n",
    "                    self.best_weights.append(liste_layers_quant[j].get_weights())\n",
    "                    self.best_outputs.append(\n",
    "                        K.function([liste_layers_quant[j].input],\n",
    "                                   [liste_layers_quant[j].output])(\n",
    "                                       [x_quant[:, j, np.newaxis]]))\n",
    "                for j in range(d2):\n",
    "                    self.best_weights.append(liste_layers_qual[j].get_weights())\n",
    "                    self.best_outputs.append(\n",
    "                        K.function([liste_layers_qual[j].input],\n",
    "                                   [liste_layers_qual[j].output])(\n",
    "                                       [liste_qual_arrays[j]]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    essai = {{uniform(4,9)}}\n",
    "\n",
    "    m_quant = [int(essai)] * d1\n",
    "    m_qual = [int(essai)] * d2\n",
    "\n",
    "    global liste_qual_arrays\n",
    "    liste_qual_arrays = [None] * d2\n",
    "    cursor = 0\n",
    "    for j in range(d2):\n",
    "        liste_qual_arrays[j] = x_qual_dummy[:, cursor:(\n",
    "            cursor + len(np.unique(x_qual[:, j])))]\n",
    "        cursor += len(np.unique(x_qual[:, j]))\n",
    "\n",
    "    global liste_qual_arrays_test\n",
    "    liste_qual_arrays_test = [None] * d2\n",
    "    cursor = 0\n",
    "    for j in range(d2):\n",
    "        liste_qual_arrays_test[j] = x_qual_dummy_test[:, cursor:(\n",
    "            cursor + len(np.unique(x_qual_test[:, j])))]\n",
    "        cursor += len(np.unique(x_qual_test[:, j]))\n",
    "  \n",
    "  \n",
    "    liste_inputs_quant = [None] * d1\n",
    "    liste_inputs_qual = [None] * d2\n",
    "\n",
    "    global liste_layers_quant\n",
    "    global liste_layers_qual\n",
    "\n",
    "    liste_layers_quant = [None] * d1\n",
    "    liste_layers_qual = [None] * d2\n",
    "\n",
    "    liste_layers_quant_inputs = [None] * d1\n",
    "    liste_layers_qual_inputs = [None] * d2\n",
    "\n",
    "    for i in range(d1):\n",
    "        liste_inputs_quant[i] = Input((1, ))\n",
    "        liste_layers_quant[i] = Dense(m_quant[i], activation='softmax')\n",
    "        liste_layers_quant_inputs[i] = liste_layers_quant[i](\n",
    "          liste_inputs_quant[i])\n",
    "\n",
    "    for i in range(d2):\n",
    "        liste_inputs_qual[i] = Input((len(np.unique(x_qual[:, i])), ))\n",
    "        if (len(np.unique(x_qual[:, i])) > m_qual[i]):\n",
    "            liste_layers_qual[i] = Dense(\n",
    "            m_qual[i], activation='softmax', use_bias=False)\n",
    "        else:\n",
    "            liste_layers_qual[i] = Dense(\n",
    "            len(np.unique(x_qual[:, i])), activation='softmax', use_bias=False)\n",
    "\n",
    "        liste_layers_qual_inputs[i] = liste_layers_qual[i](\n",
    "        liste_inputs_qual[i])\n",
    "\n",
    "\n",
    "    full_hidden = concatenate(\n",
    "        list(\n",
    "          chain.from_iterable(\n",
    "              [liste_layers_quant_inputs, liste_layers_qual_inputs])))\n",
    "    output = Dense(1, activation='sigmoid')(full_hidden)\n",
    "    model = Model(\n",
    "      inputs=list(chain.from_iterable([liste_inputs_quant, liste_inputs_qual])),\n",
    "      outputs=[output])  \n",
    "\n",
    "    \n",
    "    adam = optimizers.Adam(lr={{choice([10**-3, 10**-2, 10**-1])}})\n",
    "    rmsprop = optimizers.RMSprop(lr={{choice([10**-3, 10**-2, 10**-1])}})\n",
    "    sgd = optimizers.SGD(lr={{choice([10**-3, 10**-2, 10**-1])}})\n",
    "   \n",
    "    choiceval = {{choice(['adam', 'sgd', 'rmsprop'])}}\n",
    "    if choiceval == 'adam':\n",
    "        optim = adam\n",
    "    elif choiceval == 'rmsprop':\n",
    "        optim = rmsprop\n",
    "    else:\n",
    "        optim = sgd\n",
    "        \n",
    " \n",
    "    model.compile(loss='binary_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
    "\n",
    "    history = LossHistory()\n",
    "\n",
    "    callbacks = [\n",
    "      ReduceLROnPlateau(\n",
    "          monitor='loss',\n",
    "          factor=0.5,\n",
    "          patience=10,\n",
    "          verbose=0,\n",
    "          mode='auto',\n",
    "          min_delta=0.0001,\n",
    "          cooldown=0,\n",
    "          min_lr=0), history\n",
    "    ]\n",
    "    model.fit(\n",
    "      list(chain.from_iterable([list(x_quant.T), liste_qual_arrays])),\n",
    "      y,\n",
    "      epochs=300,\n",
    "      batch_size={{choice([32,64,128])}},\n",
    "      verbose=1,\n",
    "      callbacks=callbacks)\n",
    "\n",
    "    global n_test\n",
    "    n_test = x_quant_test.shape[0]\n",
    "    performance, predicted = evaluate_disc(\"test\",d1,d2,misc=[m_quant,m_qual,history,x_quant_test,x_qual_test,n_test])\n",
    "  \n",
    "    return {'loss': -performance, 'status': STATUS_OK, 'model': model, 'predicted': predicted}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83298
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35775,
     "status": "error",
     "timestamp": 1547113784913,
     "user": {
      "displayName": "Adrien Ehrhardt",
      "photoUrl": "https://lh6.googleusercontent.com/-YaHJkcitxbk/AAAAAAAAAAI/AAAAAAAAAC8/lVJxYfcygtE/s64/photo.jpg",
      "userId": "06833975693225147439"
     },
     "user_tz": -60
    },
    "id": "EfS_gTcl6XRi",
    "outputId": "6fb9eca8-d0b5-42fe-9418-206b9aa0e198"
   },
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "\n",
    "best_run, best_model, space = optim.minimize(model=create_model,\n",
    "                          data=data,\n",
    "                          algo=tpe.suggest,\n",
    "                          max_evals=20,\n",
    "                          trials=trials,\n",
    "                          notebook_name='hyperas - bands - dump - ICML19',\n",
    "                          eval_space=True,\n",
    "                          return_space=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68491,
     "status": "ok",
     "timestamp": 1547116449921,
     "user": {
      "displayName": "Adrien Ehrhardt",
      "photoUrl": "https://lh6.googleusercontent.com/-YaHJkcitxbk/AAAAAAAAAAI/AAAAAAAAAC8/lVJxYfcygtE/s64/photo.jpg",
      "userId": "06833975693225147439"
     },
     "user_tz": -60
    },
    "id": "6a5MOLFb7k0w",
    "outputId": "dfbacf94-7687-4a05-d329-342c3c30432e"
   },
   "outputs": [],
   "source": [
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials.best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68331,
     "status": "ok",
     "timestamp": 1547116449923,
     "user": {
      "displayName": "Adrien Ehrhardt",
      "photoUrl": "https://lh6.googleusercontent.com/-YaHJkcitxbk/AAAAAAAAAAI/AAAAAAAAAC8/lVJxYfcygtE/s64/photo.jpg",
      "userId": "06833975693225147439"
     },
     "user_tz": -60
    },
    "id": "a_ORL2LmNtzP",
    "outputId": "bb140b90-c313-47d6-d18b-19dc962a7ffc"
   },
   "outputs": [],
   "source": [
    "trials.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1547116471755,
     "user": {
      "displayName": "Adrien Ehrhardt",
      "photoUrl": "https://lh6.googleusercontent.com/-YaHJkcitxbk/AAAAAAAAAAI/AAAAAAAAAC8/lVJxYfcygtE/s64/photo.jpg",
      "userId": "06833975693225147439"
     },
     "user_tz": -60
    },
    "id": "-b6tcIR-NuzV",
    "outputId": "c34e4075-26de-4087-a951-6c83a2b5d1df"
   },
   "outputs": [],
   "source": [
    "trials.best_trial['result']['predicted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1850,
     "status": "ok",
     "timestamp": 1547116584109,
     "user": {
      "displayName": "Adrien Ehrhardt",
      "photoUrl": "https://lh6.googleusercontent.com/-YaHJkcitxbk/AAAAAAAAAAI/AAAAAAAAAC8/lVJxYfcygtE/s64/photo.jpg",
      "userId": "06833975693225147439"
     },
     "user_tz": -60
    },
    "id": "CcXOpe9gOwa9",
    "outputId": "c18b1950-b0f3-488f-c90b-94c328ff90b1"
   },
   "outputs": [],
   "source": [
    "x_quant,x_qual,x_qual_dummy,y,x_quant_test,x_qual_test,x_qual_dummy_test,y_test=data()\n",
    "\n",
    "\n",
    "alpha = .95\n",
    "y_pred = trials.best_trial['result']['predicted']\n",
    "y_true = y_test\n",
    "\n",
    "auc, auc_cov = delong_roc_variance(\n",
    "    y_true,\n",
    "    y_pred)\n",
    "\n",
    "auc_std = np.sqrt(auc_cov)\n",
    "lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "\n",
    "ci = stats.norm.ppf(\n",
    "    lower_upper_q,\n",
    "    loc=auc,\n",
    "    scale=auc_std)\n",
    "\n",
    "ci[ci > 1] = 1\n",
    "\n",
    "print('Gini:', 2*auc-1)\n",
    "print('AUC COV:', auc_cov)\n",
    "print('95% Gini CI:', 2*ci-1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hyperas - adult - dump - ICML19.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
