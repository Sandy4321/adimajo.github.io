{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YG2rOyfcuS51"
   },
   "source": [
    "# Loading real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bKQFa3L9uS52"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import sklearn.metrics\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Tue Nov  6 10:06:52 2018\n",
    "\n",
    "@author: yandexdataschool\n",
    "\n",
    "Original Code found in:\n",
    "https://github.com/yandexdataschool/roc_comparison\n",
    "\n",
    "updated: Raul Sanchez-Vazquez\n",
    "\"\"\"\n",
    "\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "\n",
    "# AUC comparison adapted from\n",
    "# https://github.com/Netflix/vmaf/\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    if sample_weight is None:\n",
    "        return fastDeLong_no_weights(predictions_sorted_transposed, label_1_count)\n",
    "    else:\n",
    "        return fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight)\n",
    "\n",
    "\n",
    "def fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank_weight(positive_examples[r, :], sample_weight[:m])\n",
    "        ty[r, :] = compute_midrank_weight(negative_examples[r, :], sample_weight[m:])\n",
    "        tz[r, :] = compute_midrank_weight(predictions_sorted_transposed[r, :], sample_weight)\n",
    "    total_positive_weights = sample_weight[:m].sum()\n",
    "    total_negative_weights = sample_weight[m:].sum()\n",
    "    pair_weights = np.dot(sample_weight[:m, np.newaxis], sample_weight[np.newaxis, m:])\n",
    "    total_pair_weights = pair_weights.sum()\n",
    "    aucs = (sample_weight[:m]*(tz[:, :m] - tx)).sum(axis=1) / total_pair_weights\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / total_negative_weights\n",
    "    v10 = 1. - (tz[:, m:] - ty[:, :]) / total_positive_weights\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def fastDeLong_no_weights(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating\n",
    "              Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count, ordered_sample_weight\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count, ordered_sample_weight)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32072,
     "status": "ok",
     "timestamp": 1538123582048,
     "user": {
      "displayName": "Adrien Ehrhardt",
      "photoUrl": "",
      "userId": "06833975693225147439"
     },
     "user_tz": -120
    },
    "id": "Vb0y8P0KvDDC",
    "outputId": "3fb7c753-0ea0-4963-d91a-75b524fa9b52"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#german = pd.read_csv(\"drive/My Drive/Discrétisation ICLR19/data_iclr19/german.csv\",sep=\";\",na_values=['#N/A', '#N/A', 'N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan', '1.#IND', '1.#QNAN', 'N/A', 'NA', 'NULL', 'NaN', 'n/a', 'nan', 'null','.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VxF5gphPuS55"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:32: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
     ]
    }
   ],
   "source": [
    "column_names = ['A1',\n",
    "                'A2',\n",
    "                'A3',\n",
    "                'A4',\n",
    "                'A5',\n",
    "                'A6',\n",
    "                'A7',\n",
    "                'A8',\n",
    "                'A9',\n",
    "                'A10',\n",
    "                'A11',\n",
    "                'A12',\n",
    "                'A13',\n",
    "                'A14',\n",
    "                'A15',\n",
    "               'A16',\n",
    "               'A17',\n",
    "               'A18',\n",
    "               'A19',\n",
    "               'A20',\n",
    "               'target']\n",
    "\n",
    "german = pd.read_csv(\n",
    "    \"~/Google Drive/Discrétisation ICLR19/opendata/german.data\",\n",
    "    sep=\"\\s\",\n",
    "    names = column_names,\n",
    "    header= None,\n",
    "    index_col = False,\n",
    "    na_values=[\n",
    "        '#N/A', '#N/A', 'N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
    "        '1.#IND', '1.#QNAN', 'N/A', 'NA', 'NULL', 'NaN', 'n/a', 'nan', 'null',\n",
    "        '.'\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "german.target = german.target-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGeDXBovFNpb"
   },
   "outputs": [],
   "source": [
    "german.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ey_WW8EkFNpx"
   },
   "outputs": [],
   "source": [
    "german.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8unEBe--uS59"
   },
   "source": [
    "# Establishing 1st benchmark: naïve logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ZZidiG7uS6N"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5CST_74HuS6b"
   },
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "urgkKgbK3-xk"
   },
   "outputs": [],
   "source": [
    "german_label_encoders = []\n",
    "\n",
    "german_encoded = german.copy()\n",
    "\n",
    "for j in [\n",
    "        'A1', 'A3', 'A4', 'A6', 'A7', 'A9', 'A10', 'A12', 'A14', 'A15', 'A17', 'A19', 'A20'\n",
    "]:\n",
    "    temp = sk.preprocessing.LabelEncoder()\n",
    "    temp.fit(german[j].astype(str))\n",
    "    german_label_encoders.append(temp)\n",
    "    german_encoded[j] = temp.transform(german[j].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLK8k1FukVRy"
   },
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kp6MivXdk7ZK"
   },
   "outputs": [],
   "source": [
    "german_one_hot_encoder = sk.preprocessing.OneHotEncoder(categories='auto',sparse=False,handle_unknown=\"ignore\")\n",
    "german_one_hot_encoder.fit(german_encoded[[\n",
    "        'A1', 'A3', 'A4', 'A6', 'A7', 'A9', 'A10', 'A12', 'A14', 'A15', 'A17', 'A19', 'A20'\n",
    "]])\n",
    "german_one_hot_encoded = german_encoded.copy()\n",
    "german_one_hot_encoded.drop(\n",
    "        ['A1', 'A3', 'A4', 'A6', 'A7', 'A9', 'A10', 'A12', 'A14', 'A15', 'A17', 'A19', 'A20'],\n",
    "    axis=1,\n",
    "    inplace=True)\n",
    "german_one_hot_encoded = pd.concat(\n",
    "    [\n",
    "        german_one_hot_encoded,\n",
    "        pd.DataFrame(\n",
    "            german_one_hot_encoder.transform(german_encoded[[\n",
    "        'A1', 'A3', 'A4', 'A6', 'A7', 'A9', 'A10', 'A12', 'A14', 'A15', 'A17', 'A19', 'A20'\n",
    "            ]]),\n",
    "            index=german_one_hot_encoded.index)\n",
    "    ],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RbtYe1QcuS6O"
   },
   "source": [
    "### Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpyvBQ5yyguG"
   },
   "outputs": [],
   "source": [
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dZG3E0cduS6P"
   },
   "outputs": [],
   "source": [
    "german_features_train, german_features_test, german_perf_train, german_perf_test = sk.model_selection.train_test_split(\n",
    "    german_one_hot_encoded.drop('target', axis=1),\n",
    "    german_one_hot_encoded.target,\n",
    "    test_size=0.33,\n",
    "    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bmQ_Qyc7FNtt"
   },
   "outputs": [],
   "source": [
    "german_nn_features_train = german_encoded.iloc[\n",
    "    german_features_train.index, :].drop(\n",
    "        'target', axis=1)\n",
    "german_nn_features_test = german_encoded.iloc[german_features_test.index, :].drop(\n",
    "    'target', axis=1)\n",
    "german_nn_perf_train = german_encoded.iloc[\n",
    "    german_features_train.index, :].target\n",
    "german_nn_perf_test = german_encoded.iloc[german_features_test.index, :].target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V25o2mnLuS6m"
   },
   "source": [
    "### LR on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gSInu6RZu1In"
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4033,
     "status": "ok",
     "timestamp": 1538123633392,
     "user": {
      "displayName": "Adrien Ehrhardt",
      "photoUrl": "",
      "userId": "06833975693225147439"
     },
     "user_tz": -120
    },
    "id": "Qx139VZfuS6n",
    "outputId": "35cec94c-273d-4a43-e214-3e4cd6527703"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1e+20, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=1e-08, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_naive_LR = sk.linear_model.LogisticRegression(C=1e20, tol=1e-8, solver=\"newton-cg\")\n",
    "german_naive_LR.fit(\n",
    "    german_features_train[german_features_train.isna().sum(axis=1) == 0],\n",
    "    german_perf_train[german_features_train.isna().sum(axis=1) == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pMPUPbOiWwBC"
   },
   "source": [
    "### Application of learnt LR on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1548,
     "status": "ok",
     "timestamp": 1538123643056,
     "user": {
      "displayName": "Adrien Ehrhardt",
      "photoUrl": "",
      "userId": "06833975693225147439"
     },
     "user_tz": -120
    },
    "id": "UxknKgYpWu8f",
    "outputId": "3423083f-beec-4e25-8e41-2ff381345833"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5210997442455243"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * sk.metrics.roc_auc_score(\n",
    "    german_perf_test,\n",
    "    german_naive_LR.predict_proba(germaan_features_test)[:, 1]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini: 0.5210997442455245\n",
      "AUC COV: 0.0008542128443291504\n",
      "95% Gini CI: [0.40653232 0.63566716]\n"
     ]
    }
   ],
   "source": [
    "alpha = .95\n",
    "y_pred = german_naive_LR.predict_proba(german_features_test)[:, 1]\n",
    "y_true = german_perf_test\n",
    "\n",
    "auc, auc_cov = delong_roc_variance(\n",
    "    y_true,\n",
    "    y_pred)\n",
    "\n",
    "auc_std = np.sqrt(auc_cov)\n",
    "lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "\n",
    "ci = stats.norm.ppf(\n",
    "    lower_upper_q,\n",
    "    loc=auc,\n",
    "    scale=auc_std)\n",
    "\n",
    "ci[ci > 1] = 1\n",
    "\n",
    "print('Gini:', 2*auc-1)\n",
    "print('AUC COV:', auc_cov)\n",
    "print('95% Gini CI:', 2*ci-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lltm03RFFNux"
   },
   "source": [
    "# Establishing 2nd benchmark: MDLP disc + Chi2 grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KakuZgryFNuy"
   },
   "source": [
    "## MDLP disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KHT84QcIFNuz"
   },
   "outputs": [],
   "source": [
    "from mdlp.discretization import MDLP\n",
    "transformer_cont_german = MDLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1BLWiHfKFNu-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MDLP(continuous_features=None, min_depth=0, random_state=None, shuffle=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_cont_german.fit(\n",
    "    german_nn_features_train[[\n",
    "        'A2','A5','A8','A11','A13','A16','A18'\n",
    "    ]], german_nn_perf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories='auto',\n",
       "       dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
       "       n_values=None, sparse=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_german_MDLP_one_hot_encoder = sk.preprocessing.OneHotEncoder(categories='auto',sparse=False,handle_unknown=\"ignore\")\n",
    "score_german_MDLP_one_hot_encoder.fit(\n",
    "    transformer_cont_german.transform(german_nn_features_train[[\n",
    "        'A2','A5','A8','A11','A13','A16','A18'\n",
    "    ]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fpiqEx5iFNvg"
   },
   "source": [
    "## Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_test(liste):\n",
    "    try:\n",
    "        return sp.stats.chi2_contingency(liste)[1]\n",
    "    except Exception:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature A1 - levels merged : ['A13', 'A14']\n",
      "Feature A3 - levels merged : ['A32', 'A33']\n",
      "Feature A3 - levels merged : ['A30', 'A31']\n",
      "Feature A4 - levels merged : ['A410', 'A44']\n",
      "Feature A4 - levels merged : ['A410 - A44', 'A45']\n",
      "Feature A4 - levels merged : ['A410 - A44 - A45', 'A46']\n",
      "Feature A4 - levels merged : ['A40', 'A410 - A44 - A45 - A46']\n",
      "Feature A4 - levels merged : ['A43', 'A48']\n",
      "Feature A4 - levels merged : ['A40 - A410 - A44 - A45 - A46', 'A49']\n",
      "Feature A4 - levels merged : ['A40 - A410 - A44 - A45 - A46 - A49', 'A42']\n",
      "Feature A6 - levels merged : ['A61', 'A62']\n",
      "Feature A6 - levels merged : ['A63', 'A65']\n",
      "Feature A6 - levels merged : ['A64', 'A63 - A65']\n",
      "Feature A7 - levels merged : ['A71', 'A73']\n",
      "Feature A7 - levels merged : ['A74', 'A75']\n",
      "Feature A7 - levels merged : ['A71 - A73', 'A74 - A75']\n",
      "Feature A9 - levels merged : ['A93', 'A94']\n",
      "Feature A9 - levels merged : ['A91', 'A92']\n",
      "Feature A10 - levels merged : ['A101', 'A103']\n",
      "Feature A10 - levels merged : ['A102', 'A101 - A103']\n",
      "Feature A12 - levels merged : ['A122', 'A123']\n",
      "Feature A12 - levels merged : ['A122 - A123', 'A124']\n",
      "Feature A14 - levels merged : ['A141', 'A142']\n",
      "Feature A15 - levels merged : ['A151', 'A153']\n",
      "Feature A17 - levels merged : ['A171', 'A173']\n",
      "Feature A17 - levels merged : ['A171 - A173', 'A172']\n",
      "Feature A17 - levels merged : ['A171 - A173 - A172', 'A174']\n",
      "Feature A19 - levels merged : ['A191', 'A192']\n",
      "Feature A20 - levels merged : ['A201', 'A202']\n"
     ]
    }
   ],
   "source": [
    "german_train_grouped = german.iloc[german_features_train.index, :].copy()\n",
    "d = dict((x, []) for x in ['A1', 'A3', 'A4', 'A6', 'A7', 'A9', 'A10', 'A12', 'A14', 'A15', 'A17', 'A19', 'A20'])\n",
    "\n",
    "for var in ['A1', 'A3', 'A4', 'A6', 'A7', 'A9', 'A10', 'A12', 'A14', 'A15', 'A17', 'A19', 'A20']:\n",
    "    \n",
    "    german_train_grouped[var] = german_train_grouped[var].astype(str)\n",
    "    d[var] = [x for x in np.unique(german_train_grouped[var])]\n",
    "    p_value = 1\n",
    "\n",
    "    while(p_value>0.05):\n",
    "        if len(np.unique(german_train_grouped[var]))>1:\n",
    "            freq_table = german_train_grouped.groupby([var,'target']).size().reset_index()\n",
    "            liste_paires_modalites = [[a,b] for a in np.unique(german_train_grouped[var]) for b in np.delete(np.unique(german_train_grouped[var]),np.where(np.unique(german_train_grouped[var])==a))]\n",
    "            liste_chi2 = [chi2_test([freq_table.iloc[np.in1d(freq_table[var],pair[0]),2],freq_table.iloc[np.in1d(freq_table[var],pair[1]),2]]) for pair in liste_paires_modalites]\n",
    "            p_value = max(liste_chi2)\n",
    "        else: break\n",
    "\n",
    "        if (p_value>0.05 and len(np.unique(german_train_grouped[var]))>1):\n",
    "            german_train_grouped[var].iloc[np.in1d(german_train_grouped[var],liste_paires_modalites[np.argmax(np.equal(liste_chi2,p_value))])] = liste_paires_modalites[np.argmax(np.equal(liste_chi2,p_value))][0] + ' - ' + liste_paires_modalites[np.argmax(np.equal(liste_chi2,p_value))][1]\n",
    "            d[var].remove(liste_paires_modalites[np.argmax(np.equal(liste_chi2,p_value))][0])\n",
    "            d[var].remove(liste_paires_modalites[np.argmax(np.equal(liste_chi2,p_value))][1])\n",
    "            d[var].append(str(liste_paires_modalites[np.argmax(np.equal(liste_chi2,p_value))][0] + ' - ' + liste_paires_modalites[np.argmax(np.equal(liste_chi2,p_value))][1]))\n",
    "            print('Feature '+var+ ' - levels merged : '+str(liste_paires_modalites[np.argmax(np.equal(liste_chi2,p_value))]))\n",
    "        else: break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_ikEvzAFNvj"
   },
   "source": [
    "## Test time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_train_mdlp = score_german_MDLP_one_hot_encoder.transform(\n",
    "    transformer_cont_german.transform(german_nn_features_train[[\n",
    "        'A2','A5','A8','A11','A13','A16','A18'\n",
    "    ]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_train_grouped_label_encoders = []\n",
    "\n",
    "german_train_grouped_encoded = german_train_grouped.copy()\n",
    "\n",
    "for j in ['A1', 'A3', 'A4', 'A6', 'A7', 'A9', 'A10', 'A12', 'A14', 'A15', 'A17', 'A19', 'A20']:\n",
    "    temp = sk.preprocessing.LabelEncoder()\n",
    "    temp.fit(german_train_grouped_encoded[j].astype(str))\n",
    "    german_train_grouped_label_encoders.append(temp)\n",
    "    german_train_grouped_encoded[j] = temp.transform(german_train_grouped_encoded[j].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features=None, categories='auto',\n",
       "       dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
       "       n_values=None, sparse=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_german_CHI2_one_hot_encoder = sk.preprocessing.OneHotEncoder(categories='auto',sparse=False,handle_unknown=\"ignore\")\n",
    "\n",
    "score_german_CHI2_one_hot_encoder.fit(\n",
    "        german_train_grouped_encoded[[\n",
    "            'A1', 'A3', 'A4', 'A6', 'A7', 'A9', 'A10', 'A12', 'A14', 'A15', 'A17', 'A19', 'A20'\n",
    "        ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_train_chi2 = score_german_CHI2_one_hot_encoder.transform(german_train_grouped_encoded[[\n",
    "                    'A1', 'A3', 'A4', 'A6', 'A7', 'A9', 'A10', 'A12', 'A14', 'A15', 'A17', 'A19', 'A20'\n",
    "\n",
    "        ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_adhoc_train = np.concatenate((german_train_chi2,german_train_mdlp),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1e+20, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=1e-08, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german_adhoc_LR = sk.linear_model.LogisticRegression(C=1e20, tol=1e-8, solver=\"newton-cg\")\n",
    "german_adhoc_LR.fit(\n",
    "    german_adhoc_train,\n",
    "    german_train_grouped_encoded['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "german_test_grouped = german.iloc[german_features_test.index, :].copy()\n",
    "\n",
    "for var in ['A1', 'A3', 'A4', 'A6', 'A7', 'A9', 'A10', 'A12', 'A14', 'A15', 'A17', 'A19', 'A20']:\n",
    "    \n",
    "    german_test_grouped[var] = german_test_grouped[var].astype(str)\n",
    "\n",
    "    for x in d[var]:\n",
    "        if x.find(' - ')>-1:\n",
    "            liste_modalites = x.split(' - ')\n",
    "            german_test_grouped[var].iloc[np.in1d(german_test_grouped[var],liste_modalites)] = x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_test_grouped_encoded = german_test_grouped.copy()\n",
    "\n",
    "for j in ['A1', 'A3', 'A4', 'A6', 'A7', 'A9', 'A10', 'A12', 'A14', 'A15', 'A17', 'A19', 'A20']:\n",
    "    indice = ['A1', 'A3', 'A4', 'A6', 'A7', 'A9', 'A10', 'A12', 'A14', 'A15', 'A17', 'A19', 'A20'].index(j)\n",
    "    german_test_grouped_encoded[j] = german_train_grouped_label_encoders[indice].transform(german_test_grouped_encoded[j].astype(str))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_test_chi2 = score_german_CHI2_one_hot_encoder.transform(german_test_grouped_encoded[[\n",
    "                    'A1', 'A3', 'A4', 'A6', 'A7', 'A9', 'A10', 'A12', 'A14', 'A15', 'A17', 'A19', 'A20'\n",
    "\n",
    "        ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_test_mdlp = score_german_MDLP_one_hot_encoder.transform(\n",
    "    transformer_cont_german.transform(german_nn_features_test[[\n",
    "        'A2','A5','A8','A11','A13','A16','A18'\n",
    "    ]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_adhoc_test = np.concatenate(\n",
    "    (german_test_chi2, german_test_mdlp), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5457161125319692"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * sk.metrics.roc_auc_score(\n",
    "    german_test_grouped_encoded\n",
    "              ['target'],\n",
    "    german_adhoc_LR.predict_proba(german_adhoc_test)[:, 1]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini: 0.5457161125319694\n",
      "AUC COV: 0.0008128572147174783\n",
      "95% Gini CI: [0.43395641 0.65747581]\n"
     ]
    }
   ],
   "source": [
    "alpha = .95\n",
    "y_pred = german_adhoc_LR.predict_proba(german_adhoc_test)[:, 1]\n",
    "y_true = german_test_grouped_encoded['target']\n",
    "\n",
    "auc, auc_cov = delong_roc_variance(\n",
    "    y_true,\n",
    "    y_pred)\n",
    "\n",
    "auc_std = np.sqrt(auc_cov)\n",
    "lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "\n",
    "ci = stats.norm.ppf(\n",
    "    lower_upper_q,\n",
    "    loc=auc,\n",
    "    scale=auc_std)\n",
    "\n",
    "ci[ci > 1] = 1\n",
    "\n",
    "print('Gini:', 2*auc-1)\n",
    "print('AUC COV:', auc_cov)\n",
    "print('95% Gini CI:', 2*ci-1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ICLR 2019 - Additional material to 'Discretization and levels merging for logistic regression via shallow neural networks' Experiment Real Data.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
