<!DOCTYPE HTML>
<!--
	Read Only by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Adrien Ehrhardt - Inria, CA CF</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<link rel="icon" type="image/png" href="images/favicon.png" />
	</head>
	<body>

		<!-- Header -->
			<section id="header">
				<header>
					<span class="image avatar"><img src="images/avatar.jpg" alt="" /></span>
					<h1 id="logo"><a>Adrien Ehrhardt</a></h1>
					<p>PhD Student @Lille University & @Inria<br />
					in Machine Learning applied to Finance<br />
					I also work for Crédit Agricole Consumer Finance</p>
				</header>
				<nav id="nav">
					<ul>
					        <li><a href="index.html">Home</a></li>
						<li><a href="cifre.html">Informations CIFRE</a></li>
						<li><a href="scoring.html">Intro to Credit Scoring</a></li>
						<li><a href="rejectinference.html">Reject Inference</a></li>
                        <li><a href="discretization.html" class="active">Discretization</a></li>
                        <li><a href="interaction_screening.html">Interaction screening</a></li>
                        <li><a href="logistic_trees.html">Logistic regression trees</a></li>
                        <li><a href="teaching.html">Teaching</a></li>
					</ul>
				</nav>
				<footer>
					<ul class="icons">
						<!--<li><a href="https://www.linkedin.com/in/adrien-ehrhardt" class="icon fa-linkedin"><span class="label">Linked-In</span></a></li>-->
						<!--<li><a href="#" class="icon fa-twitter"><span class="label">Twitter</span></a></li>-->
						<li><a href="https://www.facebook.com/adrien.ehrhardt.9" class="icon fa-facebook"><span class="label">Facebook</span></a></li>
						<li><a href="https://scholar.google.fr/citations?hl=fr&user=ISAbU0cAAAAJ&view_op=list_works" class="icon fa-google"><span class="label">Google</span></a></li>
						<li><a href="https://www.github.com/adimajo/" class="icon fa-github"><span class="label">Github</span></a></li>
						<li><a href="mailto:adrien.ehrhardt@centraliens-lille.org" class="icon fa-envelope"><span class="label">Email</span></a></li>
					</ul>
				</footer>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="one">
								<div class="container">
									<header class="major">
										<h2>Discretization and factor levels grouping in logistic regression</h2>
									</header>
                                    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>


                                    <p>To improve prediction accuracy and interpretability of <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span>-based scorecards, a preprocessing step quantizing both continuous and categorical data is usually performed: continuous features are discretized by assigning factor levels to intervals and, if numerous, levels of categorical features are grouped. However, a better predictive accuracy can be reached by embedding this quantization estimation step directly into the predictive estimation step itself. By doing so, the predictive loss has to be optimized on a huge and intractable discontinuous quantization set. To overcome this difficulty, we introduced a specific two-step optimization strategy: first, the optimization problem is relaxed by approximating discontinuous quantization functions by smooth functions; second, the resulting relaxed optimization problem is solved either <em>via</em> a particular neural network and a stochastic gradient descent or an <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm. These strategies give then access to good candidates for the original optimization problem after a straightforward <em>maximum a posteriori</em> procedure to obtain cutpoints. The good performances of these approaches, which we call <em>glmdisc</em>-NN and <em>glmdisc</em>-SEM respectively, are illustrated on simulated and real data from the UCI library and <span data-acronym-label="cacf" data-acronym-form="singular+short">CACF</span>. The results show that practitioners finally have an automatic all-in-one tool that answers their recurring needs of quantization for predictive tasks.</p>
                                    <h2 id="sec:motivation">Motivation</h2>
                                    <p>As stated in <a href="#hosmer2013applied" class="citation" data-cites="hosmer2013applied">(1)</a> and illustrated in this manuscript, in many application contexts (<em>Credit Scoring</em>, biostatistics, <span><em>etc.</em></span>), <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> is widely used for its simplicity, decent performance and interpretability in predicting a binary outcome given predictors of different types (categorical, continuous). However, to achieve higher interpretability, continuous predictors are sometimes discretized so as to produce a “scorecard”, <em>i.e.</em> a table assigning a grade to an applicant in <em>Credit Scoring</em> (or a patient in biostatistics, <span><em>etc.</em></span>) depending on its predictors being in a given interval, as exemplified in Table <a href="#tab:ex_scorecard" data-reference-type="ref" data-reference="tab:ex_scorecard">[1]</a>.</p>
                                    
                                    <table>
                                        <caption><span id="tab:ex_scorecard" label="tab:ex_scorecard">[1]</span> Example of a final scorecard on quantized data.</caption>
                                        <thead>
                                            <tr class="header">
                                                <th style="text-align: left;">Feature</th>
                                                <th style="text-align: left;">Level</th>
                                                <th style="text-align: left;">Points</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr class="odd">
                                                <td style="text-align: left;">Age</td>
                                                <td style="text-align: left;">18-25</td>
                                                <td style="text-align: left;">10</td>
                                            </tr>
                                            <tr class="even">
                                                <td style="text-align: left;">Age</td>
                                                <td style="text-align: left;">25-45</td>
                                                <td style="text-align: left;">20</td>
                                            </tr>
                                            <tr class="odd">
                                                <td style="text-align: left;">Age</td>
                                                <td style="text-align: left;">45-<span class="math inline"> + ∞</span></td>
                                                <td style="text-align: left;">30</td>
                                            </tr>
                                            <tr class="even">
                                                <td style="text-align: left;">Wages</td>
                                                <td style="text-align: left;"><span class="math inline"> − ∞</span>-1000</td>
                                                <td style="text-align: left;">15</td>
                                            </tr>
                                            <tr class="odd">
                                                <td style="text-align: left;">Wages</td>
                                                <td style="text-align: left;">1000-2000</td>
                                                <td style="text-align: left;">25</td>
                                            </tr>
                                            <tr class="even">
                                                <td style="text-align: left;">Wages</td>
                                                <td style="text-align: left;">2000-<span class="math inline"> + ∞</span></td>
                                                <td style="text-align: left;">35</td>
                                            </tr>
                                            <tr class="odd">
                                                <td style="text-align: left;">…</td>
                                                <td style="text-align: left;">…</td>
                                                <td style="text-align: left;">…</td>
                                            </tr>
                                        </tbody>
                                    </table>
                                    <p>Discretization is also an opportunity for reducing the (possibly large) modeling bias which can appear in <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> as a result of the linearity assumption on the continuous predictors in the model. Indeed, this restriction can be overcome by approximating the true predictive mapping with a step function where the tuning of the steps and their sizes allow more flexibility. However, the resulting increase of the number of parameters can lead to an increase in variance (overfitting) as shown in <a href="#yang2009discretization" class="citation" data-cites="yang2009discretization">(2)</a>. Thus, a precise tuning of the discretization procedure is required. Likewise when dealing with categorical features which take numerous levels, their respective regression coefficients suffer from high variance. A straightforward solution formalized by <a href="#maj2015delete" class="citation" data-cites="maj2015delete">(3)</a> is to merge their factor levels which leads to less coefficients and therefore less variance. We showcase this phenomenon on simple simulated data in the next section. On <em>Credit Scoring</em> data, a typical example is the number of children (although not continuous strictly speaking). The log-odd ratio of clients’ creditworthiness w.r.t. their number of children is often visually “quadratic”, <em>i.e.</em> the risk is lower for clients having 1 to 3 children, a bit higher for 0 child, and then it grows steadily with the number of children above 4. This can be fitted with a parabola, see Figure <a href="#fig:nbenf_spline" data-reference-type="ref" data-reference="fig:nbenf_spline">[1]</a>. As using a spline is not very interpretable, this is not done in practice. Without quantizing the number of children, a linear relationship is assumed as displayed on Figure <a href="#fig:nbenf_cont" data-reference-type="ref" data-reference="fig:nbenf_cont">[2]</a>. When quantizing this feature, a piecewise constant relationship is assumed, see Figure <a href="#fig:nbenf_disc" data-reference-type="ref" data-reference="fig:nbenf_disc">[3]</a>. In this example, it is visually unclear which is best, such that there is a need to formalize the problem.</p>
                                    <p>Another potential motivation for quantization is optimal data compression: as will be shown rigorously in subsequent sections, quantization aims at “squeezing” as much predictive information in the original features about the class as possible. Taking an informatics point of view, quantization of a continuous feature is equivalent to discarding a float column (taking <em>e.g.</em> 32 bits per observation) by overwriting it with its quantized version (which would either be one column of unsigned 8 bits integers - “interval” encoding without order - or several 1 bit columns - one-hot / dummy encoding). The same thought process is applicable to quantizations of categorical features. In the end, the “raw” data can be compressed by a factor of <span class="math inline">32/8 = 4</span> without losing its predictive power, which, in an era of Big Data, is useful both in terms of data storage and of computational power to process these data since by 2040, the energy needs for calculations will exceed the global energy production (see <a href="#villani2018donner" class="citation" data-cites="villani2018donner">(4)</a> p. 123).</p>
                                    <p>From now on, the generic term quantization will stand for both discretization of continuous features and level grouping of categorical ones. Its aim is to improve the prediction accuracy. Such a quantization can be seen as a special case of <em>representation learning</em> <span class="citation" data-cites="bengio2013representation"></span>, but suffers from a highly combinatorial optimization problem whatever the predictive criterion used to select the best quantization. The present work proposes a strategy to overcome these combinatorial issues by invoking a relaxed alternative of the initial quantization problem leading to a simpler estimation problem since it can be easily optimized by either a specific neural network or an <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm. These relaxed versions serve as a plausible quantization provider related to the initial criterion after a classical thresholding (<em>maximum a posteriori</em>) procedure.</p>
                                    <p>The outline of this chapter is the following. After some introductory examples, we illustrate cases where quantization is either beneficial or detrimental depending on the data generating mechanism. In the subsequent section, we formalize both continuous and categorical quantization. Selecting the best quantization in a predictive setting is reformulated as a model selection problem on a huge discrete space which size is precisely derived. In Section <a href="#sec:proposal" data-reference-type="ref" data-reference="sec:proposal">1.4</a>, a particular neural network architecture is used to optimize a relaxed version of this criterion and propose good quantization candidates. In Section <a href="#sec:sem" data-reference-type="ref" data-reference="sec:sem">1.5</a>, an <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> procedure is proposed to solve the quantization problem. Section <a href="#sec:experiments" data-reference-type="ref" data-reference="sec:experiments">1.6</a> is dedicated to numerical experiments on both simulated and real data from the field of Credit Scoring, high-lightening the good results offered by the use of the two new methods without any human intervention. A final section concludes the chapter by stating also new challenges.</p>
                                    <figure>
                                        <img src="figures/chapitre4/nbenf_spline.png" alt="[fig:nbenf_spline] Risk of cacf clients w.r.t. their number of children and output of a spline regression." width="100%"/><figcaption><span id="fig:nbenf_spline" label="fig:nbenf_spline">[1]</span> Risk of <span data-acronym-label="cacf" data-acronym-form="singular+short">CACF</span> clients w.r.t. their number of children and output of a spline regression.</figcaption>
                                    </figure>
                                    <figure>
                                        <img src="figures/chapitre4/nbenf_spline_cont.png" alt="[fig:nbenf_cont] When the lr is used without quantization, it amounts to assuming the green linear relationship." width="100%"/><figcaption><span id="fig:nbenf_cont" label="fig:nbenf_cont">[2]</span> When the <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> is used without quantization, it amounts to assuming the <span style="color: green">green</span> linear relationship.</figcaption>
                                    </figure>
                                    <figure>
                                        <img src="figures/chapitre4/nbenf_spline_disc.png" alt="[fig:nbenf_disc] When the lr is used with quantization, e.g. more or less than 3 children, it amounts to assuming the risk is similar for all levels and equals the green steps." width="100%"/><figcaption><span id="fig:nbenf_disc" label="fig:nbenf_disc">[3]</span> When the <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> is used with quantization, <em>e.g.</em> more or less than 3 children, it amounts to assuming the risk is similar for all levels and equals the <span style="color: green">green</span> steps.</figcaption>
                                    </figure>
                                    
                                
                                    <h2 id="sec:bias_variance_quant">Illustration of the bias-variance quantization trade-off</h2>
                                    <p>The previous section motivated the use of quantization on a practical level. On a theoretical level, at least in terms of probability theory, quantization is equivalent to throwing away information: for continuous features, it is only known that they belong to a certain interval and for categorical features, their granularity among the original levels is lost.</p>
                                    <p>However, two things must appear clearly: first, we are in a “statistical” setting, <em>i.e.</em> finite-dimensional setting, where variance of estimation can play a big role, which partly justifies the need to regroup categorical levels. Second, we are in a predictive setting, with an imposed classification model <span class="math inline">\(p_{\boldsymbol{\theta}}\)</span>. We focus on <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span>, for which continuous features get a single coefficient: their relationship with the logit transform of the probability of an event (bad borrower) is assumed to be linear which can yield model bias. Thus, having several coefficients per feature, which can be achieved with a variety of techniques (<em>e.g.</em> splines), can yield a lower model bias (when the true model is not linear, which is generally the case for <em>Credit Scoring</em> data) at the cost of increased variance of estimation.</p>
                                    <p>This phenomenon can be very simply captured by a small simulation: in the misspecified model setting, where the logit transform is assumed to stem from a sinusoidal transformation of <span class="math inline">\(x\)</span> on <span class="math inline">[0; 1]</span>, it can clearly be seen from Figure <a href="#fig:sinus_lin" data-reference-type="ref" data-reference="fig:sinus_lin">[fig:sinus_lin]</a> that a standard linear <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> performs poorly. Discretizing the feature <span class="math inline">\(x\)</span> results, using a very simple unsupervised heuristic named <em>equal-length</em>, in good results (<em>i.e.</em> visually mild bias / low variance) so long as the number of intervals, and subsequently of <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> coefficients, is low (see the animation on Figure <a href="#fig:anim_sinus" data-reference-type="ref" data-reference="fig:anim_sinus">[fig:anim_sinus]</a> or still on Figure <a href="#fig:sinus_deb" data-reference-type="ref" data-reference="fig:sinus_deb">[fig:sinus_deb]</a>). When the number of intervals gets large, the bias gets low (the sinus is well approximated by the little step functions), but the variance gets bigger (see the animation on Figure <a href="#fig:anim_sinus" data-reference-type="ref" data-reference="fig:anim_sinus">[fig:anim_sinus]</a> or still on Figure <a href="#fig:sinus_fin" data-reference-type="ref" data-reference="fig:sinus_fin">[fig:sinus_fin]</a>).</p>
                                    <p><span>3</span></p>
                                    
                                    
                                    
                                    
                                    
                                    
                                    <p><span id="fig:sin_trois" label="fig:sin_trois">[fig:sin_trois]</span></p>
                                    <p>As the number of intervals is directly linked to the number of coefficient, and to a notion of “complexity” of the resulting <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> model, the bias-variance trade-off plays a key role in choosing an appropriate step size, and, as will be seen in the next section which was not possible for the simple <em>equal-length</em> algorithm, appropriate step locations (hereafter called cutpoints). Again, this can be witnessed visually by looking at a model selection criterion, <em>e.g.</em> the BIC criterion, for different values of the number of intervals on Figure <a href="#fig:bic_sin" data-reference-type="ref" data-reference="fig:bic_sin">[fig:bic_sin]</a>. As expected, the continuous fit is poor, yielding a high BIC value. For a low number of bins, as described in the previous paragraph, the steps of Figure <a href="#fig:anim_sinus" data-reference-type="ref" data-reference="fig:anim_sinus">[fig:anim_sinus]</a> are poor approximations of the true relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> resulting in a high BIC value. By discretizing in more intervals, the BIC value gets lower, and eventually starts to increase again when variance kicks in and overfitting occurs. As was visually concluded from Figure <a href="#fig:anim_sinus" data-reference-type="ref" data-reference="fig:anim_sinus">[fig:anim_sinus]</a>, somewhere around 10-15 intervals seem the most satisfactory since we clearly witness a low BIC value. Of course, as the model was misspecified, the flexibility brought by discretization was beneficial. The same phenomenon can be witnessed for categorical features on Figure <a href="#fig:csp" data-reference-type="ref" data-reference="fig:csp">[fig:csp]</a> with real data from <span data-acronym-label="cacf" data-acronym-form="singular+short">CACF</span>. On Figure <a href="#fig:csp_estim" data-reference-type="ref" data-reference="fig:csp_estim">[fig:csp_estim]</a>, the <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> coefficients of the raw job types are displayed: none are significant and estimation variance is large. Grouping these levels results in narrower confidence intervals and significant <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> parameters as can be seen in Figure <a href="#fig:csp_estim_disc" data-reference-type="ref" data-reference="fig:csp_estim_disc">[fig:csp_estim_disc]</a>. We formalize these empirical findings in the next section.</p>
                                    
                                    
                                    <h2 id="sec:model_selection">Quantization as a combinatorial challenge</h2>
                                    <h3 id="quantization-definition">Quantization: definition</h3>
                                    <h5 id="general-principle">General principle</h5>
                                    <p>The quantization procedure consists in turning a <span class="math inline"><em>d</em></span>-dimensional raw vector of continuous and/or categorical features <span class="math inline">\(\boldsymbol{x} = (x_1, \ldots, x_d)\)</span> into a <span class="math inline"><em>d</em></span>-dimensional categorical vector <em>via</em> a component-wise mapping <span class="math inline">\(\boldsymbol{q}=(\boldsymbol{q}_j)_1^d\)</span>: <br /><span class="math display">$$\boldsymbol{q}(\boldsymbol{x})=(\boldsymbol{q}_1(x_1),\ldots,\boldsymbol{q}_d(x_d)).$$</span><br /> Each of the univariate quantizations <span class="math inline">\(\boldsymbol{q}_j = (\boldsymbol{q}_{j,1}(x_j), \dots, \boldsymbol{q}_{j,m_j}(x_j)\)</span> is a vector of <span class="math inline">\(m_j\)</span> dummies: <br /><span class="math display">$$\begin{align*}
                                        
                                        & q_{j,h}(x_j) = 1 \text{ if } x_j \in C_{j,h}, 0 \text{ otherwise, } 1 \leq h \leq m_j, \\
                                        & \boldsymbol{q}_j(\cdot) = e^h_{m_j} \end{align*}$$</span><br /> where <span class="math inline">\(m_j\)</span> is an integer, denoting the number of intervals / groups to which <span class="math inline">\(x_j\)</span> is mapped and the sets <span class="math inline"><em>C</em><sub><em>j</em>, <em>h</em></sub></span> are defined with respect to each feature type as is described just below.</p>
                                    <h5 id="raw-continuous-features">Raw continuous features</h5>
                                    <p>If <span class="math inline">\(x_j\)</span> is a continuous component of <span class="math inline">\(\boldsymbol{x}\)</span>, quantization <span class="math inline">\(\boldsymbol{q}_j\)</span> has to perform a discretization of <span class="math inline">\(x_j\)</span> and the <span class="math inline"><em>C</em><sub><em>j</em>, <em>h</em></sub></span>’s, <span class="math inline">\(1 \leq h \leq m_j\)</span>, are contiguous intervals: <br /><span class="math display"><em>C</em><sub><em>j</em>, <em>h</em></sub> = (<em>c</em><sub><em>j</em>, <em>h</em> − 1</sub>, <em>c</em><sub><em>j</em>, <em>h</em></sub>],</span><br /> where <span class="math inline">\(c_{j,1},\ldots,c_{j,m_j-1}\)</span> are increasing numbers called cutpoints, <span class="math inline"><em>c</em><sub><em>j</em>, 0</sub> =  − ∞</span>, <span class="math inline">\(c_{j,m_j}=\infty\)</span>. For example, the quantization of the unit segment in thirds would be defined as <span class="math inline">\(m_j=3\)</span>, <span class="math inline"><em>c</em><sub><em>j</em>, 1</sub> = 1/3</span>, <span class="math inline"><em>c</em><sub><em>j</em>, 2</sub> = 2/3</span> and subsequently <span class="math inline">\(\boldsymbol{q}_j(0.1) = (1,0,0)\)</span>. This is visually exemplified on Figure <a href="#fig:disc_cont" data-reference-type="ref" data-reference="fig:disc_cont">[fig:disc_cont]</a>.</p>
                                    <h5 id="raw-categorical-features">Raw categorical features</h5>
                                    <p>If <span class="math inline">\(x_j\)</span> is a categorical component of <span class="math inline">\(\boldsymbol{x}\)</span>, quantization <span class="math inline">\(\boldsymbol{q}_j\)</span> consists in grouping levels of <span class="math inline">\(x_j\)</span> and the <span class="math inline"><em>C</em><sub><em>j</em>, <em>h</em></sub></span>s form a partition of the set <span class="math inline">\(\mathbb{N}_{o_j} = \{1, \dots, l_j\}\)</span>: <br /><span class="math display">$$\begin{aligned}
                                        %\bigsqcup_{h=1}^{m_j}C_{j,h}=\{1,\ldots,l_j\}.
                                        \bigcup_{h=1}^{m_j}C_{j,h} &amp; = \mathbb{N}_{o_j}, \\
                                        \forall h, h', \: C_{j,h} \cap C_{j,h'} &amp; = \varnothing .\end{aligned}$$</span><br /> For example, the grouping of levels encoded as “1” and “2” would yield <span class="math inline"><em>C</em><sub><em>j</em>, 1</sub> = {1, 2}</span> such that <span class="math inline">\(\boldsymbol{q}_j(1) = \boldsymbol{q}_j(2) = (1,0,\ldots,0)\)</span>. Note that it is assumed that there are no empty buckets, <em>i.e.</em> <span class="math inline">∄<em>j</em>, <em>h</em></span> s.t. <span class="math inline"><em>C</em><sub><em>j</em>, <em>h</em></sub> = ⌀</span>. This is visually exemplified on Figure <a href="#fig:disc_disc" data-reference-type="ref" data-reference="fig:disc_disc">[fig:disc_disc]</a>.</p>
                                    <p><span>2</span></p>
                                    
                                    
                                    
                                    <h3 id="subsec:cardinality_quant">Cardinality of the quantization family</h3>
                                    <h5 id="notations-for-the-quantization-family">Notations for the quantization family</h5>
                                    <p>In both continuous and categorical cases, keep in mind that <span class="math inline">\(m_j\)</span> is the dimension of <span class="math inline">\(\boldsymbol{q}_j\)</span>. For notational convenience, the (global) order of the quantization <span class="math inline">\(\boldsymbol{q}\)</span> is set as <br /><span class="math display">$$|\boldsymbol{q}|=\sum_{j=1}^d m_j.$$</span><br /> The space where quantizations <span class="math inline">\(\boldsymbol{q}\)</span> live (resp. <span class="math inline">\(\boldsymbol{q}_j\)</span>) will be denoted by <span class="math inline">\(\boldsymbol{Q}_\boldsymbol{m}\)</span> in the sequel (resp. <span class="math inline">\(\boldsymbol{Q}_j^{m_j}\)</span>), when the number of levels <span class="math inline">\(\boldsymbol{m} = (m_j)_1^d\)</span> is fixed. Since it is not known, the full model space is <span class="math inline">\(\boldsymbol{Q} = \cup_{m \in \mathbb{N}_\star^{d}} \boldsymbol{Q}_\boldsymbol{m}\)</span> where <span class="math inline">\(\mathbb{N}_\star^{d} = (\mathbb{N} \setminus \{0\})^d\)</span>.</p>
                                    <h6 id="par:equiv">Equivalence of quantizations</h6>
                                    <p>Let <span class="math inline">\(\boldsymbol{q}^1\)</span> and <span class="math inline">\(\boldsymbol{q}^2\)</span> in <span class="math inline">\(\boldsymbol{Q}\)</span> such that <span class="math inline">\(\boldsymbol{q}^1 \mathcal{R}_{\mathcal{T}_{\text{f}}} \boldsymbol{q}^2 \equiv \forall i,j \; \boldsymbol{q}^1_j(x_{i,j}) = \boldsymbol{q}^2_j(x_{i,j})\)</span>. See Figure <a href="#fig:equiv" data-reference-type="ref" data-reference="fig:equiv">[fig:equiv]</a> for an example.</p>
                                    <h6 id="lemma">Lemma</h6>
                                    <p>Relation <span class="math inline">\(\mathcal{R}_{\mathcal{T}_{\text{f}}}\)</span> defines an equivalence relation on <span class="math inline">\(\boldsymbol{Q}\)</span>.</p>
                                    <p>Relation <span class="math inline">\(\mathcal{R}_{\mathcal{T}_{\text{f}}}\)</span> is trivially reflexive and symmetric because of the reflexive and symmetric nature of the equality relation in <span class="math inline">\(\mathbb{R}\)</span>: <span class="math inline">\(\forall i,j \; \boldsymbol{q}^1_j(x_{i,j}) = \boldsymbol{q}^1_j(x_{i,j})\)</span> and <span class="math inline">\(\forall i,j \; \boldsymbol{q}^1(x_{i,j}) = \boldsymbol{q}^2(x_{i,j})\)</span>. Similarly, let <span class="math inline">\(\boldsymbol{q}^3 \in \boldsymbol{Q}\)</span> such that <span class="math inline">\(\boldsymbol{q}^1 \mathcal{R}_{\mathcal{T}_{\text{f}}} \boldsymbol{q}^3  \equiv \forall i,j \; \boldsymbol{q}^1_j(x_{i,j}) = \boldsymbol{q}^3_j(x_{i,j})\)</span>. Again, we immediately get <span class="math inline">\(\forall i,j \; \boldsymbol{q}^2_j(x_{i,j}) = \boldsymbol{q}^3_j(x_{i,j})\)</span>, <em>i.e.</em> <span class="math inline">\(\boldsymbol{q}^2 \mathcal{R}_{\mathcal{T}_{\text{f}}} \boldsymbol{q}^3\)</span> which proves the transitivity of <span class="math inline">\(\mathcal{R}_{\mathcal{T}_{\text{f}}}\)</span>.</p>
                                    
                                    <h6 id="cardinality-of-the-quantization-family-in-the-continuous-case">Cardinality of the quantization family in the continuous case</h6>
                                    <p>For a continuous feature <span class="math inline"><em>x</em><sub><em>j</em></sub></span>, let <span class="math inline">\(\boldsymbol{q}_j \in \boldsymbol{Q}_j^{m_j}\)</span> and cutpoints <span class="math inline"><strong>c</strong><sub><em>j</em></sub></span>. Without any loss of generality, <em>i.e.</em> up to a relabelling on individuals <span class="math inline"><em>i</em></span>, it can be assumed that there are no empty intervals (all <span class="math inline"><em>m</em><sub><em>j</em></sub></span> levels are observed) and consequently <span class="math inline">\(m_j+1\)</span> observations <span class="math inline">\(x_{1,j},\dots,x_{m_j+1,j}\)</span> s.t. <span class="math inline">\(x_{1,j} &lt; c_{j,1} &lt; x_{2,j} &lt; \dots &lt; c_{m_j-1,1} &lt; x_{m_j+1,j}\)</span>. Indeed, if for example there exists <span class="math inline">\(k &lt; m_j - 1\)</span> s.t. <span class="math inline">\(c_{j,k} &lt; \dots &lt; c_{j,m_j-1}\)</span> and <span class="math inline">max<sub>1 ≤ <em>i</em> ≤ <em>n</em></sub><em>x</em><sub><em>i</em>, <em>j</em></sub> &lt; <em>c</em><sub><em>j</em>, <em>k</em></sub></span>, then discretization <span class="math inline">\(\boldsymbol{q}^{\text{bis}}_j \in \boldsymbol{Q}_{j,k}\)</span> with <span class="math inline"><em>k</em> + 1</span> cutpoints <span class="math inline">( − ∞, <em>c</em><sub><em>j</em>, 1</sub>, …, <em>c</em><sub><em>j</em>, <em>k</em> − 1</sub>,  + ∞)</span> is equivalent w.r.t. <span class="math inline">\(\mathcal{R}_{\mathcal{T}_n}\)</span> to <span class="math inline">\(\boldsymbol{q}_j\)</span>: <span class="math inline">\(\forall i, \; \boldsymbol{q}_j(x_{i,j}) = \boldsymbol{q}^{\text{bis}}_j(x_{i,j})\)</span>. A similar proof can be conducted with cutpoints below the minimum of <span class="math inline">\(\boldsymbol{x}_j\)</span> or with several cutpoints in-between consecutive values of the observations. Subsequently, there are <span class="math inline">\(\binom{n-1}{m_j-1}\)</span> ways to construct <span class="math inline"><strong>c</strong><sub><em>j</em></sub></span>, <em>i.e.</em> equivalence classes <span class="math inline">\([\boldsymbol{q}_j]\)</span> for a fixed <span class="math inline">\(m_j \leq n\)</span>. The number of intervals <span class="math inline">\(m_j\)</span> can range from <span class="math inline">2</span> (binarization) to <span class="math inline"><em>n</em></span> (each <span class="math inline"><em>x</em><sub><em>i</em>, <em>j</em></sub></span> is in its own interval, thus <span class="math inline">\(\boldsymbol{q}_j(x_{i,j}) \neq \boldsymbol{q}_j(x_{i',j})\)</span> for <span class="math inline"><em>i</em> ≠ <em>i</em>′</span>; ), so that the number of admissible discretizations of <span class="math inline">\(\boldsymbol{\mathbf{x}}_j\)</span> is <span class="math inline">\(|\boldsymbol{Q}_j| = \sum_{i=2}^{n}\)</span> <span class="math inline">\({n-1}\choose{i-1}\)</span>. Note that <span class="math inline">\(|\boldsymbol{Q}_j|\)</span> depends on the number of observations <span class="math inline"><em>n</em></span>; we shall go back to this property in the following section.</p>
                                    <h6 id="cardinality-of-the-quantization-family-in-the-categorical-case">Cardinality of the quantization family in the categorical case</h6>
                                    <p>For a continuous feature <span class="math inline"><em>x</em><sub><em>j</em></sub></span>, let <span class="math inline">\(\boldsymbol{q}_j \in \boldsymbol{Q}_j\)</span> with <span class="math inline">\(m_j\)</span> groups. The number of re-arrangements of <span class="math inline">\(l_j\)</span> labelled elements into <span class="math inline">\(m_j\)</span> unlabelled groups is given by the Stirling number of the second kind <span class="math inline">\(S(l_j,m_j) = \frac{1}{m_j!} \sum_{i=0}^{m_j} (-1)^{m_j-i} {m_j \choose i} i^{l_j}\)</span>. As <span class="math inline">\(m_j\)</span> is unknown, it must be searched over the range <span class="math inline">\(\{1,\dots,l_j\}\)</span>. Thus for categorical features, model space <span class="math inline">\(\boldsymbol{Q}_j\)</span> is also discrete; subsequently, <span class="math inline">\(\boldsymbol{Q}\)</span> is discrete.</p>
                                    <h3 id="literature-review">Literature review</h3>
                                    <p>The current practice of quantization is prior to any predictive task, thus ignoring its consequences on the final predictive ability. It consists in optimizing a heuristic criterion, often totally unrelated (unsupervised methods) or at least explicitly (supervised methods) to prediction, and mostly univariate (each feature is quantized irrespective of other features’ values). The cardinality of the quantization space <span class="math inline">\(\boldsymbol{Q}\)</span> was calculated explicitely w.r.t. <span class="math inline"><em>d</em></span>, <span class="math inline">\((m_j)_1^d\)</span> and, for categorical features, <span class="math inline">\(l_j\)</span>: it is huge, so that a greedy approach is intractable and such heuristics are needed, as will be detailed in the next section.</p>
                                    <p>Many algorithms have thus been designed and a review of approximatively 200 discretization strategies, gathering both criteria and related algorithms, can be found in <a href="#ramirez2016data" class="citation" data-cites="ramirez2016data">(5)</a>, preceded by other enlightening review articles such as <a href="#dougherty1995supervised" class="citation" data-cites="dougherty1995supervised">(6)</a>, <a href="#liu2002discretization" class="citation" data-cites="liu2002discretization">(7)</a>. They classify discretization methods by distinguishing, among other criteria and as said previously, unsupervised and supervised methods (<span class="math inline"><em>y</em></span> is used to discretize <span class="math inline">\(\boldsymbol{x}\)</span>), for which model-specific (assumptions on <span class="math inline">\(p_{\boldsymbol{\theta}}\)</span>) or model-free approaches are distinguished, univariate and multivariate methods (features <span class="math inline">\(\boldsymbol{x}_{-\{j\}} = (x_{1},\ldots,x_{j-1},x_{j+1},\ldots,x_{d})\)</span> may influence the quantization scheme of <span class="math inline">\(x_j\)</span>) and other criteria as can be seen from Figure <a href="#fig:taxonomy" data-reference-type="ref" data-reference="fig:taxonomy">[fig:taxonomy]</a> reproduced from <a href="#ramirez2016data" class="citation" data-cites="ramirez2016data">(5)</a> with permission.</p>
                                    <p>For factor levels grouping, we found no such taxonomy, but some discretization methods, <em>e.g.</em> <span class="math inline"><em>χ</em><sup>2</sup></span> independence test-based methods can be naturally extended to this type of quantization, which is for example what the CHAID algorithm, proposed in <a href="#kass1980exploratory" class="citation" data-cites="kass1980exploratory">(8)</a> and applied to each categorical feature, relies on. A simple idea is also to use Group LASSO <span class="citation" data-cites="meier2008group"></span> which attempts to shrink to zero all coefficients of a categorical feature to avoid situations where a few levels enter the model, which is arguably less interpretable. Another idea would be to use Fused LASSO <span class="citation" data-cites="tibshirani2005sparsity"></span>, which seeks to shrink the pairwise absolute difference of selected coefficients, and apply it to all pairs of levels: the levels for which the difference would be shrunk to zero would be grouped. A combination of both approaches would allow both selection and grouping<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
                                    <p>For benchmarking purposes, and following results found in the taxonomy of <a href="#ramirez2016data" class="citation" data-cites="ramirez2016data">(5)</a>, we used the MDLP <a href="#fayyad1993multi" class="citation" data-cites="fayyad1993multi">(9)</a> discretization method, which is a popular supervised univariate discretization method, and we implemented an extension of the discretization method ChiMerge <a href="#kerber1992chimerge" class="citation" data-cites="kerber1992chimerge">(10)</a> to categorical features, performing pairwise <span class="math inline"><em>χ</em><sup>2</sup></span> independence tests rather than only pairs of contiguous intervals, which we called ChiCollapse. Note that various refinements of ChiMerge have been proposed in the literature, Chi2 <a href="#liu1995chi2" class="citation" data-cites="liu1995chi2">(11)</a>, ConMerge <a href="#wang1998concurrent" class="citation" data-cites="wang1998concurrent">(12)</a>, ModifiedChi2 <a href="#tay2002modified" class="citation" data-cites="tay2002modified">(13)</a>, and ExtendedChi2 <a href="#su2005extended" class="citation" data-cites="su2005extended">(14)</a>, which seek to correct for multiple hypothesis testing <a href="#shaffer1995multiple" class="citation" data-cites="shaffer1995multiple">(15)</a> and automize the choice of the confidence parameter <span class="math inline"><em>α</em></span> in the <span class="math inline"><em>χ</em><sup>2</sup></span> tests, but adapting them to categorical features for benchmarking purposes would have been too time-consuming. A similar measure, called Zeta, has been proposed in place of <span class="math inline"><em>χ</em><sup>2</sup></span> in <a href="#ho1997zeta" class="citation" data-cites="ho1997zeta">(16)</a> and subsequent refinement <a href="#ho1998efficient" class="citation" data-cites="ho1998efficient">(17)</a>: it is the classification error achievable by using only two contiguous intervals; if it is low, the two intervals are dissimilar w.r.t. the prediction task, if not, they can be merged.</p>
                                    <figure>
                                        <img src="figures/chapitre4/taxonomy.PNG" alt="Taxonomy of discretization methods." id="fig:taxonomy" width="100%"/><figcaption>Taxonomy of discretization methods.<span label="fig:taxonomy"></span></figcaption>
                                    </figure>
                                    <h3 id="subsec:embedding">Quantization embedded in a predictive process</h3>
                                    <p>In what follows, focus is given to <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> since it is a requirement from <span data-acronym-label="cacf" data-acronym-form="singular+short">CACF</span> but it is applicable to any other supervised classification model.</p>
                                    <h5 id="logistic-regression-on-quantized-data">Logistic regression on quantized data</h5>
                                    <p>Quantization is a widespread preprocessing step to perform a learning task consisting in predicting, say, a binary variable <span class="math inline">\(y\in\{0,1\}\)</span>, from a quantized predictor <span class="math inline">\(\boldsymbol{q}(\boldsymbol{x})\)</span>, through, say, a parametric conditional distribution <span class="math inline">\(p_{\boldsymbol{\theta}}(y|\boldsymbol{q}(\boldsymbol{x}))\)</span> like <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span>; the whole process can be visually represented as a dependence structure among <span class="math inline">\(\boldsymbol{x}\)</span>, its quantization <span class="math inline">\(\boldsymbol{q}(\boldsymbol{x})\)</span> and the target <span class="math inline">\(y\)</span> on Figure <a href="#fig:dep" data-reference-type="ref" data-reference="fig:dep">[fig:dep]</a>. Considering quantized data instead of raw data has a double benefit. First, the quantization order <span class="math inline">\(|\boldsymbol{q}|\)</span> acts as a tuning parameter for controlling the model’s flexibility and thus the bias/variance trade-off of the estimate of the parameter <span class="math inline">\(\boldsymbol{\theta}\)</span> (or of its predictive accuracy) for a given dataset. This claim becomes clearer with the example of logistic regression we focus on, as a still very popular model for many practitioners. On quantized data, logistic regression becomes: <br /><span class="math display">$$\label{eq:reglogq}
                                        \ln \left( \dfrac{p_{\boldsymbol{\theta}}(1|\boldsymbol{q}(\boldsymbol{x}))}{1 - p_{\boldsymbol{\theta}}(1|\boldsymbol{q}(\boldsymbol{x}))} \right) = \theta_0 + \sum_{j=1}^d \boldsymbol{q}_j(x_j)' \boldsymbol{\theta}_j,$$</span><br /> where <span class="math inline">\(\boldsymbol{\theta} = (\theta_{0},(\boldsymbol{\theta}_j)_1^d) \in \mathbb{R}^{|\boldsymbol{q}|+1}\)</span> and <span class="math inline">\(\boldsymbol{\theta}_j = (\theta_{j}^{1},\dots,\theta_{j}^{m_j})\)</span> with <span class="math inline">\(\theta_{j}^{m_j} = 0\)</span>, <span class="math inline"><em>j</em> = 1…<em>d</em></span>, for identifiability reasons. Second, at the practitioner level, the previous tuning of <span class="math inline">\(|\boldsymbol{q}|\)</span> through each feature’s quantization order <span class="math inline">\(m_j\)</span>, especially when it is quite low, allows an easier interpretation of the most important predictor values involved in the predictive process. Denoting the <span class="math inline"><em>n</em></span>-sample of financed clients by <span class="math inline">\((\boldsymbol{\mathbf{x}}_{\text{f}},\boldsymbol{\mathbf{y}}_{\text{f}})\)</span>, with <span class="math inline">\(\boldsymbol{\mathbf{x}}_{\text{f}}=(\boldsymbol{x}_1,\ldots,\boldsymbol{x}_n)\)</span> and <span class="math inline">\(\boldsymbol{\mathbf{y}}_{\text{f}}=(y_1,\ldots,y_n)\)</span>, the log-likelihood <br /><span class="math display">$$\label{eq:lq}
                                            \ell_{\boldsymbol{q}}(\boldsymbol{\theta} ; \mathcal{T}_{\text{f}})=\sum_{i=1}^n \ln p_{\boldsymbol{\theta}}(y_i|\boldsymbol{q}(\boldsymbol{x}_i))$$</span><br /> provides a maximum likelihood estimator <span class="math inline">\(\hat{\boldsymbol{\theta}}_{\boldsymbol{q}}\)</span> of <span class="math inline">\(\boldsymbol{\theta}\)</span> for a given quantization <span class="math inline">\(\boldsymbol{q}\)</span>. For the rest of the post, the approach is exemplified with <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> as <span class="math inline">\(p_{\boldsymbol{\theta}}\)</span> but it can be applied to any other predictive model, as will be recalled in the concluding section.</p>
                                        
                                        
                                        <h5 id="par:model_selec">Quantization as a model selection problem</h5>
                                        <p>As dicussed in the previous section, and emphasized in the literature review, quantization is often a preprocessing step; however, quantization can be embedded directly in the predictive model. Continuing our logistic example, a standard information criteria such as the BIC can be used to select the best quantization: <br /><span class="math display">$$\label{eq:BICq}
                                            \hat{\boldsymbol{q}}=\arg\!\min_{\boldsymbol{q} \in \boldsymbol{Q}} \text{BIC}(\hat{\boldsymbol{\theta}}_{\boldsymbol{q}})$$</span><br /> where the “complexity” parameter depends on <span class="math inline">\({\boldsymbol{q}}\)</span> and is traditionally the number of continuous parameters to be estimated in the <span class="math inline">\(\boldsymbol{\theta}\)</span>-parameter space. The practitioner can swap this criterion with any other penalized criterion on training data such as AIC <a href="#akaike1973information" class="citation" data-cites="akaike1973information">(18)</a> or, as <em>Credit Scoring</em> people like, the Gini index on a test set. Note however that, regardless of the used criterion, an exhaustive search of <span class="math inline">\(\hat{\boldsymbol{q}}\in\boldsymbol{Q}\)</span> is an intractable task due to its highly combinatorial nature as was explicitly formulated in the previous section. Anyway, the optimization of the above BIC criterion requires a new specific strategy that is described in the next section.</p>
                                        <h5 id="par:consistency">Remarks on model selection consistency</h5>
                                        <p>In high-dimensional spaces and among models with a wildly varying number of parameters, classical model selection tools like BIC can have disappointing asymptotic properties, as emphasized in <a href="#chen2008extended" class="citation" data-cites="chen2008extended">(19)</a>, where a modified BIC criterion, taking into account the number of models per parameter size, is proposed. Moreover in essence, as is apparent from the <span class="math inline">\(\hat{\boldsymbol{\theta}}_{\boldsymbol{q}}\)</span> symbol, and supplemental to the <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> coefficients <em>per se</em>, the inherent “parameters” <span class="math inline"><em>C</em><sub><em>j</em>, <em>h</em></sub></span> of <span class="math inline">\(\boldsymbol{q}\)</span> shall be accounted for in the penalization term <span class="math inline"><em>ν</em></span>: they are estimated indirectly in all quantization methods, and in particular in the one we propose in the subsequent section.</p>
                                        <p>In addition, the BIC criterion relies on the Laplace approximation <span class="citation" data-cites="lebarbier"></span> which requires the likelihood to be twice differentiable in the parameters. However, as <span class="math inline">\(\boldsymbol{q}\)</span> consists in a collection of step functions of parameters <span class="math inline"><em>C</em><sub><em>j</em>, <em>h</em></sub></span>, this is not the case. For continuous features, since it is nevertheless almost everywhere differentiable, for the properties of the BIC criterion to hold, it suffices that there exists a neighbourhood <span class="math inline"><em>V</em><sub><em>j</em>, <em>h</em></sub></span> around true parameters <span class="math inline"><em>c</em><sub><em>j</em>, <em>h</em></sub><sup>⋆</sup></span> where there is no observation: <span class="math inline">\(\not\exists i, \: x_{i,j} \in V_{j,h}\)</span>. For categorical features, the Laplace approximation <span class="citation" data-cites="lebarbier"></span> is no longer valid and there is no way, in general, to approximate the integral (<em>i.e.</em> the sum over the discrete parameter space) by “counting” the number of parameters as in the continuous case <a href="#vincent_disc" class="citation" data-cites="vincent_disc">(20)</a>.</p>
                                        <h2 id="sec:proposal">The proposed neural network based quantization</h2>
                                        <h3 id="subsec:relaxation">A relaxation of the optimization problem</h3>
                                        <p>In this section, we propose to relax the constraints on <span class="math inline">\(\boldsymbol{q}_j\)</span> to simplify the search of <span class="math inline">\(\hat{\boldsymbol{q}}\)</span>. Indeed, the derivatives of <span class="math inline">\(\boldsymbol{q}_j\)</span> are zero almost everywhere and consequently a gradient descent cannot be directly applied to find an optimal quantization.</p>
                                        <h5 id="smooth-approximation-of-the-quantization-mapping">Smooth approximation of the quantization mapping</h5>
                                        <p>A classical approach is to replace the binary functions <span class="math inline"><em>q</em><sub><em>j</em>, <em>h</em></sub></span> (see Equation (<a href="#eq:qj" data-reference-type="ref" data-reference="eq:qj">[eq:qj]</a>)) by smooth parametric ones with a simplex condition, namely with <span class="math inline">\(\boldsymbol{\alpha}_j=(\boldsymbol{\alpha}_{j,1},\ldots, \boldsymbol{\alpha}_{j,m_j})\)</span>: <br /><span class="math display">$$%\label{eq:qaj}
                                            {\boldsymbol{q}_{\boldsymbol{\alpha}_j}(\cdot)=\left(q_{\boldsymbol{\alpha}_{j,h}}(\cdot)\right)_{h=1}^{m_j} \text{ with } \sum_{h=1}^{m_j}q_{\boldsymbol{\alpha}_{j,h}}(\cdot)=1 \text{ and } 0 \leq q_{\boldsymbol{\alpha}_{j,h}}(\cdot) \leq 1,}$$</span><br /> where functions <span class="math inline">\(q_{\boldsymbol{\alpha}_{j,h}}(\cdot)\)</span>, properly defined hereafter for both continuous and categorical features, represent a fuzzy quantization in that, here, each level <span class="math inline"><em>h</em></span> is weighted by <span class="math inline">\(q_{\boldsymbol{\alpha}_{j,h}}(\cdot)\)</span> instead of being selected once and for all. The resulting fuzzy quantization for all components depends on the global parameter <span class="math inline">\(\boldsymbol{\alpha} = (\boldsymbol{\alpha}_1, \ldots, \boldsymbol{\alpha}_d)\)</span> and is denoted by <span class="math inline">\(\boldsymbol{q}_{\boldsymbol{\alpha}}(\cdot)=\left(\boldsymbol{q}_{\boldsymbol{\alpha}_j}(\cdot)\right)_{j=1}^d\)</span>.</p>
                                        <p><span><strong>For continuous features</strong></span>, we set for <span class="math inline">\(\boldsymbol{\alpha}_{j,h} = (\alpha^0_{j,h},\alpha^1_{j,h}) \in \mathbb{R}^2\)</span> <br /><span class="math display">$$\label{eq:softmax}
                                            q_{\boldsymbol{\alpha}_{j,h}}(\cdot) = \frac{\exp(\alpha^0_{j,h} + \alpha^1_{j,h}  \cdot)}{\sum_{g=1}^{m_j} \exp(\alpha^0_{j,g} + \alpha^1_{j,g}  \cdot)}$$</span><br /> where <span class="math inline">\(\boldsymbol{\alpha}_{j,m_j}\)</span> is set to <span class="math inline">(0, 0)</span> for identifiability reasons.</p>
                                        <p><span><strong>For categorical features</strong></span>, we set for <span class="math inline">\(\boldsymbol{\alpha}_{j,h}=\left(\alpha_{j,h}(1),\ldots, \alpha_{j,h}(l_j)\right) \in \mathbb{R}^{l_j}\)</span> <br /><span class="math display">$$q_{\boldsymbol{\alpha}_{j,h}}(\cdot) = \frac{\exp\left(\alpha_{j,h}(\cdot)\right)}{\sum_{g=1}^{m_j} \exp\left(\alpha_{j,g}(\cdot)\right)}$$</span><br /> where <span class="math inline">\(l_j\)</span> is the number of levels of the categorical feature <span class="math inline">\(x_j\)</span>.</p>
                                        <h5 id="parameter-estimation">Parameter estimation</h5>
                                        <p>With this new fuzzy quantization, the <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> for the predictive task is then expressed as <br /><span class="math display">$$\label{eq:reglogqa}
                                            \ln \left( \dfrac{p_{\boldsymbol{\theta}}(1|\boldsymbol{q}_{\boldsymbol{\alpha}} (\boldsymbol{x}))}{1 - p_{\boldsymbol{\theta}}(1|\boldsymbol{q}_{\boldsymbol{\alpha}} (\boldsymbol{x}))} \right) = \theta_0 + \sum_{j=1}^d { \boldsymbol{q}_{\boldsymbol{\alpha}_{j}}(x_j)' \boldsymbol{\theta}_j },$$</span><br /> where <span class="math inline">\(\boldsymbol{q}\)</span> has been replaced by <span class="math inline">\(\boldsymbol{q}_{\boldsymbol{\alpha}}\)</span>. Note that as <span class="math inline">\(\boldsymbol{q}_{\boldsymbol{\alpha}}\)</span> is a sound approximation of <span class="math inline">\(\boldsymbol{q}\)</span> (see above), this <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> in <span class="math inline">\(\boldsymbol{q}_{\boldsymbol{\alpha}}\)</span> is consequently a good approximation of the <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> in <span class="math inline">\(\boldsymbol{q}\)</span> from Equation (<a href="#eq:reglogq" data-reference-type="ref" data-reference="eq:reglogq">[eq:reglogq]</a>). The relevant log-likelihood is here <br /><span class="math display">$$\label{eq:lqa}
                                                \ell_{\boldsymbol{q}_{\boldsymbol{\alpha}}}(\boldsymbol{\theta} ; \mathcal{T}_{\text{f}})=\sum_{i=1}^n \ln p_{\boldsymbol{\theta}}(y_i|\boldsymbol{q}_{\boldsymbol{\alpha}}(\boldsymbol{x}_i))$$</span><br /> and can be used as a tractable substitute for <span class="math inline">\(\ell_q\)</span> to solve the original optimization problem, where now both <span class="math inline">\(\boldsymbol{\alpha}\)</span> and <span class="math inline">\(\boldsymbol{\theta}\)</span> have to be estimated, which is discussed in the next section.</p>
                                            <h6 id="deducing-quantizations-from-the-relaxed-problem">Deducing quantizations from the relaxed problem</h6>
                                            <p>We wish to maximize the log-likelihood <span class="math inline">\(\ell_{\boldsymbol{q}_{\boldsymbol{\alpha}}}\)</span> which would yield parameters <span class="math inline">\((\hat{\boldsymbol{\alpha}},\hat{\boldsymbol{\theta}})\)</span>; these are consistent if the model is well-specified (<em>i.e.</em> there is a “true” quantization under classical regularity conditions). Denoting by <span class="math inline"><em>A</em></span> the space of <span class="math inline">\(\boldsymbol{\alpha}\)</span> and <span class="math inline">\(\boldsymbol{Q}_A\)</span> the space of <span class="math inline">\(\boldsymbol{q}_{\boldsymbol{\alpha}}\)</span>, to “push” <span class="math inline">\(\boldsymbol{Q}_A\)</span> further into <span class="math inline">\(\boldsymbol{Q}\)</span>, <span class="math inline">\(\hat{\boldsymbol{q}}\)</span> is deduced from a <em>maximum a posteriori</em> procedure applied to <span class="math inline">\(\boldsymbol{q}_{\hat{\boldsymbol{\alpha}}}\)</span>: <br /><span class="math display">$$\label{eq:ht}
                                                \hat{q}_{j,h}(x_j) = 1 \equiv \hat{\boldsymbol{q}}_j(x_j) = e^h_{m_j} \text{ if } h = \arg\!\max_{1 \leq h' \leq m_j} q_{\hat{\boldsymbol{\alpha}}_{j,h'}}(x_j), 0 \text{ otherwise.}
                                                %\nonumber$$</span><br /> If there are several levels <span class="math inline"><em>h</em></span> that satisfy the above argmax, we simply take the level that corresponds to smaller values of <span class="math inline">\(x_j\)</span> to be in accordance with the definition of <span class="math inline"><em>C</em><sub><em>j</em>, <em>h</em></sub></span>. This <span><em>maximum a posteriori</em></span> principle will be exemplified in Figure <a href="#fig:MAP" data-reference-type="ref" data-reference="fig:MAP">[fig:MAP]</a> on simulated data. These approximations are justified by the following arguments.</p>
                                            <h6 id="rationale">Rationale</h6>
                                            <p>From a deterministic point of view, we have <span class="math inline">\(\boldsymbol{Q} \subset \boldsymbol{Q}_A\)</span>: First, the <em>maximum a posteriori</em> step (<a href="#eq:ht" data-reference-type="ref" data-reference="eq:ht">[eq:ht]</a>) produces contiguous intervals (<em>i.e.</em> there exists <span class="math inline"><em>C</em><sub><em>j</em>, <em>h</em></sub></span>; <span class="math inline">1 ≤ <em>j</em> ≤ <em>d</em></span>, <span class="math inline">\(1 \leq h \leq m_j\)</span>, s.t. <span class="math inline">\({\hat{\boldsymbol{q}}}\)</span> can be written as in <a href="#eq:qj" data-reference-type="ref" data-reference="eq:qj">[eq:qj]</a>) <a href="#same2011model" class="citation" data-cites="same2011model">(21)</a>. Second, in the continuous case, the higher <span class="math inline"><em>α</em><sub><em>j</em>, <em>h</em></sub><sup>1</sup></span>, the less smooth the transition from one quantization <span class="math inline"><em>h</em></span> to its “neighbor” <span class="math inline"><em>h</em> + 1</span>, whereas <span class="math inline">\(\dfrac{\alpha_{j,h}^0}{\alpha_{j,h}^1}\)</span> controls the point in <span class="math inline">\(\mathbb{R}\)</span> where the transition occurs <a href="#chamroukhi2009regression" class="citation" data-cites="chamroukhi2009regression">(22)</a>. Concerning the categorical case, the rationale is even simpler as <span class="math inline">\(q_{\lambda \boldsymbol{\alpha}_j}(x_j) \to e^h_{m_j} \text{ if } h = \arg\!\max_{h'} q_{\alpha_{j,h'}}(x_j)\)</span> as <span class="math inline"><em>λ</em> →  + ∞</span> <a href="#reverdy2016parameter" class="citation" data-cites="reverdy2016parameter">(23)</a>.</p>
                                            <p>From a statistical point of view, provided the model is well-specified, <em>i.e.</em>: <br /><span class="math display">$$\label{eq:well_quant}
                                                \exists \boldsymbol{q}^\star, \boldsymbol{\theta}^\star, \forall \boldsymbol{x},y, \; p(y | \boldsymbol{x}) = p_{\boldsymbol{\theta}^\star}(y | \boldsymbol{x});$$</span><br /> and under standard regularity conditions and with a suitable estimation procedure (see later for the proposed estimation procedure), the maximum likelihood framework would ensure the consistency of <span class="math inline">\((\boldsymbol{q}_{\hat{\boldsymbol{\alpha}}}, \hat{\boldsymbol{\theta}})\)</span> towards <span class="math inline">\((\boldsymbol{q}^\star,\boldsymbol{\theta}^\star)\)</span> if <span class="math inline">\(\boldsymbol{\alpha}^\star\)</span> s.t. <span class="math inline">\(\boldsymbol{q}_{\boldsymbol{\alpha}^\star} = \boldsymbol{q}\)</span> was an interior point of the parameter space <span class="math inline"><em>A</em></span>. However, as emphasized in the previous paragraph, “<span class="math inline">\(\boldsymbol{\alpha}^\star = + \infty\)</span>” such that the maximum likelihood parameter is on the edge of the parameter space which hinders asymptotic properties (<em>e.g.</em> normality) in some settings <a href="#10.2307/2289471" class="citation" data-cites="10.2307/2289471">(24)</a>, but not “convergence” on which we focus here. We did not investigate this issue further since numerical experiments showed consistency: from an empirical point of view, we will see in Section <a href="#sec:experiments" data-reference-type="ref" data-reference="sec:experiments">1.6</a> and in particular in Figure <a href="#fig:MAP" data-reference-type="ref" data-reference="fig:MAP">[fig:MAP]</a>, that the smooth approximation <span class="math inline">\(\boldsymbol{q}_{\hat{\boldsymbol{\alpha}}}\)</span> converges towards “hard” quantizations <span class="math inline">\(\boldsymbol{q}\)</span>.</p>
                                            <p>However, and as is usual, the log-likelihood <span class="math inline">\(\ell_{\boldsymbol{q}_{\boldsymbol{\alpha}}}(\boldsymbol{\theta},(\boldsymbol{\mathbf{x}},\boldsymbol{\mathbf{y}}))\)</span> cannot be directly maximized w.r.t. <span class="math inline">\((\boldsymbol{\alpha},\boldsymbol{\theta})\)</span>, so that we need an iterative procedure. To this end, the next section introduces a neural network of suitable architecture.</p>
                                            
                                            <h3 id="sec:estim">A neural network-based estimation strategy</h3>
                                            <h5 id="neural-network-architecture">Neural network architecture</h5>
                                            <p>To estimate parameters <span class="math inline">\(\boldsymbol{\alpha}\)</span> and <span class="math inline">\(\boldsymbol{\theta}\)</span> in the relaxed logistic regression model, a particular neural network architecture can be used. We shall insist that this network is only a way to use common deep learning frameworks, namely Tensorflow <a href="#tensorflow2015-whitepaper" class="citation" data-cites="tensorflow2015-whitepaper">(25)</a> through the high-level API Keras <a href="#chollet2015keras" class="citation" data-cites="chollet2015keras">(26)</a> instead of building a gradient descent algorithm from scratch to optimize <span class="math inline">\(\ell_{\boldsymbol{q}_{\boldsymbol{\alpha}}}(\boldsymbol{\theta},(\boldsymbol{\mathbf{x}},\boldsymbol{\mathbf{y}}))\)</span>. The most obvious part is the output layer that must produce <span class="math inline">\(p_{\boldsymbol{\theta}}(1|\boldsymbol{q}_{\boldsymbol{\alpha}}(\boldsymbol{x}))\)</span> which is equivalent to a densely connected layer with a sigmoid activation (the reciprocal function of logit).</p>
                                            <p>For a continuous feature <span class="math inline">\(x_j\)</span> of <span class="math inline">\(\boldsymbol{x}\)</span>, the combined use of <span class="math inline">\(m_j\)</span> neurons including affine transformations and softmax activation obviously yields <span class="math inline">\(\boldsymbol{q}_{\boldsymbol{\alpha}_{j}}(x_j)\)</span>. Similarly, an input categorical feature <span class="math inline">\(x_j\)</span> with <span class="math inline">\(l_j\)</span> levels is equivalent to <span class="math inline">\(l_j\)</span> binary input neurons (presence or absence of the factor level). These <span class="math inline">\(l_j\)</span> neurons are densely connected to <span class="math inline">\(m_j\)</span> neurons without any bias term and a softmax activation. The softmax outputs are next aggregated via the summation, say <span class="math inline">\(\Sigma_{\boldsymbol{\theta}}\)</span> for short, and then the sigmoid function <span class="math inline"><em>σ</em></span> gives the final output. All in all, the proposed model is straightforward to optimize with a simple neural network, as shown in Figure <a href="#fig:nn" data-reference-type="ref" data-reference="fig:nn">[fig:nn]</a>.</p>
                                            
                                            <h5 id="stochastic-gradient-descent-as-a-quantization-provider">Stochastic gradient descent as a quantization provider</h5>
                                            <p>By relying on stochastic gradient descent, the smoothed likelihood <span class="math inline">\(\ell_{\boldsymbol{q}_{\boldsymbol{\alpha}}}(\boldsymbol{\theta},(\boldsymbol{\mathbf{x}},\boldsymbol{\mathbf{y}}))\)</span> can be maximized over <span class="math inline">\(\left(\boldsymbol{\alpha}, \boldsymbol{\theta} \right)\)</span>. Due to its convergence properties <a href="#bottou2010large" class="citation" data-cites="bottou2010large">(27)</a>, the results should be close to the maximizers of the original likelihood <span class="math inline">\(\ell_{\boldsymbol{q}}\)</span> if the model is well-specified, when there is a true underlying quantization. However, in the misspecified model case, there is no such guarantee. Therefore, to be more conservative, we evaluate at each training epoch <span class="math inline">(<em>s</em>)</span> the quantization <span class="math inline">\(\hat{\boldsymbol{q}}^{(s)}\)</span> resulting from the <em>maximum a posteriori</em> procedure, then classically estimate the <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> parameter <em>via</em> maximum likelihood: <br /><span class="math display">$$\label{eq:lr_param_q}
                                                \hat{\boldsymbol{\theta}}^{(s)} = \arg\!\max_{\boldsymbol{\theta}} \ell_{\hat{q}^{(s)}}(\boldsymbol{\theta}; \mathcal{T}_{\text{f}})$$</span><br /> and the resulting <span class="math inline">\(\mbox{BIC}(\hat{\boldsymbol{\theta}}^{(s)})\)</span>. If <span class="math inline"><em>S</em></span> is a given maximum number of iterations of the stochastic gradient descent algorithm, the quantization retained at the end is then determined by the optimal epoch <br /><span class="math display">$$\label{eq:opt_epoch}
                                                    s^\star=\arg\!\min_{s\in \{1,\ldots, S\}} \mbox{BIC}(\hat{\boldsymbol{\theta}}^{(s)}).$$</span><br /> You can think of <span class="math inline"><em>S</em></span> as a computational budget: contrary to classical early stopping rules (<em>e.g.</em> based on validation loss) used in neural network fitting, this network only acts as a stochastic quantization provider which will naturally prevent overfitting. We reiterate that the BIC can be swapped for the user’s favourite model choice criterion.</p>
                                                <p>Lots of optimization algorithms for neural networks have been proposed, which all come with their hyperparameters. As, in the general case, <span class="math inline">\(\ell_{\boldsymbol{q}_{\boldsymbol{\alpha}}}(\boldsymbol{\theta} ; (\boldsymbol{\mathbf{x}},\boldsymbol{\mathbf{y}}))\)</span> is not guaranteed to be convex, there might be several local maxima, such that all these optimization methods might diverge, converge to a different maximum, or at least converge in very different numbers of epochs, as can be examplified in the animation of Figure <a href="#fig:anim_sgd" data-reference-type="ref" data-reference="fig:anim_sgd">[fig:anim_sgd]</a><span><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></span>. We chose the RMSProp method, which showed good results and is one of the standard methods.</p>
                                                <p><span>3</span></p>
                                                <h5 id="choosing-an-appropriate-number-of-levels">Choosing an appropriate number of levels</h5>
                                                <p>Concerning now the number of intervals or factor levels <span class="math inline">\(\boldsymbol{m} = (m_j)_1^d\)</span>, they have also to be estimated since in practice they are unknown. Looping over all candidates <span class="math inline"><strong>m</strong></span> is intractable. But in practice, by relying on the <em>maximum a posteriori</em> procedure, a lot of unseen factor levels might be dropped. Indeed, for a given level <span class="math inline"><em>h</em></span>, all training observations <span class="math inline">\(x_{i,j}\)</span> in <span class="math inline">\(\mathcal{T}_{\text{f}}\)</span> and all other levels <span class="math inline"><em>h</em>′</span>, if <span class="math inline">\(q_{\boldsymbol{\alpha}_{j,h}}(x_{i,j}) &lt; q_{\boldsymbol{\alpha}_{j,h'}}(x_{i,j})\)</span>, then the level <span class="math inline"><em>h</em></span> “vanishes”.</p>
                                                <p>This phenomenon can be witnessed in Figure <a href="#fig:MAP1" data-reference-type="ref" data-reference="fig:MAP1">[fig:MAP1]</a> (algorithm and experiments detailed later) where <span class="math inline"><em>q</em><sub><em>α</em><sub>0, 2</sub></sub></span> is “flat” across the support of <span class="math inline">\(\boldsymbol{\mathbf{x}}_0\)</span> and only two intervals are produced. In practice, we recommend to start with a user-chosen <span class="math inline">\(\boldsymbol{m}=\boldsymbol{m}_{\max}\)</span> and we will see in the experiments of Section <a href="#sec:experiments" data-reference-type="ref" data-reference="sec:experiments">1.6</a> that the proposed approach is able to explore small values of <span class="math inline"><strong>m</strong></span> and to select a value <span class="math inline">\(\hat{\boldsymbol{m}}\)</span> drastically smaller than <span class="math inline"><strong>m</strong><sub>max</sub></span>. This phenomenon, which reduces the computational burden of the quantization task, is also illustrated in Section <a href="#sec:experiments" data-reference-type="ref" data-reference="sec:experiments">1.6</a>.</p>
                                                <h2 id="sec:sem">An alternative <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> approach</h2>
                                                <p>In what follows, the quantization <span class="math inline">\(\boldsymbol{q}(\boldsymbol{x})\)</span> is seen as a latent (unobserved) feature denoted by <span class="math inline">\(\boldsymbol{\mathfrak{q}}\)</span>, which is still the vector of quantizations <span class="math inline">\((\boldsymbol{\mathfrak{q}}_1, \dots, \boldsymbol{\mathfrak{q}}_d)\)</span> of features <span class="math inline">\((x_1, \dots, x_d)\)</span> (see Equation <a href="#eq:qj" data-reference-type="eqref" data-reference="eq:qj">[eq:qj]</a>). These component-wise quantizations are themselves binary-valued vectors <span class="math inline">\((\mathfrak{q}_{j,h})_1^{m_j}\)</span> where <span class="math inline">𝔮<sub><em>j</em>, <em>h</em></sub> ∈ {0, 1}</span> and <span class="math inline">\(\sum_{h=1}^{m_j} \mathfrak{q}_{j,h} = 1\)</span>. We denote by <span class="math inline">\(\boldsymbol{\mathfrak{Q}}_{\boldsymbol{m}}\)</span> the space of such latent features and the <span class="math inline"><em>n</em></span>-sample of latent quantizations corresponding to <span class="math inline">\(\boldsymbol{q}(\boldsymbol{\mathbf{x}}_\text{f})\)</span> by <span class="math inline">\(\boldsymbol{\mathbf{q}}\)</span>, <em>i.e.</em> the matrix of all <span class="math inline"><em>n</em></span> quantizations.</p>
                                                <p>In the following section, we translate earlier assumptions on the function <span class="math inline">\(\boldsymbol{q}(\cdot)\)</span> in probabilistic terms for the latent featutre <span class="math inline">\(\boldsymbol{\mathfrak{q}}\)</span>. In the subsequent section, we make good use of these assumptions to provide a continuous relaxation of the quantization problem, as was empirically argumented in Section <a href="#subsec:relaxation" data-reference-type="ref" data-reference="subsec:relaxation">1.4.1</a>. The main reason to resort to this formulation as a latent feature problem will be made clear in Section <a href="#subsec:stoch" data-reference-type="ref" data-reference="subsec:stoch">1.5.3</a>, where we provide a stochastic estimation algorithm for the latent feature <span class="math inline">\(\boldsymbol{\mathfrak{q}}\)</span>.</p>
                                                <h3 id="probabilistic-assumptions-regarding-the-quantization-latent-feature">Probabilistic assumptions regarding the quantization latent feature</h3>
                                                <p>Firstly, only the well-specified model case is considered. This hypothesis translates with this new latent feature as a probabilistic assumption: <br /><span class="math display">$$\label{hyp:true}
                                                    \exists \boldsymbol{\theta}^\star, \boldsymbol{\mathfrak{q}}^\star \text{s.t.} Y \sim p_{\boldsymbol{\theta}^\star}(\cdot | \boldsymbol{\mathfrak{q}}^\star).$$</span><br /> Secondly, the result of the quantization is assumed to be “self-contained” w.r.t. the predictive information in <span class="math inline">\(\boldsymbol{x}\)</span>, <em>i.e.</em> it is assumed that all available information about <span class="math inline"><em>y</em></span> in <span class="math inline">\(\boldsymbol{x}\)</span> has been “squeezed” by quantizing the data: <br /><span class="math display">$$\label{hyp:squeeze}
                                                        \forall \boldsymbol{x},y,\: p(y|\boldsymbol{x},\boldsymbol{\mathfrak{q}}) = p(y|\boldsymbol{\mathfrak{q}}).$$</span><br /> Thirdly, the component-wise nature of the quantization can be stated as a conditional independence assumption: <br /><span class="math display">$$\label{hyp:component}
                                                            \forall \boldsymbol{x},\boldsymbol{\mathfrak{q}},\: p(\boldsymbol{\mathfrak{q}}|\boldsymbol{x}) = \prod_{j=1}^d p(\boldsymbol{\mathfrak{q}}_j | x_j).$$</span><br /> These two hypotheses are also implicitly assumed in the neural network approach by looking at the computational graph of Figure <a href="#fig:nn" data-reference-type="ref" data-reference="fig:nn">[fig:nn]</a>.</p>
                                                        <h3 id="subsec:fuzzy">Continuous relaxation of the quantization as seen as fuzzy assignment</h3>
                                                        <p>If we consider the deterministic discretization scheme defined in Section <a href="#sec:model_selection" data-reference-type="ref" data-reference="sec:model_selection">1.3</a>, we have, analogous to Equation <a href="#eq:qj" data-reference-type="eqref" data-reference="eq:qj">[eq:qj]</a>: <br /><span class="math display">$$p(\boldsymbol{\mathfrak{q}}_j = e^h_{m_j} | x_j) = 1 \text{ if } x_j \in C_{j,h},$$</span><br /> which is a step function. Rewriting <span class="math inline">\(p(y| \boldsymbol{x})\)</span> by integrating over these new latent features, we get: <br /><span class="math display">$$\begin{aligned}
                                                            p(y | \boldsymbol{x}) &amp; = p_{\boldsymbol{\theta}^\star}(y | \boldsymbol{\mathfrak{q}}^\star) &amp; \\
                                                            &amp; = \sum_{\boldsymbol{\mathfrak{q}} \in \boldsymbol{\mathfrak{Q}}_{\boldsymbol{m}}} p(y, \boldsymbol{\mathfrak{q}} | \boldsymbol{x}) \\
                                                            &amp; = \sum_{\boldsymbol{\mathfrak{q}} \in \boldsymbol{\mathfrak{Q}}_{\boldsymbol{m}}} p(y | \boldsymbol{\mathfrak{q}}, \boldsymbol{x}) p(\boldsymbol{\mathfrak{q}} | \boldsymbol{x}) \\
                                                            &amp; = \sum_{\boldsymbol{\mathfrak{q}} \in \boldsymbol{\mathfrak{Q}}_{\boldsymbol{m}}} p(y | \boldsymbol{\mathfrak{q}}) p(\boldsymbol{\mathfrak{q}} | \boldsymbol{x}) &amp; \\
                                                            &amp; = \sum_{\boldsymbol{\mathfrak{q}} \in \boldsymbol{\mathfrak{Q}}_{\boldsymbol{m}}} p(y | \boldsymbol{\mathfrak{q}}) \prod_{j=1}^d p(\boldsymbol{\mathfrak{q}}_j | x_j). &amp; \end{aligned}$$</span><br /> This sum over <span class="math inline">\(\boldsymbol{\mathfrak{Q}}_{\boldsymbol{m}}\)</span> is intractable. The well-specified model hypothesis yields for all <span class="math inline">\(x_j\)</span>, <span class="math inline">\(p(\boldsymbol{\mathfrak{q}}_j^\star | x_j) = 1\)</span>. Conversely, for <span class="math inline">\(\boldsymbol{\mathfrak{q}} \in \boldsymbol{Q}\)</span> such that <span class="math inline">\(\boldsymbol{\mathfrak{q}} \not{\mathcal{R}_{\mathcal{T}_n}} \boldsymbol{\mathfrak{q}}^\star\)</span>, there exists a feature <span class="math inline"><em>j</em></span> and an observation <span class="math inline">\(x_{i,j}\)</span> such that <span class="math inline">\(p(\boldsymbol{\mathfrak{q}}_j | x_j) = 0\)</span>. Consequently, the above sum, over all training observations in <span class="math inline">\(\mathcal{T}_n\)</span>, reduces to: <br /><span class="math display">$$\begin{aligned}
                                                                p(\boldsymbol{\mathbf{y}}_\text{f} | \boldsymbol{\mathbf{x}}_\text{f}) &amp; = \prod_{i=1}^n p(y_i | \boldsymbol{x}_i) \\
                                                                &amp; = \sum_{\boldsymbol{\mathfrak{q}} \in \boldsymbol{Q}} \prod_{i=1}^n p(y_i | \boldsymbol{\mathfrak{q}}_i) \prod_{j=1}^d p(\boldsymbol{\mathfrak{q}}_{i,j} | x_{i,j}) \\
                                                                &amp; = \prod_{i=1}^n p(y_i | \boldsymbol{\mathfrak{q}}_i^\star) \prod_{j=1}^d p(\boldsymbol{\mathfrak{q}}_{i,j}^\star | x_{i,j}).\end{aligned}$$</span><br /> Thus, we have : <br /><span class="math display">$$\boldsymbol{\mathfrak{q}}^\star = \arg\!\max_{\boldsymbol{\mathfrak{q}} \in \boldsymbol{Q}} \prod_{i=1}^n p(y_i | \boldsymbol{\mathfrak{q}}_i ) \prod_{j=1}^d p(\boldsymbol{\mathfrak{q}}_{i,j} | x_{i,j}).$$</span><br /> This new formulation of the best quantization is still intractable since it requires to evaluate all quantizations in <span class="math inline">\(\boldsymbol{Q}\)</span>, although all terms except <span class="math inline">\(\boldsymbol{\mathfrak{q}}^\star\)</span> contribute to <span class="math inline">0</span> in the above <span class="math inline">\(\arg\!\max\)</span>. In the misspecfied model-case however, there is no such guarantee but it can still be claimed that the best candidate <span class="math inline">\(\boldsymbol{\mathfrak{q}}^\star\)</span> in terms of BIC criterion dominates the sum.</p>
                                                            <p>Our goal in the next section is to generate good candidates <span class="math inline">\(\boldsymbol{\mathbf{q}}\)</span> as in Section <a href="#sec:estim" data-reference-type="ref" data-reference="sec:estim">1.4.2</a>. Among other things detailed later on, models for <span class="math inline">\(p(y | \boldsymbol{\mathfrak{q}})\)</span> and <span class="math inline">\(p(\boldsymbol{\mathfrak{q}}_j | x_j)\)</span> shall be proposed. A stochastic “quantization provider” is designed as in the previous section. Following arguments of the preceding paragraph, its empirical distribution of generated candidates shall be dominated by <span class="math inline">\(\boldsymbol{q}^\star\)</span>, which, as in Section <a href="#sec:proposal" data-reference-type="ref" data-reference="sec:proposal">1.4</a> with the neural network approach, can be selected with the BIC criterion. It seems natural to use a <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> for <span class="math inline">\(p( y | \boldsymbol{\mathfrak{q}}_j)\)</span>. Following Section <a href="#sec:proposal" data-reference-type="ref" data-reference="sec:proposal">1.4</a> and as was empirically argumented in Section <a href="#subsec:relaxation" data-reference-type="ref" data-reference="subsec:relaxation">1.4.1</a>, the instrumental distribution <span class="math inline">\(p(\boldsymbol{\mathfrak{q}}_j | x_j)\)</span> will take a similar form as <span class="math inline">\(\boldsymbol{q}_{\boldsymbol{\alpha}}\)</span>. However, contrary to the neural network approach which iteratively optimizes <span class="math inline">\(\boldsymbol{\theta}\)</span> given the “fuzzy” quantization <span class="math inline">\(\boldsymbol{q}_{\boldsymbol{\alpha}}\)</span> (continuous values in <span class="math inline">]0; 1[</span> for all its values), this approach iteratively draws candidates <span class="math inline">\(\boldsymbol{\mathfrak{q}}\)</span> (where only one of its entries is equal to <span class="math inline">1</span> for each feature, all others are equal to <span class="math inline">0</span>) which we call a “stochastic” quantization.</p>
                                                            <p><span><strong>For a continuous feature</strong></span>, we resort to a polytomous logistic regression, similar to the softmax function without the over-parametrization (one level per feature <span class="math inline"><em>j</em></span>, say <span class="math inline">\(m_j\)</span>, is considered reference): <br /><span class="math display">$$p_{\boldsymbol{\alpha}_{j,h}}(\boldsymbol{\mathfrak{q}}_j = e^h_{m_j} | x_j) = \begin{cases} \frac{1}{\sum_{h'=1}^{m_j-1} \exp(\alpha_{j,h'}^0 + \alpha_{j,h'}^1 x_j)} \text{ if } h = m_j, \\ \frac{\alpha_{j,h}^0 + \alpha_{j,h}^1 x_j}{\sum_{h'=1}^{m_j-1} \exp(\alpha_{j,h'}^0 + \alpha_{j,h'}^1 x_j)} \text{ otherwise.} \end{cases}$$</span><br /> <span><strong>For categorical features</strong></span>, simple contingency tables are employed: <br /><span class="math display">$$p_{\boldsymbol{\alpha}_{j,h}^o}(\boldsymbol{\mathfrak{q}}_j = e^h_{m_j} | x_j) = \boldsymbol{\alpha}_{j,h}^o.$$</span><br /> Similarly, <span class="math inline">\(p_{\boldsymbol{\alpha}_j}(\boldsymbol{\mathfrak{q}}_j | x_j)\)</span> are no more step functions but smooth functions as in Figure <a href="#fig:MAP" data-reference-type="ref" data-reference="fig:MAP">[fig:MAP]</a>.</p>
                                                            <h5 id="remark-on-polytomous-logistic-regressions">Remark on polytomous logistic regressions</h5>
                                                            <p>Since the resulting latent categorical feature can be interpreted as an ordered categorical features (the <em>maximum a posterior</em> operation yields contiguous intervals as argued in Section <a href="#subsec:relaxation" data-reference-type="ref" data-reference="subsec:relaxation">1.4.1</a>), ordinal “parallel” <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> <a href="#o2006logistic" class="citation" data-cites="o2006logistic">(28)</a> could be used (provided levels <span class="math inline"><em>h</em></span> are reordered). This particular model is of the form: <br /><span class="math display">$$\ln \frac{p(\boldsymbol{\mathfrak{q}}_{j} = \boldsymbol{e}_{h+1}^{m_j} | x_j)}{p(\boldsymbol{\mathfrak{q}}_{j} = e^h_{m_j} | x_j)} = \alpha_{j,h,0} + \alpha_{j} x_j, 1 \leq h &lt; m_j,$$</span><br /> which restricts the number of parameters since all levels <span class="math inline"><em>h</em></span> share the same slope <span class="math inline"><em>α</em><sub><em>j</em></sub></span>. Its advantages lie in the fact that it might lead to sharper door functions quicker, and that it has fewer parameters to estimate, thus reducing <em>de facto</em> the estimation variance of each “soft” quantization <span class="math inline">\(p_{\boldsymbol{\alpha}_j}\)</span>. However, it makes it harder for levels to “vanish” which would require to iterate over the number of levels per feature <span class="math inline">\(m_j\)</span> which we wanted to avoid. In practice, it yielded similar results to polytomous <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> such that they remain a parameter of the <span class="sans-serif">R</span> package <em>glmdisc</em>.</p>
                                                            <h3 id="subsec:stoch">Stochastic search of the best quantization</h3>
                                                            <p>We parametrized <span class="math inline">\(p(y|\boldsymbol{x})\)</span> as: <br /><span class="math display">$$p(y | \boldsymbol{x}; \boldsymbol{\theta}, \boldsymbol{\alpha}) = \sum_{\boldsymbol{\mathfrak{q}} \in \boldsymbol{Q}} p_{\boldsymbol{\theta}}(y | \boldsymbol{\mathfrak{q}}) \prod_{j=1}^d p_{\boldsymbol{\alpha}_j}(\boldsymbol{\mathfrak{q}}_j | \boldsymbol{\mathbf{x}}_j).$$</span><br /> A straightforward way to maximize the likelihood of <span class="math inline">\(p(y | \boldsymbol{x}; \boldsymbol{\theta}, \boldsymbol{\alpha})\)</span> in <span class="math inline">\((\boldsymbol{\theta}, \boldsymbol{\alpha})\)</span>, as was done in Section <a href="#sec:proposal" data-reference-type="ref" data-reference="sec:proposal">1.4</a>, to deduce <span class="math inline">\(\boldsymbol{q}^\star\)</span> from <span class="math inline">\(\boldsymbol{\alpha}\)</span> <em>via</em> the <span class="math inline">\(\arg\!\max\)</span> operation (see Section <a href="#subsec:relaxation" data-reference-type="ref" data-reference="subsec:relaxation">1.4.1</a>), is to use an <span data-acronym-label="em" data-acronym-form="singular+short">EM</span> algorithm <span class="citation" data-cites="dempster1977maximum"></span>.</p>
                                                            <p>However, maximizing this likelihood directly is intractable as the Expectation step requires to sum over <span class="math inline">\(\boldsymbol{\mathfrak{q}} \in \boldsymbol{Q}\)</span>:</p>
                                                            <h5 id="e-step">E-step:</h5>
                                                            <p><br /><span class="math display">$$t_{i,\boldsymbol{\mathfrak{q}}}^{(s)} = \frac{p_{\boldsymbol{\theta}^{(s)}}(y_i | \boldsymbol{\mathfrak{q}}) \prod_{j=1}^d p_{\boldsymbol{\alpha}_j^{(s)}}(\boldsymbol{\mathfrak{q}}_j | x_{i,j})}{\sum_{\boldsymbol{\mathfrak{q}} ' \in \boldsymbol{Q}_{\boldsymbol{m}}} p_{\boldsymbol{\theta}^{(s)}}(y_i | \boldsymbol{\mathfrak{q}}) \prod_{j=1}^d p_{\boldsymbol{\alpha}_j^{(s)}}(\boldsymbol{\mathfrak{q}}_j'  | x_{i,j})}.$$</span><br /></p>
                                                            <p>Classically, the <span data-acronym-label="em" data-acronym-form="singular+short">EM</span> can be replaced by the <span data-acronym-label="sem" data-acronym-form="singular+long">SEM</span> <a href="#celeux_sem" class="citation" data-cites="celeux_sem">(29)</a> algorithm: the expectation (the sum over <span class="math inline">\(\boldsymbol{\mathfrak{q}} \in \boldsymbol{Q}\)</span>) is approximated by the empirical distribution (up to a normalization constant) of draws <span class="math inline">\(\boldsymbol{\mathfrak{q}}^{(1)}, \dots, \boldsymbol{\mathfrak{q}}^{(S)}\)</span> from <span class="math inline">\(p_{\boldsymbol{\theta}}(y | \cdot) \prod_{j=1}^d p_{\boldsymbol{\alpha}_j}(\cdot | x_j)\)</span>.</p>
                                                            <h4 id="subsubsec:multinouilli"><span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> as a quantization provider</h4>
                                                            <p>As the parameters <span class="math inline">\(\boldsymbol{\alpha}\)</span> of <span class="math inline">\(\boldsymbol{q}_{\boldsymbol{\alpha}}\)</span> were initialized randomly in the neural network approach, the latent features observations <span class="math inline">\(\boldsymbol{\mathbf{q}}^{(0)}\)</span> are initialized uniformly (<em>i.e.</em> by sampling from an equiprobable multinouilli distribution). At step <span class="math inline"><em>s</em></span>, the <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm allows us to draw <span class="math inline">\(\boldsymbol{\mathbf{q}}^{(s)}\)</span>. As the <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> <span class="math inline">\(p_{\boldsymbol{\theta}}(y | \boldsymbol{\mathfrak{q}})\)</span> is multivariate, it is hard to sample simultaneously all latent features. We have to resort to the Gibbs-sampler <a href="#casella1992explaining" class="citation" data-cites="casella1992explaining">(30)</a>: <span class="math inline">\(\boldsymbol{\mathfrak{q}}_j\)</span> is sampled while holding latent features <span class="math inline">\(\boldsymbol{\mathfrak{q}}_{-\{j\}} = (\boldsymbol{\mathfrak{q}}_1, \dots, \boldsymbol{\mathfrak{q}}_{j-1}, \boldsymbol{\mathfrak{q}}_{j+1}, \dots, \boldsymbol{\mathfrak{q}}_d)\)</span> fixed:</p>
                                                            <h5 id="s-step">S-step:</h5>
                                                            <p><br /><span class="math display">$$\label{eq:q_draw}
                                                                \boldsymbol{\mathfrak{q}}_j^{(s)} \sim p_{\hat{\boldsymbol{\theta}}^{(s-1)}}(y | \boldsymbol{\mathfrak{q}}_{-\{j\}}^{(s-1)}, \cdot) p_{\hat{\boldsymbol{\alpha}}_j^{(s-1)}}(\cdot | x_j).$$</span><br /> This process is repeated for all features <span class="math inline">1 ≤ <em>j</em> ≤ <em>d</em></span>.</p>
                                                            <p>Using these latent features, we can compute the <span data-acronym-label="mle" data-acronym-form="singular+short">MLE</span> <span class="math inline">\({\boldsymbol{\theta}}^{(s)}\)</span> (resp. <span class="math inline">\({\boldsymbol{\alpha}}^{(s)}\)</span>) of <span class="math inline">\(\boldsymbol{\theta}\)</span> (resp. <span class="math inline">\(\boldsymbol{\alpha}\)</span>) given <span class="math inline">\(\boldsymbol{\mathbf{q}}^{(s)}\)</span> by maximizing the following likelihoods (M-steps):</p>
                                                            <h5 id="m-steps">M-steps:</h5>
                                                            <p><br /><span class="math display">$$\begin{aligned}
                                                                {\boldsymbol{\theta}}^{(s)} &amp; = \arg\!\max_{\boldsymbol{\theta}} \ell(\boldsymbol{\theta}; \boldsymbol{\mathbf{q}}^{(s)}, \boldsymbol{\mathbf{y}}_\text{f}) &amp;&amp; = \arg\!\max_{\boldsymbol{\theta}} \sum_{i=1}^n \ln p_{\boldsymbol{\theta}}(y_i | \boldsymbol{\mathfrak{q}}^{(s)}_i), \nonumber \\
                                                                {\boldsymbol{\alpha}_j}^{(s)} &amp; = \arg\!\max_{\boldsymbol{\alpha}_j} \ell(\boldsymbol{\alpha}_j; \boldsymbol{\mathbf{x}}_{\text{f},j}, \boldsymbol{\mathbf{q}}^{(s)}_j) &amp;&amp; = \arg\!\max_{\boldsymbol{\alpha}_j} \sum_{i=1}^n \ln p_{\boldsymbol{\alpha}_j}(\boldsymbol{\mathfrak{q}}^{(s)}_{i,j} | x_{i,j}) \text{ for } 1 \leq j \leq d. \label{eq:mle_ag}\end{aligned}$$</span><br /></p>
                                                            <h5 id="remark-on-their-optimization-algorithms">Remark on their optimization algorithms</h5>
                                                            <p>The <span data-acronym-label="mle" data-acronym-form="singular+short">MLE</span> of <span class="math inline">\(\boldsymbol{\theta}\)</span> is obtained, as in the preceding sections and as was thoroughly discussed in Chapter <a href="#chap1" data-reference-type="ref" data-reference="chap1">[chap1]</a>, <em>via</em> Newton-Raphson. For polytomous logistic regression, the same optimization procedure can be used. For categorical features, the <span data-acronym-label="mle" data-acronym-form="singular+short">MLE</span> is simply the proportion of observations in each category: <br /><span class="math display">$$\hat{\boldsymbol{\alpha}}_{j,h}^o = \frac{|\boldsymbol{\mathbf{q}}_{j,h}|}{|\{x_j=o\}|} \text{ for } 1 \leq o \leq l_j.$$</span><br /></p>
                                                            <p>This <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> provides parameters <span class="math inline">\({\boldsymbol{\alpha}}^{(1)}, \dots, {\boldsymbol{\alpha}}^{(S)}\)</span> which can be used to produce <span class="math inline">\(\hat{\boldsymbol{q}}^{(1)}, \dots, \hat{\boldsymbol{q}}^{(S)}\)</span> following the <em>maximum a posteriori</em> scheme, adapted to this new formulation: <br /><span class="math display">$$\hat{\boldsymbol{q}}_j^{(s)}(\cdot) = \arg\!\max_{h} p_{\boldsymbol{\alpha}_j^{(s)}}(e^h_{m_j} | \cdot ) .$$</span><br /> The <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> parameters <span class="math inline">\(\hat{\boldsymbol{\theta}}^{(s)}\)</span> on quantized data are obtained similarly as in <a href="#eq:lr_param_q" data-reference-type="eqref" data-reference="eq:lr_param_q">[eq:lr_param_q]</a>. The best proposed quantization <span class="math inline">\(\boldsymbol{q}^\star\)</span> is thus chosen among them <em>via e.g.</em> the BIC criterion as in Equation <a href="#eq:opt_epoch" data-reference-type="eqref" data-reference="eq:opt_epoch">[eq:opt_epoch]</a>.</p>
                                                            <h4 id="validity-of-the-approach">Validity of the approach</h4>
                                                            <p>The pseudo-completed sample <span class="math inline">\((\boldsymbol{\mathbf{x}}_{\text{f}}, \boldsymbol{\mathbf{q}}^{(s)}, \boldsymbol{\mathbf{y}}_{\text{f}})\)</span> allows to compute <span class="math inline">\(({\boldsymbol{\theta}}^{(s)},{\boldsymbol{\alpha}}^{(s)})\)</span> which does not converge to the <span data-acronym-label="mle" data-acronym-form="singular+short">MLE</span> of <span class="math inline">\(p(\boldsymbol{\mathbf{y}} | \boldsymbol{\mathbf{x}}; \boldsymbol{\theta}, \boldsymbol{\alpha})\)</span>, for the simple reason that, being random in essence, it does not converge pointwise. From its authors, the <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> is however expected to be directed by the <span data-acronym-label="em" data-acronym-form="singular+short">EM</span> dynamics <a href="#celeux_sem" class="citation" data-cites="celeux_sem">(29)</a> and its empirical distribution converges to the target distribution <span class="math inline">\(p(\boldsymbol{\mathbf{y}} | \boldsymbol{\mathbf{x}}; \boldsymbol{\theta}^\star, \boldsymbol{\alpha}^\star)\)</span> provided such a distribution exists and is unique. This existence is guaranteed by remarking that for all features <span class="math inline"><em>j</em></span>, <span class="math inline">\(p(\boldsymbol{\mathfrak{q}}_j | x_j, \boldsymbol{\mathfrak{q}}^{(s)}_{-\{j\}}, y; \boldsymbol{\theta}, \boldsymbol{\alpha}) \propto p_{\boldsymbol{\theta}}(y | \boldsymbol{\mathfrak{q}}^{(s)}_{-\{j\}}, \boldsymbol{\mathfrak{q}}_j) p_{\boldsymbol{\alpha}_j}(\boldsymbol{\mathfrak{q}}_j | x_j) &gt; 0\)</span> by definition of the <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> and polytomous logistic regressions or the contingency tables respectively. The uniqueness is not guaranteed since levels can disappear and there is an absorbing state (the empty model): this point is detailed in the next section.</p>
                                                            <p>In its original purpose <a href="#celeux_sem" class="citation" data-cites="celeux_sem">(29)</a>, the <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> was employed either to find good starting points for the <span data-acronym-label="em" data-acronym-form="singular+short">EM</span> (<em>e.g.</em> to avoid local maxima) or to propose an estimator of the <span data-acronym-label="mle" data-acronym-form="singular+short">MLE</span> of the target distribution as the mean or the mode of the resulting empirical distribution, eventually after a burn-in phase. However, in our setting, we are not directly interested in the <span data-acronym-label="mle" data-acronym-form="singular+short">MLE</span> but only to the best quantization in the sense of the BIC criterion. The best proposed quantization <span class="math inline">\(\boldsymbol{q}^\star\)</span> is thus chosen among them <em>via</em> the BIC criterion as for the neural network approach.</p>
                                                            <h4 id="par:choosing_sem">Choosing an appropriate number of levels</h4>
                                                            <p>Contrary to the neural network approach developed in Section <a href="#sec:proposal" data-reference-type="ref" data-reference="sec:proposal">1.4</a>, the <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm alternates between drawing <span class="math inline">\(\boldsymbol{\mathbf{q}}^{(s)}\)</span> and fitting <span class="math inline">\({\boldsymbol{\theta}}^{(s)}\)</span> and <span class="math inline">\({\boldsymbol{\alpha}}^{(s)}\)</span> at each step <span class="math inline"><em>s</em></span>. Therefore, additionally to the phenomenon of “vanishing” levels caused by the <em>maximum a posteriori</em> procedure similar to the neural network approach, if a level <span class="math inline"><em>h</em></span> of <span class="math inline">\(\boldsymbol{\mathfrak{q}}\)</span> is not drawn at step <span class="math inline"><em>s</em></span>, then at step <span class="math inline"><em>s</em> + 1</span> when adjusting parameters <span class="math inline">\(\boldsymbol{\alpha}_j\)</span> by maximum likelihood, this level will have disappeared and cannot be drawn again. A Reversible-Jump MCMC approach would be needed <a href="#green1995reversible" class="citation" data-cites="green1995reversible">(31)</a> to “resuscitate” these levels, which is not needed in the neural network approach because its architecture is fixed in advance. As a consequence, with a design matrix of fixed size <span class="math inline"><em>n</em></span>, there is a non-zero probability that for any given feature, any of its levels collapses at each step such that <span class="math inline">\(m_j^{(s+1)} = m_j^{(s)} - 1\)</span>.</p>
                                                            <p>The MCMC has thus an absorbing state for which all features are quantized into one level (the empty model with no features) which is reached in a finite number of steps (although very high if <span class="math inline"><em>n</em></span> is sufficiently large as is the case with <em>Credit Scoring</em> data). The <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm is an effective way to start from a high number of levels per feature <span class="math inline">\(\boldsymbol{m}_{\max}\)</span> and explore smaller values.</p>
                                                            <p>The full algorithm is described schematically in Figure <a href="#fig:schema_sem" data-reference-type="ref" data-reference="fig:schema_sem">[fig:schema_sem]</a>.</p>

                                                            
                                                            <h2 id="sec:experiments">Numerical experiments</h2>
                                                            <p>This section is divided into three complementary parts to assess the validity of our proposal, that is called hereafter <em>glmdisc</em>-NN and <em>glmdisc</em>-SEM, designating respectively the approaches developed in Sections <a href="#sec:proposal" data-reference-type="ref" data-reference="sec:proposal">1.4</a> and <a href="#sec:sem" data-reference-type="ref" data-reference="sec:sem">1.5</a>. First, simulated data are used to evaluate its ability to recover the true data generating mechanism. Second, the predictive quality of the new learned representation approach is illustrated on several classical benchmark datasets from the UCI library. Third, we use it on <em>Credit Scoring</em> datasets provided by <span data-acronym-label="cacf" data-acronym-form="singular+short">CACF</span>.</p>
                                                            <h3 id="simulated-data-empirical-consistency-and-robustness">Simulated data: empirical consistency and robustness</h3>
                                                            <p>Focus is here given on discretization of continuous features (similar experiments could be conducted on categorical ones). Two continuous features <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> are sampled from the uniform distribution on <span class="math inline">[0, 1]</span> and discretized as exemplified on Figure <a href="#fig:exp_sim" data-reference-type="ref" data-reference="fig:exp_sim">[fig:exp_sim]</a> by using <br /><span class="math display">$$\boldsymbol{q}_1(\cdot)=\boldsymbol{q}_2(\cdot) = ({1}_{]-\infty,1/3]}(\cdot),{1}_{]1/3,2/3]}(\cdot),{1}_{]2/3,\infty[}(\cdot)).$$</span><br /> Here, we have <span class="math inline"><em>d</em> = 2</span> and <span class="math inline"><em>m</em><sub>1</sub> = <em>m</em><sub>2</sub> = 3</span> and the cutpoints are <span class="math inline"><em>c</em><sub><em>j</em>, 1</sub> = 1/3</span> and <span class="math inline"><em>c</em><sub><em>j</em>, 2</sub> = 2/3</span> for <span class="math inline"><em>j</em> = 1, 2</span>. Setting <span class="math inline">\(\boldsymbol{\theta}=(0,-2,2,0,-2,2,0)\)</span>, the target feature <span class="math inline"><em>y</em></span> is then sampled from <span class="math inline">\(p_{\boldsymbol{\theta}}(\cdot | \boldsymbol{q}(\boldsymbol{\mathbf{x}}))\)</span> via the logistic model.</p>
                                                            
                                                            <p>From the <em>glmdisc</em> algorithm, we studied three cases:</p>
                                                            <ol type="a">
                                                                <li><p>First, the quality of the cutoff estimator <span class="math inline"><em>ĉ</em><sub><em>j</em>, 2</sub></span> of <span class="math inline"><em>c</em><sub><em>j</em>, 2</sub> = 2/3</span> is assessed when the starting maximum number of intervals per discretized continuous feature is set to its true value <span class="math inline"><em>m</em><sub>1</sub> = <em>m</em><sub>2</sub> = 3</span>;</p></li>
                                                                <li><p>Second, we estimated the number of intervals <span class="math inline"><em>m̂</em><sub>1</sub></span> of <span class="math inline"><em>m</em><sub>1</sub> = 3</span> when the starting maximum number of intervals per discretized continuous feature is set to <span class="math inline"><em>m</em><sub>max</sub> = 10</span>;</p></li>
                                                                <li><p>Last, we added a third feature <span class="math inline">\(x_3\)</span> also drawn uniformly on <span class="math inline">[0, 1]</span> but uncorrelated to <span class="math inline">\(y\)</span> and estimated the number <span class="math inline"><em>m̂</em><sub>3</sub></span> of discretization intervals selected for <span class="math inline">\(x_3\)</span>. The reason is that a non-predictive feature which is discretized or grouped into a single value is <em>de facto</em> excluded from the model, and this is a positive side effect.</p></li>
                                                            </ol>
                                                            <p>From a statistical point of view, experiment (a) assesses the empirical consistency of the estimation of <span class="math inline"><em>C</em><sub><em>j</em>, <em>h</em></sub></span>, whereas experiments (b) and (c) focus on the consistency of the estimation of <span class="math inline">\(m_j\)</span>. The results are summarized in Table <a href="#tab:estim_precision" data-reference-type="ref" data-reference="tab:estim_precision">[tab:estim_precision]</a> where 95% confidence intervals (CI <a href="#sun2014fast" class="citation" data-cites="sun2014fast">(32)</a>) are given, with a varying sample size. Note in particular that the slight underestimation in (b) is a classical consequence of the BIC criterion on small samples.</p>
                                                            <p>The neural network approach <em>glmdisc</em>-NN seems to outperform the <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> approach on experiments (b) and (c) where the true number of levels <span class="math inline">\(\boldsymbol{m}\)</span> has to be estimated. This is rather surprising since theoretically, the <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> approach can explore the model space easier than <em>glmdisc</em>-SEM thanks to the additional “disappearing” effect of the drawing procedure (the absorbing state of the MCMC: see Section <a href="#subsec:stoch" data-reference-type="ref" data-reference="subsec:stoch">1.5.3</a>). This inferior performance is somewhat confirmed with real data in the subsequent sections. A rough guess about this performance drop with an equivalent computational budget <span class="math inline"><em>S</em></span> is the “noise” introduced by drawing <span class="math inline">\(\boldsymbol{\mathfrak{q}}\)</span> rather than maximizing directly the log-likelihood <span class="math inline">\(\ell_{\boldsymbol{q},\boldsymbol{\alpha}}\)</span> which can be achieved by <em>glmdisc</em>-NN through gradient descent. Therefore, <em>glmdisc</em>-SEM might need way more iterations than <em>glmdisc</em>-NN to converge, especially in a misspecified model setting.</p>
                                                            
                                                            <p><span id="tab:estim_precision" label="tab:estim_precision">[tab:estim_precision]</span></p>
                                                            <table>
                                                                <caption>For <em>glmdisc</em>-NN and <em>glmdisc</em>-SEM and different sample sizes <span class="math inline"><em>n</em></span>, (a) CI of <span class="math inline"><em>ĉ</em><sub><em>j</em>, 2</sub></span> for <span class="math inline"><em>c</em><sub><em>j</em>, 2</sub> = 2/3</span>. (b) Bar plot of <span class="math inline"><em>m̂</em> = 2, 3, 4</span> (resp.) for <span class="math inline"><em>m</em><sub>1</sub> = 3</span>. (c) Bar plot of <span class="math inline"><em>m̂</em><sub>3</sub> = 1, 2, 3</span> (resp.) for <span class="math inline"><em>m</em><sub>3</sub> = 1</span> with a computational budget <span class="math inline"><em>S</em> = 500</span> iterations.</caption>
                                                                <thead>
                                                                    <tr class="header">
                                                                        <th style="text-align: left;">Algorithm</th>
                                                                        <th style="text-align: left;"><span class="math inline"><em>n</em></span></th>
                                                                        <th style="text-align: left;">(a) <span class="math inline"><em>ĉ</em><sub><em>j</em>, 2</sub></span></th>
                                                                        <th style="text-align: left;">(b)</th>
                                                                        <th style="text-align: left;"><span class="math inline"><em>m̂</em><sub>1</sub></span></th>
                                                                        <th style="text-align: left;">(c)</th>
                                                                        <th style="text-align: left;"><span class="math inline"><em>m̂</em><sub>3</sub></span></th>
                                                                    </tr>
                                                                </thead>
                                                                <tbody>
                                                                    <tr class="odd">
                                                                        <td style="text-align: left;"><em>glmdisc</em>-NN</td>
                                                                        <td style="text-align: left;"><span class="math inline">1, 000</span></td>
                                                                        <td style="text-align: left;"><span class="math inline">[0.656, 0.666]</span></td>
                                                                        <td style="text-align: left;"></td>
                                                                        <td style="text-align: left;"></td>
                                                                        <td style="text-align: left;"></td>
                                                                        <td style="text-align: left;"></td>
                                                                    </tr>
                                                                    <tr class="even">
                                                                        <td style="text-align: left;"><em>glmdisc</em>-SEM</td>
                                                                        <td style="text-align: left;"><span class="math inline">1, 000</span></td>
                                                                        <td style="text-align: left;"><span class="math inline">[0.664, 0.669]</span></td>
                                                                        <td style="text-align: left;"></td>
                                                                        <td style="text-align: left;"></td>
                                                                        <td style="text-align: left;"></td>
                                                                        <td style="text-align: left;"></td>
                                                                    </tr>
                                                                    <tr class="odd">
                                                                        <td style="text-align: left;"><em>glmdisc</em>-NN</td>
                                                                        <td style="text-align: left;"><span class="math inline">10, 000</span></td>
                                                                        <td style="text-align: left;"><span class="math inline">[0.666, 0.666]</span></td>
                                                                        <td style="text-align: left;"></td>
                                                                        <td style="text-align: left;"></td>
                                                                        <td style="text-align: left;"></td>
                                                                        <td style="text-align: left;"></td>
                                                                    </tr>
                                                                    <tr class="even">
                                                                        <td style="text-align: left;"><em>glmdisc</em>-SEM</td>
                                                                        <td style="text-align: left;"><span class="math inline">10, 000</span></td>
                                                                        <td style="text-align: left;"><span class="math inline">[0.666, 0.666]</span></td>
                                                                        <td style="text-align: left;"></td>
                                                                        <td style="text-align: left;"></td>
                                                                        <td style="text-align: left;"></td>
                                                                        <td style="text-align: left;"></td>
                                                                    </tr>
                                                                </tbody>
                                                            </table>
                                                            <p>To complement these experiments on simulated data following a well-specified model, a similar study can be done for categorical features: 10 levels are drawn uniformly and 3 groups of levels, which share the same log-odd ratio, are created. The same phenomenon as in Table <a href="#tab:estim_precision" data-reference-type="ref" data-reference="tab:estim_precision">[tab:estim_precision]</a> is witnessed: the empirical distribution of the estimated number of groups of levels is peaked at its true value of 3.</p>
                                                            <p>Finally, it was argued in Section <a href="#sec:model_selection" data-reference-type="ref" data-reference="sec:model_selection">1.3</a> that by considering all features when quantizing the data, relying on a multivariate approach could yield better results than classical univariate techniques in presence of correlation. This claim is verified in Table <a href="#tab:sim_false" data-reference-type="ref" data-reference="tab:sim_false">[tab:sim_false]</a> where multivariate heteroskedastic Gaussian data is simulated on which the log odd ratio of <span class="math inline"><em>y</em></span> depends linearly (misspecified model setting for the quantized <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span>). The proposed <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> approach yields significantly better results then ChiMerge and MDLP.</p>
                                                            
                                                            <p><span id="tab:sim_false" label="tab:sim_false">[tab:sim_false]</span></p>
                                                            <table>
                                                                <caption>Gini of the resulting misspecified <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> from quantized data using ChiMerge, MDLP and <em>glmdisc</em>-SEM: the multivariate approach is able to capture information about the correlation structure.</caption>
                                                                <thead>
                                                                    <tr class="header">
                                                                        <th style="text-align: left;"></th>
                                                                        <th style="text-align: left;">ChiMerge</th>
                                                                        <th style="text-align: left;">MDLP</th>
                                                                        <th style="text-align: left;"><em>glmdisc</em>-SEM</th>
                                                                    </tr>
                                                                </thead>
                                                                <tbody>
                                                                    <tr class="odd">
                                                                        <td style="text-align: left;">Performance</td>
                                                                        <td style="text-align: left;">50.1 (1.6)</td>
                                                                        <td style="text-align: left;">77.1 (0.9)</td>
                                                                        <td style="text-align: left;"><strong>80.6</strong> (0.6)</td>
                                                                    </tr>
                                                                </tbody>
                                                            </table>
                                                            
                                                            
                                                            
                                                            
                                                            
                                                            
                                                            
                                                            
                                                            
                                                            
                                                            
                                                            
                                                            
                                                            <h3 id="subsec:exp_benchmark">Benchmark data</h3>
                                                            <p>To test further the effectiveness of <em>glmdisc</em> in a predictive setting, we gathered 6 datasets from the UCI library: the Adult dataset (<span class="math inline"><em>n</em> = 48, 842</span>, <span class="math inline"><em>d</em> = 14</span>), the Australian dataset (<span class="math inline"><em>n</em> = 690</span>, <span class="math inline"><em>d</em> = 14</span>), the Bands dataset (<span class="math inline"><em>n</em> = 512</span>, <span class="math inline"><em>d</em> = 39</span>), the Credit-screening dataset (<span class="math inline"><em>n</em> = 690</span>, <span class="math inline"><em>d</em> = 15</span>), the German dataset (<span class="math inline"><em>n</em> = 1, 000</span>, <span class="math inline"><em>d</em> = 20</span>) and the Heart dataset (<span class="math inline"><em>n</em> = 270</span>, <span class="math inline"><em>d</em> = 13</span>). Each of these datasets have mixed (continuous and categorical) features and a binary response to predict. To get more information about these datasets, their respective features, and the predictive task associated with them, readers may refer to the UCI website<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>.</p>
                                                            <p>Now that the proposed approach was shown empirically consistent, <em>i.e.</em> it is able to find the true quantization in a well-specified setting, it is desirable to verify the previous claim that embedding the learning of a good quantization in the predictive task <em>via glmdisc</em> is better than other methods that rely on <em>ad hoc</em> criteria. As we were primarily interested in <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span>, I will compare the proposed approach to a naïve linear <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> (hereafter ALLR), <em>i.e.</em> on non-quantized data, a <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> on continuous discretized data using the now standard MDLP algorithm from <a href="#fayyad1993multi" class="citation" data-cites="fayyad1993multi">(9)</a> and categorical grouped data using <span class="math inline"><em>χ</em><sup>2</sup></span> tests of independence between each pair of factor levels and the target in the same fashion as the ChiMerge discretization algorithm proposed by <a href="#kerber1992chimerge" class="citation" data-cites="kerber1992chimerge">(10)</a> (hereafter MDLP/<span class="math inline"><em>χ</em><sup>2</sup></span>). In this section and the next, Gini indices are reported on a random 30 % test set and CIs are given following a method found in <a href="#sun2014fast" class="citation" data-cites="sun2014fast">(32)</a>. Table <a href="#tab:banchmark" data-reference-type="ref" data-reference="tab:banchmark">[tab:banchmark]</a> shows our approach yields significantly better results on these rather small datasets where the added flexibility of quantization might help the predictive task.</p>
                                                            <p>As argued in the preceding section, <em>glmdisc</em>-SEM yields slightly worse results than <em>glmdisc</em>-NN which, additionally to their inherent difference (the S-step of the <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span>), might be due to the sensitivity of the <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> to its starting points (a single Markov Chain is run in this section and the next).</p>
                                                            
                                                            <table>
                                                                <caption>Gini indices (the greater the value, the better the performance) of our proposed quantization algorithm <em>glmdisc</em> and two baselines: ALLR and MDLP / <span class="math inline"><em>χ</em><sup>2</sup></span> tests obtained on several benchmark datasets from the UCI library with a single run and a computational budget <span class="math inline"><em>S</em> = 500</span> iterations.<span label="tab:banchmark"></span></caption>
                                                                <tbody>
                                                                    <tr class="odd">
                                                                        <td style="text-align: left;">Dataset</td>
                                                                        <td style="text-align: left;">ALLR</td>
                                                                        <td style="text-align: left;"><em>ad hoc</em> methods</td>
                                                                        <td style="text-align: left;"><em>glmdisc</em>-NN</td>
                                                                        <td style="text-align: left;">glmdisc</em>-SEM</td>
                                                                    </tr>
                                                                    <tr class="even">
                                                                        <td style="text-align: left;">Adult</td>
                                                                        <td style="text-align: left;">81.4 (1.0)</td>
                                                                        <td style="text-align: left;"><strong>85.3</strong> (0.9)</td>
                                                                        <td style="text-align: left;">80.4 (1.0)</td>
                                                                        <td style="text-align: left;">81.5 (1.0)</td>
                                                                    </tr>
                                                                    <tr class="odd">
                                                                        <td style="text-align: left;">Australian</td>
                                                                        <td style="text-align: left;">72.1 (10.4)</td>
                                                                        <td style="text-align: left;">84.1 (7.5)</td>
                                                                        <td style="text-align: left;">92.5 (4.5)</td>
                                                                        <td style="text-align: left;"><strong>100</strong> (0)</td>
                                                                    </tr>
                                                                    <tr class="even">
                                                                        <td style="text-align: left;">Bands</td>
                                                                        <td style="text-align: left;">48.3 (17.8)</td>
                                                                        <td style="text-align: left;">47.3 (17.6)</td>
                                                                        <td style="text-align: left;">58.5 (12.0)</td>
                                                                        <td style="text-align: left;"><strong>58.7</strong> (12.0)</td>
                                                                    </tr>
                                                                    <tr class="odd">
                                                                        <td style="text-align: left;">Credit</td>
                                                                        <td style="text-align: left;">81.3 (9.6)</td>
                                                                        <td style="text-align: left;">88.7 (6.4)</td>
                                                                        <td style="text-align: left;"><strong>92.0</strong> (4.7)</td>
                                                                        <td style="text-align: left;">87.7 (6.4)</td>
                                                                    </tr>
                                                                    <tr class="even">
                                                                        <td style="text-align: left;">German</td>
                                                                        <td style="text-align: left;">52.0 (11.3)</td>
                                                                        <td style="text-align: left;">54.6 (11.2)</td>
                                                                        <td style="text-align: left;"><strong>69.2</strong> (9.1)</td>
                                                                        <td style="text-align: left;">54.5 (10)</td>
                                                                    </tr>
                                                                    <tr class="odd">
                                                                        <td style="text-align: left;">Heart</td>
                                                                        <td style="text-align: left;">80.3 (12.1)</td>
                                                                        <td style="text-align: left;">78.7 (13.1)</td>
                                                                        <td style="text-align: left;"><strong>86.3</strong> (10.6)</td>
                                                                        <td style="text-align: left;">82.2 (11.2)</td>
                                                                    </tr>
                                                                </tbody>
                                                            </table>
                                                            <h3 id="subsec:exp_real"><em>Credit Scoring</em> data</h3>
                                                            <p>Discretization, grouping and interaction screening are preprocessing steps relatively “manually” performed in the field of <em>Credit Scoring</em>, using <span class="math inline"><em>χ</em><sup>2</sup></span> tests for each feature or so-called Weights of Evidence (<a href="#zeng2014necessary" class="citation" data-cites="zeng2014necessary">(33)</a>). This back and forth process takes a lot of time and effort and provides no particular statistical guarantee.</p>
                                                            <p>Table <a href="#tab:real_data" data-reference-type="ref" data-reference="tab:real_data">[tab:real_data]</a> shows Gini coefficients of several portfolios for which there are <span class="math inline"><em>n</em> = 50, 000</span>, <span class="math inline"><em>n</em> = 30, 000</span>, <span class="math inline"><em>n</em> = 50, 000</span>, <span class="math inline"><em>n</em> = 100, 000</span>, <span class="math inline"><em>n</em> = 235, 000</span> and <span class="math inline"><em>n</em> = 7, 500</span> clients respectively and <span class="math inline"><em>d</em> = 25</span>, <span class="math inline"><em>d</em> = 16</span>, <span class="math inline"><em>d</em> = 15</span>, <span class="math inline"><em>d</em> = 14</span>, <span class="math inline"><em>d</em> = 14</span> and <span class="math inline"><em>d</em> = 16</span> features respectively. Approximately half of these features were categorical, with a number of factor levels ranging from <span class="math inline">2</span> to <span class="math inline">100</span>. All portfolios come from approximately one year of financed clients. The Automobile dataset is composed of car loans (and thus we have data about the cars, such as the brand, the cost, the motor, <em>etc</em> which generally boosts predictive performance), the Renovation, Mass retail and Electronics datasets are composed of standard loans through partners that respectively sell construction material (to private persons, not companies), retail products (<em>i.e.</em> supermarkets), and electronics goods (smartphones, TVs, <em>etc</em>). The Standard and Revolving datasets are clients coming directly to Sofinco (<span data-acronym-label="cacf" data-acronym-form="singular+short">CACF</span>’s main brand) through the phone or the web.</p>
                                                            <p>We compare the rather manual, in-house approach that yields the current performance, the naïve linear <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> and <em>ad hoc</em> methods introduced in the previous section and finally our <em>glmdisc</em> proposal. Beside the classification performance, interpretability is maintained and unsurprisingly, the learned representation comes often close to the “manual” approach: for example, the complicated in-house coding of job types is roughly grouped by <em>glmdisc</em> into <em>e.g.</em> “worker”, “technician”, <em>etc.</em> Notice that even if the “naïve” <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> reaches some very decent predictive results, its poor interpretability skill (no quantization at all) excludes it from standard use in the company.</p>
                                                            <p>Our approach shows approximately similar results than MDLP/<span class="math inline"><em>χ</em><sup>2</sup></span>, potentially due to the fact that contrary to the two previous experiments with simulated or UCI data, the classes are imbalanced (<span class="math inline"> &lt; 3%</span> defaulting loans), which would require special treatment while back-propagating the gradients <a href="#anand1993improved" class="citation" data-cites="anand1993improved">(34)</a>. Note however that it is never significantly worse; for the Electronics dataset and as was the case for most UCI datasets, <em>glmdisc</em> is significantly superior, which in the <em>Credit Scoring</em> business might end up saving millions to the financial institution.</p>
                                                            <p>Table <a href="#tab:real_data_cont" data-reference-type="ref" data-reference="tab:real_data_cont">[tab:real_data_cont]</a> is somewhat similar but is an earlier work (<em>glmdisc</em>-NN was not implemented at that time): no CI is reported, only continuous features are considered so that pure discretization methods can be compared, namely MDLP and ChiMerge. Three portfolios are used with approx. 10 features and <span class="math inline"><em>n</em> = 180, 000</span>, <span class="math inline"><em>n</em> = 30, 000</span>, and <span class="math inline"><em>n</em> = 100, 000</span> respectively. The Automobile 2 dataset is again a car loan dataset with information on the cars, the Young clients datasets features only clients less than 30 years old (that are difficult to address in the industry: poor credit history or stability), and the Basel II dataset is a small portion of known clients for which we’d like to provision the expected losses (regulation obligation per the Basel II requirements). The proposed algorithm <em>glmdisc</em>-SEM performs best, but is rather similar to the achieved performance of MDLP. ChiMerge does poorly since its parameter <span class="math inline"><em>α</em></span> (the rejection zone of the <span class="math inline"><em>χ</em><sup>2</sup></span> tests) is not optimized which is blatant on Portfolio 3 where approx. <span class="math inline">2, 000</span> intervals are created, so that predictions are very “noisy”.</p>
                                                            <p>The usefulness of discretization and grouping is clear on <em>Credit Scoring</em> data and although <em>glmdisc</em> does not always perform significantly better than the manual approach, it allows practitioners to focus on other tasks by saving a lot of time, as was already stressed out. As a rule of thumb, a month is generally allocated to data pre-processing for a single data scientist working on a single scorecard. On Google Collaboratory, and relying on Keras (<a href="#chollet2015keras" class="citation" data-cites="chollet2015keras">(26)</a>) and Tensorflow (<a href="#tensorflow2015-whitepaper" class="citation" data-cites="tensorflow2015-whitepaper">(25)</a>) as a backend, it took less than an hour to perform discretization and grouping for all datasets. As for the <em>glmdisc</em>-SEM method, quantization of datasets of approx. <span class="math inline"><em>n</em> = 10, 000</span> observations and approx. <span class="math inline"><em>d</em> = 10</span> take about 2 hours on a laptop within a single CPU core. On such a small rig, <span class="math inline"><em>n</em> = 100, 000</span> observations and trying to perform interaction screening becomes however prohibitive (approx. 3 days). However, using higher computing power aside, there is still room for improvement, <em>e.g.</em> parallel computing, replacing bottleneck functions with C++ code, etc. Moreover, the ChiMerge and MDLP methods implemented in the <span class="math inline">R</span> package are not much faster while showing inferior performance and being capable of only discretization on non-missing values.</p>
                                                            
                                                            <p><span id="tab:real_data" label="tab:real_data">[tab:real_data]</span></p>
                                                            <table>
                                                                <caption>Gini indices (the greater the value, the better the performance) of our proposed quantization algorithm <em>glmdisc</em>, the two baselines of Table <a href="#tab:banchmark" data-reference-type="ref" data-reference="tab:banchmark">[tab:banchmark]</a> and the current scorecard (manual / expert representation) obtained on several portfolios of Crédit Agricole Consumer Finance with a single run and a computational budget <span class="math inline"><em>S</em> = 500</span> iterations.</caption>
                                                                <tbody>
                                                                    <tr class="odd">
                                                                        <td style="text-align: left;">Portfolio</td>
                                                                        <td style="text-align: left;">ALLR</td>
                                                                        <td style="text-align: left;">Current performance</td>
                                                                        <td style="text-align: left;">ad hoc methods</td>
                                                                        <td style="text-align: left;">glmdisc-NN</td>
                                                                        <td style="text-align: left;">glmdisc-SEM</td>
                                                                    </tr>
                                                                    <tr class="even">
                                                                        <td style="text-align: left;">Automobile</td>
                                                                        <td style="text-align: left;"><strong><span>59.3</span> (3.1)</strong></td>
                                                                        <td style="text-align: left;">55.6 (3.4)</td>
                                                                        <td style="text-align: left;"><strong><span>59.3</span> (3.0)</strong></td>
                                                                        <td style="text-align: left;">58.9 (2.6)</td>
                                                                        <td style="text-align: left;">57.8 (2.9)</td>
                                                                    </tr>
                                                                    <tr class="odd">
                                                                        <td style="text-align: left;">Renovation</td>
                                                                        <td style="text-align: left;">52.3 (5.5)</td>
                                                                        <td style="text-align: left;">50.9 (5.6)</td>
                                                                        <td style="text-align: left;">54.0 (5.1)</td>
                                                                        <td style="text-align: left;"><strong><span>56.7</span> (4.8)</strong></td>
                                                                        <td style="text-align: left;">55.5 (5.2)</td>
                                                                    </tr>
                                                                    <tr class="even">
                                                                        <td style="text-align: left;">Standard</td>
                                                                        <td style="text-align: left;">39.7 (3.3)</td>
                                                                        <td style="text-align: left;">37.1 (3.8)</td>
                                                                        <td style="text-align: left;"><strong><span>45.3</span> (3.1)</strong></td>
                                                                        <td style="text-align: left;">43.8 (3.2)</td>
                                                                        <td style="text-align: left;">36.7 (3.7)</td>
                                                                    </tr>
                                                                    <tr class="odd">
                                                                        <td style="text-align: left;">Revolving</td>
                                                                        <td style="text-align: left;">62.7 (2.8)</td>
                                                                        <td style="text-align: left;">58.5 (3.2)</td>
                                                                        <td style="text-align: left;"><strong><span>63.2</span> (2.8)</strong></td>
                                                                        <td style="text-align: left;">62.3 (2.8)</td>
                                                                        <td style="text-align: left;">60.7 (2.8)</td>
                                                                    </tr>
                                                                    <tr class="even">
                                                                        <td style="text-align: left;">Mass retail</td>
                                                                        <td style="text-align: left;">52.8 (5.3)</td>
                                                                        <td style="text-align: left;">48.7 (6.0)</td>
                                                                        <td style="text-align: left;">61.4 (4.7)</td>
                                                                        <td style="text-align: left;"><strong><span>61.8</span> (4.6)</strong></td>
                                                                        <td style="text-align: left;">61.0 (4.7)</td>
                                                                    </tr>
                                                                    <tr class="odd">
                                                                        <td style="text-align: left;">Electronics</td>
                                                                        <td style="text-align: left;">52.9 (11.9)</td>
                                                                        <td style="text-align: left;">55.8 (10.8)</td>
                                                                        <td style="text-align: left;">56.3 (10.2)</td>
                                                                        <td style="text-align: left;"><strong><span>72.6</span> (7.4)</strong></td>
                                                                        <td style="text-align: left;">62.0 (9.5)</td>
                                                                    </tr>
                                                                </tbody>
                                                            </table>
                                                            
                                                            <p><span id="tab:real_data_cont" label="tab:real_data_cont">[tab:real_data_cont]</span></p>
                                                            <table>
                                                                <caption>Gini indices for three other portfolios of Crédit Agricole Consumer Finance involving only continuous features and following three methods: ChiMerge, MDLP and <em>glmdisc</em>-SEM compared to the current performance.</caption>
                                                                <tbody>
                                                                    <tr class="odd">
                                                                        <td style="text-align: left;">Portfolio</td>
                                                                        <td style="text-align: left;">Current performance</td>
                                                                        <td style="text-align: left;">ad hoc methods</td>
                                                                        <td style="text-align: left;">glmdisc-NN</td>
                                                                        <td style="text-align: left;">glmdisc-SEM</td>
                                                                    </tr>
                                                                    <tr class="even">
                                                                        <td style="text-align: left;">Automobile 2</td>
                                                                        <td style="text-align: left;">57.5</td>
                                                                        <td style="text-align: left;">16.5</td>
                                                                        <td style="text-align: left;"><strong>58.0</strong></td>
                                                                        <td style="text-align: left;"><strong>58.0</strong></td>
                                                                    </tr>
                                                                    <tr class="odd">
                                                                        <td style="text-align: left;">Young clients</td>
                                                                        <td style="text-align: left;">27.0</td>
                                                                        <td style="text-align: left;">26.7</td>
                                                                        <td style="text-align: left;">29.2</td>
                                                                        <td style="text-align: left;"><strong>30.0</strong></td>
                                                                    </tr>
                                                                    <tr class="even">
                                                                        <td style="text-align: left;">Basel II</td>
                                                                        <td style="text-align: left;">70.0</td>
                                                                        <td style="text-align: left;">0</td>
                                                                        <td style="text-align: left;"><strong>71.3</strong></td>
                                                                        <td style="text-align: left;"><strong>71.3</strong></td>
                                                                    </tr>
                                                                </tbody>
                                                            </table>
                                                            <h2 id="concluding-remarks">Concluding remarks</h2>
                                                            <h3 id="handling-missing-data">Handling missing data</h3>
                                                            <p>For categorical features, handling missing data is straightforward: the level “missing” is simply considered as a separate level, that can eventually be merged in the proposed algorithm with any other level. If it is <span data-acronym-label="mnar" data-acronym-form="singular+short">mnar</span> (<em>e.g.</em> co-borrower information missing because there is none) and such clients are significantly different from other clients in terms of creditworthiness, then such a treatment makes sense. If it is <span data-acronym-label="mar" data-acronym-form="singular+short">mar</span> and <em>e.g.</em> highly correlated with some of the feature’s levels (for example, the feature “number of children” could be either <span class="math inline">0</span> or missing to mean the borrower has no child), the proposed algorithm is highly likely to group these levels.</p>
                                                            <p>For continuous features, the same strategy can be employed: they can be encoded as “missing” and considered a separate level. However, this prevents this level to be merged with another one by having <em>e.g.</em> a level “<span class="math inline">[0; 200] or missing</span>”.</p>
                                                            <h3 id="integrating-constraints-on-the-cut-points">Integrating constraints on the cut-points</h3>
                                                            <p>Another problem that <span data-acronym-label="cacf" data-acronym-form="singular+short">CACF</span> faces is to have interpretable cutpoints,  having discretization intervals of the form <span class="math inline">[0; 200]</span> and not <span class="math inline">[0.389; 211.2]</span> which are arguably less interpretable. But it is also highly subjective, and it would require the addition of an hyperparameter, namely the set of admissible discretization and / or the rounding to perform for each feature <span class="math inline"><em>j</em></span> such that we did not pursue this problem. For the record, it is interesting to note that a straightforward rounding might not work: in the optimization community, it is well known that integer problems require special algorithmic treatment (dubbed integer programming). As an undergraduate, I applied some of these techniques to financial data in <ah ref="#projet_recherche" class="citation" data-cites="projet_recherche">(35)</a> where I give a counterexample. Additionally, forcing estimated cutpoints to fall into a constrained set might drastically change predictive performance if levels collapse as on Figure <a href="#fig:constraint" data-reference-type="ref" data-reference="fig:constraint">[fig:constraint]</a>.</p>
                                                            <h3 id="wrapping-up">Wrapping up</h3>
                                                            <p>Feature quantization (discretization for continuous features, grouping of factor levels for categorical ones) in a supervised multivariate classification setting is a recurring problem in many industrial contexts. This setting was formalized as a highly combinatorial representation learning problem and a new algorithmic approach, named <em>glmdisc</em>, has been proposed as a sensible approximation of a classical statistical information criterion.</p>
                                                            <p>The first proposed implementation relies on the use of a neural network of particular architecture and specifically a softmax approximation of each discretized or grouped feature. The second proposed implementation relies on an <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm and a polytomic multiclass <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span> approximation in the same flavor as the softmax. These proposals can alternatively be replaced by any other univariate multiclass predictive model, which make them flexible and adaptable to other problems. Prediction of the target feature, given quantized features, was exemplified with <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span>, although here as well, it can be swapped with any other supervised classification model.</p>
                                                            <p>Although both estimation methods are arguably much alike, since they rely on the same continuous approximation, results differed sensibly. Indeed, the <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> approach necessitated a sampling step, whereas the neural network , which has nevertheless the clear advantage of exploring . Additionally, the neural network approach relies on standard deep learning libraries, highly parallelizable and , which can make it way faster than the <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> approach that cannot be parallelized straightforwardly .</p>
                                                            <p>The experiments showed that, as was sensed empirically by statisticians in the field of <em>Credit Scoring</em>, discretization and grouping can indeed provide better models than standard <span data-acronym-label="lr" data-acronym-form="singular+short">logistic regression</span>. This novel approach allows practitioners to have a fully automated and statistically well-grounded tool that achieves better performance than <em>ad hoc</em> industrial practices at the price of decent computing time but much less of the practitioner’s valuable time.</p>
                                                            
                                                            <section class="footnotes">
                                                                <hr />
                                                                <ol>
                                                                    <li id="fn1"><p>See <a href="https://stats.stackexchange.com/questions/60100/penalized-methods-for-categorical-data-combining-levels-in-a-factor" class="uri">https://stats.stackexchange.com/questions/60100/penalized-methods-for-categorical-data-combining-levels-in-a-factor</a><a href="#fnref1" class="footnote-back">↩</a></p></li>
                                                                    <li id="fn2"><p>Reproduced from <a href="https://github.com/wassname/viz_torch_optim" class="uri">https://github.com/wassname/viz_torch_optim</a><a href="#fnref2" class="footnote-back">↩</a></p></li>
                                                                    <li id="fn3"><p><a href="#Dua:2017" class="citation" data-cites="Dua:2017">(36)</a> : http://archive.ics.uci.edu/ml<a href="#fnref3" class="footnote-back">↩</a></p></li>
                                                                </ol>
                                                            </section>
                                                            
                                                    <table>
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="hosmer2013applied">1</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                David&nbsp;W Hosmer&nbsp;Jr, Stanley Lemeshow, and Rodney&nbsp;X Sturdivant.
                                                                <em>Applied logistic regression</em>, volume 398.
                                                                John Wiley &amp; Sons, 2013.
                                                                [&nbsp;<a href="chapitre4_bib.html#hosmer2013applied">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="yang2009discretization">2</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Ying Yang and Geoffrey&nbsp;I Webb.
                                                                Discretization for naive-bayes learning: managing discretization bias
                                                                and variance.
                                                                <em>Machine learning</em>, 74(1):39-74, 2009.
                                                                [&nbsp;<a href="chapitre4_bib.html#yang2009discretization">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="maj2015delete">3</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Aleksandra Maj-Ka&#x144;ska, Piotr Pokarowski, Agnieszka Prochenka, et&nbsp;al.
                                                                Delete or merge regressors for linear model selection.
                                                                <em>Electronic Journal of Statistics</em>, 9(2):1749-1778, 2015.
                                                                [&nbsp;<a href="chapitre4_bib.html#maj2015delete">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="villani2018donner">4</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                C&eacute;dric Villani, Yann Bonnet, Charly Berthet, Fran&ccedil;ois Levin, Marc
                                                                Schoenauer, Anne-Charlotte Cornut, and Bertrand Rondepierre.
                                                                <em>Donner un sens &agrave; l'intelligence artificielle: pour une
                                                                    strat&eacute;gie nationale et europ&eacute;enne</em>.
                                                                Conseil national du num&eacute;rique, 2018.
                                                                [&nbsp;<a href="chapitre4_bib.html#villani2018donner">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="ramirez2016data">5</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Sergio Ram&iacute;rez-Gallego, Salvador Garc&iacute;a, H&eacute;ctor
                                                                Mouri&ntilde;o-Tal&iacute;n, David Mart&iacute;nez-Rego, Ver&oacute;nica
                                                                Bol&oacute;n-Canedo, Amparo Alonso-Betanzos, Jos&eacute;&nbsp;Manuel Ben&iacute;tez, and
                                                                Francisco Herrera.
                                                                Data discretization: taxonomy and big data challenge.
                                                                <em>Wiley Interdisciplinary Reviews: Data Mining and Knowledge
                                                                    Discovery</em>, 6(1):5-21, 2016.
                                                                [&nbsp;<a href="chapitre4_bib.html#ramirez2016data">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="dougherty1995supervised">6</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                James Dougherty, Ron Kohavi, and Mehran Sahami.
                                                                Supervised and unsupervised discretization of continuous features.
                                                                In <em>Machine Learning Proceedings 1995</em>, pages 194-202. Elsevier,
                                                                1995.
                                                                [&nbsp;<a href="chapitre4_bib.html#dougherty1995supervised">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="liu2002discretization">7</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Huan Liu, Farhad Hussain, Chew&nbsp;Lim Tan, and Manoranjan Dash.
                                                                Discretization: An enabling technique.
                                                                <em>Data mining and knowledge discovery</em>, 6(4):393-423, 2002.
                                                                [&nbsp;<a href="chapitre4_bib.html#liu2002discretization">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="kass1980exploratory">8</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Gordon&nbsp;V Kass.
                                                                An exploratory technique for investigating large quantities of
                                                                categorical data.
                                                                <em>Applied statistics</em>, pages 119-127, 1980.
                                                                [&nbsp;<a href="chapitre4_bib.html#kass1980exploratory">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="fayyad1993multi">9</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Usama Fayyad and Keki Irani.
                                                                Multi-interval discretization of continuous-valued attributes for
                                                                classification learning.
                                                                In <em>13th International Joint Conference on Artificial
                                                                    Intelligence</em>, pages 1022–-1029, 1993.
                                                                [&nbsp;<a href="chapitre4_bib.html#fayyad1993multi">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="kerber1992chimerge">10</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Randy Kerber.
                                                                Chimerge: Discretization of numeric attributes.
                                                                In <em>Proceedings of the tenth national conference on Artificial
                                                                    intelligence</em>, pages 123-128. Aaai Press, 1992.
                                                                [&nbsp;<a href="chapitre4_bib.html#kerber1992chimerge">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="liu1995chi2">11</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Huan Liu and Rudy Setiono.
                                                                Chi2: Feature selection and discretization of numeric attributes.
                                                                In <em>Tools with artificial intelligence, 1995. proceedings.,
                                                                    seventh international conference on</em>, pages 388-391. IEEE, 1995.
                                                                [&nbsp;<a href="chapitre4_bib.html#liu1995chi2">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="wang1998concurrent">12</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Ke&nbsp;Wang and Bing Liu.
                                                                Concurrent discretization of multiple attributes.
                                                                In <em>Pacific Rim International Conference on Artificial
                                                                    Intelligence</em>, pages 250-259. Springer, 1998.
                                                                [&nbsp;<a href="chapitre4_bib.html#wang1998concurrent">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="tay2002modified">13</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Francis&nbsp;EH Tay and Lixiang Shen.
                                                                A modified chi2 algorithm for discretization.
                                                                <em>IEEE Transactions on Knowledge &amp; Data Engineering</em>,
                                                                (3):666-670, 2002.
                                                                [&nbsp;<a href="chapitre4_bib.html#tay2002modified">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="su2005extended">14</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Chao-Ton Su and Jyh-Hwa Hsu.
                                                                An extended chi2 algorithm for discretization of real value
                                                                attributes.
                                                                <em>IEEE transactions on knowledge and data engineering</em>,
                                                                17(3):437-441, 2005.
                                                                [&nbsp;<a href="chapitre4_bib.html#su2005extended">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="shaffer1995multiple">15</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Juliet&nbsp;Popper Shaffer.
                                                                Multiple hypothesis testing.
                                                                <em>Annual review of psychology</em>, 46(1):561-584, 1995.
                                                                [&nbsp;<a href="chapitre4_bib.html#shaffer1995multiple">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="ho1997zeta">16</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                KM&nbsp;Ho and PD&nbsp;Scott.
                                                                Zeta: a global method for discretization of cotitinuous variables.
                                                                In <em>Proceedings of the 3rd International Conference on Knowledge
                                                                    Discovery and Data Mining</em>, pages 191-194, 1997.
                                                                [&nbsp;<a href="chapitre4_bib.html#ho1997zeta">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="ho1998efficient">17</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                KM&nbsp;Ho and Paul&nbsp;D Scott.
                                                                An efficient global discretization method.
                                                                In <em>Pacific-Asia Conference on Knowledge Discovery and Data
                                                                    Mining</em>, pages 383-384. Springer, 1998.
                                                                [&nbsp;<a href="chapitre4_bib.html#ho1998efficient">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="akaike1973information">18</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Hirotugu Akaike.
                                                                Information theory and an extension of the maximum likelihood
                                                                principle.
                                                                In <em>2nd International Symposium on Information Theory, 1973</em>,
                                                                pages 267-281. Akademiai Kiado, 1973.
                                                                [&nbsp;<a href="chapitre4_bib.html#akaike1973information">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="chen2008extended">19</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Jiahua Chen and Zehua Chen.
                                                                Extended bayesian information criteria for model selection with large
                                                                model spaces.
                                                                <em>Biometrika</em>, 95(3):759-771, 2008.
                                                                [&nbsp;<a href="chapitre4_bib.html#chen2008extended">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="vincent_disc">20</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Vincent Vandewalle.
                                                                How to take into account the discrete parameters in the bic
                                                                criterion?
                                                                In <em>COMPStat</em>, 2010.
                                                                [&nbsp;<a href="chapitre4_bib.html#vincent_disc">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="same2011model">21</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Allou Sam&eacute;, Faicel Chamroukhi, G&eacute;rard Govaert, and Patrice Aknin.
                                                                Model-based clustering and segmentation of time series with changes
                                                                in regime.
                                                                <em>Advances in Data Analysis and Classification</em>, 5(4):301-321,
                                                                2011.
                                                                [&nbsp;<a href="chapitre4_bib.html#same2011model">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="chamroukhi2009regression">22</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Faicel Chamroukhi, Allou Sam&eacute;, G&eacute;rard Govaert, and Patrice Aknin.
                                                                A regression model with a hidden logistic process for feature
                                                                extraction from time series.
                                                                In <em>Neural Networks, 2009. IJCNN 2009. International Joint
                                                                    Conference on</em>, pages 489-496. IEEE, 2009.
                                                                [&nbsp;<a href="chapitre4_bib.html#chamroukhi2009regression">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="reverdy2016parameter">23</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Paul Reverdy and Naomi&nbsp;Ehrich Leonard.
                                                                Parameter estimation in softmax decision-making models with linear
                                                                objective functions.
                                                                <em>IEEE Transactions on Automation Science and Engineering</em>,
                                                                13(1):54-67, 2016.
                                                                [&nbsp;<a href="chapitre4_bib.html#reverdy2016parameter">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="10.2307/2289471">24</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Steven&nbsp;G. Self and Kung-Yee Liang.
                                                                Asymptotic properties of maximum likelihood estimators and likelihood
                                                                ratio tests under nonstandard conditions.
                                                                <em>Journal of the American Statistical Association</em>,
                                                                82(398):605-610, 1987.
                                                                [&nbsp;<a href="chapitre4_bib.html#10.2307/2289471">bib</a>&nbsp;|
                                                                <a href="http://www.jstor.org/stable/2289471">http</a>&nbsp;]
                                                                <blockquote><font size="-1">
                                                                    Large sample properties of the likelihood function when the true parameter value may be on the boundary of the parameter space are described. Specifically, the asymptotic distribution of maximum likelihood estimators and likelihood ratio statistics are derived. These results generalize the work of Moran (1971), Chant (1974), and Chernoff (1954). Some of Chant's results are shown to be incorrect. The approach used in deriving these results follows from comments made by Moran and Chant. The problem is shown to be asymptotically equivalent to the problem of estimating the restricted mean of a multivariate Gaussian distribution from a sample of size 1. In this representation the Gaussian random variable corresponds to the limit of the normalized score statistic and the estimate of the mean corresponds to the limit of the normalized maximum likelihood estimator. Thus the limiting distribution of the maximum likelihood estimator is the same as the distribution of the projection of the Gaussian random variable onto the region of admissible values for the mean. A variety of examples is provided for which the limiting distributions of likelihood ratio statistics are mixtures of chi-squared distributions. One example is provided with a nuisance parameter on the boundary for which the asymptotic distribution is not a mixture of chi-squared distributions.
                                                                </font></blockquote>
                                                                <p>
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="tensorflow2015-whitepaper">25</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Mart&iacute;n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
                                                                Craig Citro, Greg&nbsp;S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,
                                                                Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
                                                                Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
                                                                Levenberg, Dandelion Man&eacute;, Rajat Monga, Sherry Moore, Derek Murray, Chris
                                                                Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal
                                                                Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vi&eacute;gas,
                                                                Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and
                                                                Xiaoqiang Zheng.
                                                                TensorFlow: Large-scale machine learning on heterogeneous systems,
                                                                2015.
                                                                Software available from tensorflow.org.
                                                                [&nbsp;<a href="chapitre4_bib.html#tensorflow2015-whitepaper">bib</a>&nbsp;|
                                                                <a href="https://www.tensorflow.org/">http</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="chollet2015keras">26</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Fran&ccedil;ois Chollet et&nbsp;al.
                                                                Keras.
                                                                <a href="https://keras.io">https://keras.io</a>, 2015.
                                                                [&nbsp;<a href="chapitre4_bib.html#chollet2015keras">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="bottou2010large">27</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                L&eacute;on Bottou.
                                                                Large-scale machine learning with stochastic gradient descent.
                                                                In <em>Proceedings of COMPSTAT'2010</em>, pages 177-186. Springer,
                                                                2010.
                                                                [&nbsp;<a href="chapitre4_bib.html#bottou2010large">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="o2006logistic">28</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Ann&nbsp;A O'Connell.
                                                                <em>Logistic regression models for ordinal response variables</em>.
                                                                Number 146. Sage, 2006.
                                                                [&nbsp;<a href="chapitre4_bib.html#o2006logistic">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="celeux_sem">29</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Gilles Celeux, Didier Chauveau, and Jean Diebolt.
                                                                On Stochastic Versions of the EM Algorithm.
                                                                Research Report RR-2514, INRIA, 1995.
                                                                [&nbsp;<a href="chapitre4_bib.html#celeux_sem">bib</a>&nbsp;|
                                                                <a href="https://hal.inria.fr/inria-00074164">http</a>&nbsp;|
                                                                <a href="https://hal.inria.fr/inria-00074164/file/RR-2514.pdf">.pdf</a>&nbsp;]
                                                                <blockquote><font size="-1">
                                                                    Keywords: MONTE-CARLO EXPERIMENTS ; MIXTURE OF DOSTRIBUTION ; INCOMPLETE DATA MODELS ; STOCHASTIC ALGORITHMS
                                                                </font></blockquote>
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="casella1992explaining">30</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                George Casella and Edward&nbsp;I George.
                                                                Explaining the gibbs sampler.
                                                                <em>The American Statistician</em>, 46(3):167-174, 1992.
                                                                [&nbsp;<a href="chapitre4_bib.html#casella1992explaining">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="green1995reversible">31</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Peter&nbsp;J Green.
                                                                Reversible jump markov chain monte carlo computation and bayesian
                                                                model determination.
                                                                <em>Biometrika</em>, 82(4):711-732, 1995.
                                                                [&nbsp;<a href="chapitre4_bib.html#green1995reversible">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="sun2014fast">32</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Xu&nbsp;Sun and Weichao Xu.
                                                                Fast implementation of delong's algorithm for comparing the areas
                                                                under correlated receiver oerating characteristic curves.
                                                                <em>IEEE Signal Processing Letters</em>, 21(11):1389-1393, 2014.
                                                                [&nbsp;<a href="chapitre4_bib.html#sun2014fast">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="zeng2014necessary">33</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Guoping Zeng.
                                                                A necessary condition for a good binning algorithm in credit scoring.
                                                                <em>Applied Mathematical Sciences</em>, 8(65):3229-3242, 2014.
                                                                [&nbsp;<a href="chapitre4_bib.html#zeng2014necessary">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="anand1993improved">34</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Rangachari Anand, Kishan&nbsp;G Mehrotra, Chilukuri&nbsp;K Mohan, and Sanjay Ranka.
                                                                An improved algorithm for neural network classification of imbalanced
                                                                training sets.
                                                                <em>IEEE Transactions on Neural Networks</em>, 4(6):962-969, 1993.
                                                                [&nbsp;<a href="chapitre4_bib.html#anand1993improved">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="projet_recherche">35</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Adrien Ehrhardt.
                                                                Projet recherche : optimisation non-linéaire - application à la
                                                                création d'indices boursiers.
                                                                Master's thesis, &Eacute;cole Centrale de Lille, 2014.
                                                                [&nbsp;<a href="chapitre4_bib.html#projet_recherche">bib</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        <tr valign="top">
                                                            <td align="right" class="bibtexnumber">
                                                                [<a name="Dua:2017">36</a>]
                                                            </td>
                                                            <td class="bibtexitem">
                                                                Dua Dheeru and Efi Karra&nbsp;Taniskidou.
                                                                UCI machine learning repository, 2017.
                                                                [&nbsp;<a href="chapitre4_bib.html#Dua:2017">bib</a>&nbsp;|
                                                                <a href="http://archive.ics.uci.edu/ml">http</a>&nbsp;]
                                                                
                                                            </td>
                                                        </tr>
                                                        
                                                        
                                                    </table>
                                                            
                                                            
<div w3-include-html="https://github.com/adimajo/glmdisc/blob/master/inst/doc/glmdisc.html"></div>

<script>
function includeHTML() {
  var z, i, elmnt, file, xhttp;
  /*loop through a collection of all HTML elements:*/
  z = document.getElementsByTagName("*");
  for (i = 0; i < z.length; i++) {
    elmnt = z[i];
    /*search for elements with a certain atrribute:*/
    file = elmnt.getAttribute("w3-include-html");
    if (file) {
      /*make an HTTP request using the attribute value as the file name:*/
      xhttp = new XMLHttpRequest();
      xhttp.onreadystatechange = function() {
        if (this.readyState == 4) {
          if (this.status == 200) {elmnt.innerHTML = this.responseText;}
          if (this.status == 404) {elmnt.innerHTML = "Page not found.";}
          /*remove the attribute, and call this function once more:*/
          elmnt.removeAttribute("w3-include-html");
          includeHTML();
        }
      } 
      xhttp.open("GET", file, true);
      xhttp.send();
      /*exit the function:*/
      return;
    }
  }
}
</script>

<script>
includeHTML();
</script>





								</div>
							</section>

							<section id="footer">
						<div class="container">
							<ul class="copyright">
								<li>&copy; Adrien Ehrhardt. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollzer.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>
