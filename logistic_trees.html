<!DOCTYPE HTML>
<!--
	Read Only by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Adrien Ehrhardt - Inria, CA CF</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<link rel="icon" type="image/png" href="images/favicon.png" />
	</head>
	<body>

		<!-- Header -->
			<section id="header">
				<header>
					<span class="image avatar"><img src="images/avatar.jpg" alt="" /></span>
					<h1 id="logo"><a>Adrien Ehrhardt</a></h1>
                    <p>AI Assistant Professor @Polytechnique<br />
                    PhD @Lille University & @Inria<br />
                    in Machine Learning applied to Finance<br />
                    Machine Learning Engineer @Crédit Agricole</p>
				</header>
				<nav id="nav">
					<ul>
					        <li><a href="index.html">Home</a></li>
						<li><a href="cifre.html">Informations CIFRE</a></li>
						<li><a href="scoring.html">Intro to Credit Scoring</a></li>
						<li><a href="rejectinference.html">Reject Inference</a></li>
                        <li><a href="discretization.html">Discretization</a></li>
                        <li><a href="interaction_screening.html">Interaction screening</a></li>
                        <li><a href="logistic_trees.html" class="active">Logistic regression trees</a></li>
                        <li><a href="teaching.html">Teaching</a></li>
					</ul>
				</nav>
				<footer>
					<ul class="icons">
						<!--<li><a href="https://www.linkedin.com/in/adrien-ehrhardt" class="icon fa-linkedin"><span class="label">Linked-In</span></a></li>-->
						<!--<li><a href="#" class="icon fa-twitter"><span class="label">Twitter</span></a></li>-->
						<li><a href="https://www.facebook.com/adrien.ehrhardt.9" class="icon fa-facebook"><span class="label">Facebook</span></a></li>
						<li><a href="https://scholar.google.fr/citations?hl=fr&user=ISAbU0cAAAAJ&view_op=list_works" class="icon fa-google"><span class="label">Google</span></a></li>
						<li><a href="https://www.github.com/adimajo/" class="icon fa-github"><span class="label">Github</span></a></li>
						<li><a href="mailto:adrien.ehrhardt@centraliens-lille.org" class="icon fa-envelope"><span class="label">Email</span></a></li>
					</ul>
				</footer>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

						<!-- One -->
							<section id="one">
								<div class="container">
									<header class="major">
										<h2>Tree-structure segmentation for logistic regression</h2>
									</header>
                                    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
                                    <p>N.B.: ce post s’inspire librement de mon manuscrit de thèse.</p>

                                    <p>In Credit Scoring, we usually learn “expert” logistic regression models on separate “segments” of clients arranged in a tree. Its more theoretical justification is similar to that of quantization, which is to achieve a good bias-variance trade-off of the predictive task. Here again, the resulting segmentation and scorecards therein can be viewed as a single model for the whole population. In the next section, we give some industrial context to the problem which is followed in Section <a href="#sec:literature" data-reference-type="ref" data-reference="sec:literature">1.2</a> by a literature review. Section <a href="#sec:model_selec_tree" data-reference-type="ref" data-reference="sec:model_selec_tree">1.3</a> reinterprets this problem, as advertised, as a model selection problem for which a specific approach is designed in subsequent sections.</p>
                                    <h2 id="introduction">Introduction</h2>
                                    <h3 id="subsec:context">Context</h3>
                                    <p>As was emphasized in all previous articles, logistic regression is the building block of a scorecard predicting the creditworthiness of an applicant and partly automating the acceptance / rejection mechanism. However, estimating logistic regression coefficients means that training data <span class="math inline">\((\boldsymbol{\mathbf{x}},\boldsymbol{\mathbf{y}})\) is available. This is not the case when a new product, <em>e.g.</em> smartphone leasing, is added to the acceptance system. On a practical note, some other previously learnt scorecard may not be applicable on this new market because the same information is not asked to applicants, <em>e.g.</em> marital status, because given the low amounts at stake, it was decided to collect the fewest data possible, to make the process as simple and quick as possible. On a more theoretical note, it is probable that applicants to smartphone leasing are not stemming from the same data generating mechanism <span class="math inline">\(\boldsymbol{X},Y \sim p\) as any other previous applicants (<em>i.e.</em> on other markets). Put it another way, the possibility of having several logistic regression scorecards on sub-populations of the total portfolio allows to have more flexibility, and thus it potentially reduces model bias.</p>
                                    <p>For these reasons, several industries, among which <em>Credit Scoring</em>, rely on several “weak learners” such as logistic regression, arranged in a tree. Such decision process is illustrated on Figure <a href="#fig:arbre" data-reference-type="ref" data-reference="fig:arbre">[1]</a>. This tree structure and the vocabulary of “weak learners” would indicate a use-case of Mixtures of Experts <a href="#jordan1994hierarchical" class="citation" data-cites="jordan1994hierarchical">[1]</a> or aggregation / ensemble methods <a href="#opitz1999popular" class="citation" data-cites="opitz1999popular">[2]</a> respectively. However, these fuzzy methods imply that all applicants are scored by all scorecards, which is obviously neither desirable (for interpretation purposes) nor feasible (since available features differ).</p>
                                    <p>The next section illustrates how such a structure is achieved using <span data-acronym-label="CACF" data-acronym-form="singular+short">CACF</span>’s in-house practices.</p>
                                    
                                    <figure>
                                    <img src="figures/chapitre6/fig_arbre.png" alt="" width="100%"/></span>
                                    <figurecaption><a id="fig:arbre">[1]</a> Typical segmentation</figurecaption>
                                    </figure>

                                    
                                    <h3 id="subsec:adhoc">In-house <em>ad hoc</em> practice</h3>
                                    <p><em>Credit Scoring</em> practitioners are often asked by the management to “locally” study the decision process displayed on Figure <a href="#fig:arbre" data-reference-type="ref" data-reference="fig:arbre">[1]</a> by <em>e.g.</em> merging branches (Standard loans and Leasing for Fiat) or conversely to separate sub-populations by splitting a leaf (Kawasaki into Standard loans and Leasing). To do so, they resort to simple unsupervised generative “clustering” techniques, such as <span data-acronym-label="pca" data-acronym-form="singular+short">PCA</span> and its refinements on categorical or mixed data (<span data-acronym-label="mca" data-acronym-form="singular+short">MCA</span> or <span data-acronym-label="famd" data-acronym-form="singular+short">FAMD</span> resp.) which are described hereafter, used to represent the data on the two first principal axes, and derive “clusters” from separated point clouds.</p>
                                    <h5 id="section"></h5>
                                    <p>The goal of <span data-acronym-label="pca" data-acronym-form="singular+short">PCA</span> <a href="#pages2014multiple" class="citation" data-cites="pages2014multiple">[3]</a> is to represent observations graphically in a way that exhibits most efficiently their similitude and differences by combining input features in so-called orthogonal “principal components” <span class="math inline"><strong>u</strong> = (<strong>u</strong><sub>1</sub>, …, <strong>u</strong><sub><em>d</em></sub>)</span> such that the inertia of each axis <span class="math inline"><em>j</em></span> (the variance of <span class="math inline">\(\boldsymbol{x}' \boldsymbol{u}_j\)) is maximized. It can be shown that it is equivalent to seeking the ordering of the eigenvalues <span class="math inline"><strong>λ</strong> = (<em>λ</em><sub>1</sub>, …, <em>λ</em><sub><em>d</em></sub>)</span> of the covariance matrix <span class="math inline">\(\Sigma = \boldsymbol{\mathbf{x}}'\boldsymbol{\mathbf{x}}\). The explained variance of each axis <span class="math inline"><em>j</em></span> is given by <span class="math inline">\(\frac{\lambda_j}{\sum_{j'=1}^d \lambda_j'}\). Classically, only the two first axes <span class="math inline">(<strong>u</strong><sub>1</sub>, <strong>u</strong><sub>2</sub>)</span> (after reordering from largest to lowest explained variance) are used. The composition of theses axes in the original features <span class="math inline">\(x_j\) is often represented first, to see if groups of features can be formed which would define the subsequent segments. <span data-acronym-label="pca" data-acronym-form="singular+short">PCA</span> has been applied to the Automobile dataset from <span data-acronym-label="CACF" data-acronym-form="singular+short">CACF</span> (<span class="math inline"><em>n</em> = 50,000</span>, <span class="math inline"><em>d</em> = 25</span> among which <span class="math inline">18</span> continuous and <span class="math inline">7</span> categorical features which number of levels go from <span class="math inline">5</span> - family status - to <span class="math inline">100</span> - brand of the vehicle and <span class="math inline">200,000</span> missing entries) on Figure <a href="#fig:pca" data-reference-type="ref" data-reference="fig:pca">[2]</a>, where the aforementioned principal components’ composition is displayed on Figure <a href="#fig:pca" data-reference-type="ref" data-reference="fig:pca">[2]</a> (relying on the FactoMineR <span class="smallcaps">R</span> package <a href="#JSSv025i01" class="citation" data-cites="JSSv025i01">[4]</a>): interestingly, the first axis is dominated by car and loan characteristics such as the vehicle’s price (“APPORT”, “MCDE”, “CREDAC”), its fiscal and mechanical characteristics (“CVFISC”, “POIDSVH”, “CYLVH”) while the second axis is composed of clients’ characteristics such as their age (“anc_DNAISS”, “anc_DNACJ”), their number of children (“NBENF”), their job stability (“anc_AMEMBC”). Note that a good portion of the total variance is explained by the first axes (22.54 % and 18.97 % resp.). The second classical representation is the observations themselves in this new space, which is displayed on Figure <a href="#fig:pca" data-reference-type="ref" data-reference="fig:pca">[2]</a>: no clear group is distinguishable from the pack. With these two representations, the <em>Credit Scoring</em> practitioner decide if, visually, clusters are formed (<em>i.e.</em> clouds with little intra-class variance) which would be used to build separate scorecards <span class="math inline">\((p_{\boldsymbol{\theta}^{1}},p_{\boldsymbol{\theta}^{2}})\).</p>
                                    
                                    <figure>
                                    <img src="figures/chapitre6/fig_pca.png" alt="" width="100%"/></span>
                                    <figurecaption><a id="fig:pca">[2]</a> PCA</figurecaption>
                                    </figure>

                                    
                                    <p>However, the <em>Credit Scoring</em> data is of mixed type and <em>Credit Scoring</em> practitioners are used to quantizing the data, such that the <span data-acronym-label="mca" data-acronym-form="singular+short">MCA</span> algorithm, specific to categorical features, becomes applicable to all features, by using <em>e.g. equal-freq</em> or <span class="math inline"><em>χ</em><sup>2</sup></span> tests.</p>
                                    
                                    <p>In presence of only categorical features (or following quantization), the <span data-acronym-label="mca" data-acronym-form="singular+short">MCA</span> algorithm is more appropriate: it extends the <span data-acronym-label="pca" data-acronym-form="singular+short">PCA</span> approach to categorical features by using the disjunctive table (dummy / one-hot encoding of <span class="math inline">\(\boldsymbol{\mathbf{x}}\)). For a thorough introduction to <span data-acronym-label="mca" data-acronym-form="singular+short">MCA</span>, see <em>e.g.</em> <a href="#lebart1995statistique" class="citation" data-cites="lebart1995statistique">[5]</a>. What is most interesting in this method is that both categorical features’ levels and observations can be simultaneously displayed on the first principal components axes. This is of high practical interest in <em>Credit Scoring</em> because clouds of points are directly characterized by the categorical levels that are displayed nearest, contrary to <span data-acronym-label="pca" data-acronym-form="singular+short">PCA</span> where groups correspond to surfaces of equation <span class="math inline">\(\boldsymbol{x} ' \boldsymbol{u}_1 + \boldsymbol{x} ' \boldsymbol{u}_2 \geq \alpha\) where <span class="math inline"><em>α</em></span> encodes the separation boundary of resulting clusters, which would be the edges of our decision system, as on Figure <a href="#fig:arbre" data-reference-type="ref" data-reference="fig:arbre">[1]</a>, and make it arguably less interpretable.</p>
                                    <p>When applied to the Automobile dataset, the <span data-acronym-label="mca" data-acronym-form="singular+short">MCA</span> algorithm yields Figure <a href="#fig:mca" data-reference-type="ref" data-reference="fig:mca">[3]</a> (relying again on the FactoMineR <span class="smallcaps">R</span> package <a href="#JSSv025i01" class="citation" data-cites="JSSv025i01">[4]</a>). As categorical features’ levels are dummy encoded, they are all represented separately as in Figure <a href="#fig:mca" data-reference-type="ref" data-reference="fig:mca">[3]</a>. Unfortunately, as the vehicle’s brand takes a lot of levels, this figure is not very informative. A useful trick, apart from grouping levels, is to plot the barycentre of a feature’s levels (weighted by the number of observations in each level), as displayed on Figure <a href="#fig:mca" data-reference-type="ref" data-reference="fig:mca">[3]</a>. Note that a low portion of the total variance is explained by the first axes (1.51 % and 1.14 % resp.) since the data, when one-hot encoded, is very high dimensional. As for <span data-acronym-label="pca" data-acronym-form="singular+short">PCA</span>, no groups are formed when displaying the (uninformative) equivalent of Figure <a href="#fig:pca" data-reference-type="ref" data-reference="fig:pca">[2]</a> and no factor level(s) is / are isolated from the others in Figure <a href="#fig:mca" data-reference-type="ref" data-reference="fig:mca">[3]</a>. A practitioner would conclude the absence of segments on which to build several scorecards.</p>
                                    <p>Nevertheless, a method applicable to mixed data exists as well and could directly be applied to “raw” features.</p>
                                    
                                    <figure>
                                    <img src="figures/chapitre6/fig_mca.png" alt="" width="100%"/></span>
                                    <figurecaption><a id="fig:mca">[3]</a> MCA</figurecaption>
                                    </figure>

                                    
                                    <p>The <span data-acronym-label="famd" data-acronym-form="singular+short">FAMD</span> algorithm <a href-"#pages2014multiple" class="citation" data-cites="pages2014multiple">[3]</a> aims at performing both <span data-acronym-label="mca" data-acronym-form="singular+short">MCA</span> on categorical features and <span data-acronym-label="pca" data-acronym-form="singular+short">PCA</span> on continuous features in a simultaneous fashion. Resulting principal component axes depend on both data types, as can be seen from Figure <a href="#fig:famd" data-reference-type="ref" data-reference="fig:famd">[4]</a> (relying again on the FactoMineR <span class="smallcaps">R</span> package <a href="#JSSv025i01" class="citation" data-cites="JSSv025i01">[4]</a>). As on Figures <a href="#fig:pca" data-reference-type="ref" data-reference="fig:pca">[2]</a> and <a href="#fig:mca" data-reference-type="ref" data-reference="fig:mca">[3]</a>, categorical features’ levels’ and continuous features’ contributions to the two first principal components can be displayed on Figure <a href="#fig:famd" data-reference-type="ref" data-reference="fig:famd">[4]</a>, where vehicle brands make it rather hard to read. When switching to the categorical features’ levels barycentre representation, as in Figure <a href="#fig:mca" data-reference-type="ref" data-reference="fig:mca">[3]</a>, interpretation is easier and somewhat similar to the <span data-acronym-label="pca" data-acronym-form="singular+short">PCA</span> method (up to a permutation on the first and second components): the first axis contains information about the client, represented by continuous (<em>e.g.</em> age) and categorical features (<em>e.g.</em> job category) while the second axis is about the loan and the vehicle (its brand, cost, etc.). This method has the advantage of not requiring the <em>Credit Scoring</em> practitioner to preprocess the “raw” data by quantizing it, which could have a huge impact on the results of the subsequent method employed. Moreover, the practitioner would fine-tune the quantization of each sub-population which, if fed back to the <span data-acronym-label="mca" data-acronym-form="singular+short">MCA</span>, would potentially yield completely different results! As for <span data-acronym-label="pca" data-acronym-form="singular+short">PCA</span> and <span data-acronym-label="mca" data-acronym-form="singular+short">MCA</span>, the equivalent of Figure <a href="#fig:pca" data-reference-type="ref" data-reference="fig:pca">[2]</a> (not shown here) for <span data-acronym-label="famd" data-acronym-form="singular+short">FAMD</span> does not display distinguishable groups of observations. Nevertheless, the luxury car brands are now well separated in Figure <a href="#fig:famd" data-reference-type="ref" data-reference="fig:famd">[4]</a> from other continuous features and other categorical features’ levels which would be interpreted by a <em>Credit Scoring</em> practitioner as the need to build a specific scorecard for this market. However, due to the low volumes of applicants and considering that all of them are probably good clients that are all accepted (the score has little relevance for such markets), no segmentation would be performed.</p>
                                    
                                    <figure>
                                    <img src="figures/chapitre6/fig_famd.png" alt="" width="100%"/></span>
                                    <figurecaption><a id="fig:famd">[4]</a> FAMD</figurecaption>
                                    </figure>

                                    
                                    <p>It appears clearly that all these methods do not directly optimize a predictive goal such as the one optimized by logistic regression. Moreover, the <em>ad hoc</em> preprocessing step of quantization might influence the structure of the retained segmentation.</p>
                                    <p>For numerical experiments of Section <a href="#sec:num_exp" data-reference-type="ref" data-reference="sec:num_exp">1.6</a>, we will use, among others, the <span data-acronym-label="famd" data-acronym-form="singular+short">FAMD</span> approach.</p>
                                    
                                    <h3 id="subsec:fail">These practices can fail</h3>
                                    <p>Of course, like all <em>ad hoc</em> methods that rely on “two-stages” procedures (find segments using an <span data-acronym-label="mca" data-acronym-form="singular+short">MCA</span> algorithm and learn separate logistic regression scorecards on them) which do not share a common objective, the aforementioned in-house practice can fail. <em>Credit Scoring</em> practitioners are probably aware that their methods are not bullet-proof, but like most industries, unless provided to them with easily usable software replacing these methods, these practices remain.</p>
                                    <p>This article has no intent in filling that gap but rather to give insights on more elaborate, readily usable methods that will be covered in Section <a href="#sec:literature" data-reference-type="ref" data-reference="sec:literature">1.2</a> and to propose a few ideas for future research. That is why, in the present, we show two data generating mechanisms where current in-house methods fail. In Section <a href="#sec:model_selec_tree" data-reference-type="ref" data-reference="sec:model_selec_tree">1.3</a>, we will propose an <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm that shares similitude with the one proposed for quantization that performs well where current methods fail.</p>
                                    <p>The first of these failing situations is when the <span data-acronym-label="pdf" data-acronym-form="singular+short">pdf</span> of covariates (suppose for simplicity that all of them are continuous) <span class="math inline">\(p(\boldsymbol{x})\) is multi-modal as on Figure <a href="#fig:fail1" data-reference-type="ref" data-reference="fig:fail1">[5]</a> where we distinguish the lower, middle and upper-classes of respective low, average and high wages and indebtedness. An unsupervised generative approach like <span data-acronym-label="pca" data-acronym-form="singular+short">PCA</span> would urge the practitioner to construct 3 scorecards (one for each of the aforementioned classes). However, displaying <span class="math inline"><em>y</em></span> as <span style="color: red">red</span> (resp. <span style="color: green">green</span>) for bad borrowers (resp. good borrowers), we can see that perfect separation can be achieved: it depends solely on the indebtedness level (the ratio of wages over indebtedness). Thus, the resulting scorecards would be asymptotically the same, but they use three times more parameters! In a finite sample setting, it will imply lower performance since each of these coefficients have three times less samples to train on, which amounts to increasing the variance by the same factor. On a practical note, one could argue that it reduces interpretability by adding an avoidable complexity to the decision system. This particular data generating mechanism is revisited in the experiments of Section <a href="#subsec:num_sim" data-reference-type="ref" data-reference="subsec:num_sim">1.6.1</a>.</p>
                                    
                                    <p>The second failing situation is the counterpart of the first tailored data generating mechanism. This time, suppose the covariates wages and indebtedness are uniformly sampled. Suppose there is a third categorical feature “wages source” which is drawn uniformly from three levels: renters, salaried workers and self-employed. One could argue that renters’ risk level do not depend on their indebtedness, which is typically low (and a higher one is a major red flag), salaried workers’ risk level is positively correlated with their indebtedness ratio as was the case for the first introductory example (see Figure <a href="#fig:fail1" data-reference-type="ref" data-reference="fig:fail1">[5]</a>) and self-employed people’s risk level is negatively correlated with this indebtedness ratio (say, the higher their personal engagement, the higher the chances of success of their business). This example data generating mechanism is illustrated on Figure <a href="#fig:fail2" data-reference-type="ref" data-reference="fig:fail2">[6]</a>. In this situation, and contrary to the first example, an unsupervised generative “clustering” algorithm like the projection of the data on the two first <span data-acronym-label="pca" data-acronym-form="singular+short">PCA</span> axes shown here would not partition the data and the <em>Credit Scoring</em> practitioner would construct only one scorecard. This scorecard would have high model bias since it is too simple to accommodate for the variety of the data generating mechanism and consequently perform poorly. This particular data generating mechanism is also revisited in the experiments of Section <a href="#subsec:num_sim" data-reference-type="ref" data-reference="subsec:num_sim">1.6.1</a>.</p>
                                    
                                    
                                    <figure>
                                    <img src="figures/chapitre6/fig_fail_1.png" alt="" width="100%"/></span>
                                    <figurecaption><a id="fig:fail1">[5]</a> First failing situation</figurecaption>
                                    </figure>

                                    <figure>
                                    <img src="figures/chapitre6/fig_fail_2.png" alt="" width="100%"/></span>
                                    <figurecaption><a id="fig:fail2">[6]</a> Second failing situation</figurecaption>
                                    </figure>

                                    
                                    
                                    <h2 id="sec:literature">Literature review</h2>
                                    <p>This section aims at providing an eluded literature review of some well-known supervised "clustering approaches" that could be transposed to the <em>Credit Scoring</em> industry.</p>
                                    <h3 id="subsec:sup_gen">Supervised generative clustering methods</h3>
                                    <p>In the preceding section, examples of classical unsupervised “clustering” methods were given: <span data-acronym-label="pca" data-acronym-form="singular+short">PCA</span> (continuous data), <span data-acronym-label="mca" data-acronym-form="singular+short">MCA</span> (categorical data) and ultimately <span data-acronym-label="famd" data-acronym-form="singular+short">FAMD</span> (mixed data), completed with a projection of the data on their respective two first axes. In this section, focus is given to supervised generative methods. Indeed, a fully generative model <span class="math inline">\(p(\boldsymbol{x},y)\), if sufficiently flexible, could have easily spotted the bottlenecks of the failures of the <span data-acronym-label="pca" data-acronym-form="singular+short">PCA</span> approach illustrated on Figures <a href="#fig:fail1" data-reference-type="ref" data-reference="fig:fail1">[5]</a> and <a href="#fig:fail2" data-reference-type="ref" data-reference="fig:fail2">[6]</a>.</p>
                                    <h5 id="section-3"></h5>
                                    <p>The <span data-acronym-label="pls" data-acronym-form="singular+short">PLS</span> <a href="#wold1984collinearity" class="citation" data-cites="wold1984collinearity">[6]</a> algorithm seeks to combine the strengths, in its original proposal, of <span data-acronym-label="pca" data-acronym-form="singular+short">PCA</span> in explaining the variance of the features <span class="math inline">\(\boldsymbol{x}\) and regression in predicting <span class="math inline"><em>y</em></span> with the resulting principal components. In a classification setting, it is termed PLS-DA where DA stands for discriminant analysis.</p>
                                    <p>The main idea is to construct a first component from the sum of the univariate regressions of <span class="math inline">\(\boldsymbol{\mathbf{x}}_j\) on <span class="math inline">\(\boldsymbol{\mathbf{y}}\), then a second component from the sum of the univariate regressions of <span class="math inline">\(\boldsymbol{\mathbf{x}}_j\) subtracted by the first component on <span class="math inline">\(\boldsymbol{\mathbf{y}}\), and so on. In a sense, a trade-off between reconstruction quality of <span class="math inline">\(\boldsymbol{\mathbf{x}}\) and <span class="math inline">\(\boldsymbol{\mathbf{y}}\) with as few components as possible is achieved.</p>
                                    <p>The <span data-acronym-label="pls" data-acronym-form="singular+short">PLS</span> algorithm was used in <a href="#schwartz2009human" class="citation" data-cites="schwartz2009human">[7]</a> in a classification setting which results in Figure <a href="#fig:pca_vs_pls" data-reference-type="ref" data-reference="fig:pca_vs_pls">[7]</a> reproduced with permission<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. It is striking how classes are better separated when using <span data-acronym-label="pls" data-acronym-form="singular+short">PLS</span>. However, this does not guarantee that the resulting inferred segments’ logistic regression will yield better predictive performance, considering that a <em>Credit Scoring</em> practitioner would effectively spot two groups in Figure <a href="#fig:pca_vs_pls" data-reference-type="ref" data-reference="fig:pca_vs_pls">[7]</a> (right) and separate them on the first <span data-acronym-label="pls" data-acronym-form="singular+short">PLS</span> axis being above or below a threshold of approximately <span class="math inline">0.01</span>. When applied to the Automobile dataset, it does not show such spectacular results (see Figure <a href="#fig:simu_pls" data-reference-type="ref" data-reference="fig:simu_pls">[8]</a>).</p>
                                    <figure>
                                    <img src="figures/chapitre6/pca_vs_pls.png" alt="Cloud points resulting from the application of pca (left) and pls (right) on a binary-labelled multivariate continuous dataset." width="100%"/><figcaption>Cloud points resulting from the application of <span data-acronym-label="pca" data-acronym-form="singular+short">PCA</span> (left) and <span data-acronym-label="pls" data-acronym-form="singular+short">PLS</span> (right) on a binary-labelled multivariate continuous dataset.</figcaption>
                                    </figure>
                                    <p><span id="fig:pca_vs_pls" label="fig:pca_vs_pls">[7]</span></p>
                                    
                                    <figure>
                                    <img src="figures/chapitre6/fig_pls.png" alt="Cloud points resulting from the application of pca (left) and pls (right) on a binary-labelled multivariate continuous dataset." width="100%"/>
                                    </figure>
                                    <p><span id="fig:simu_pls" label="fig:simu_pls">[8]</span></p>

                                    <p>The <span data-acronym-label="spc" data-acronym-form="singular+short">SPC</span> <a href="#bair2006prediction" class="citation" data-cites="bair2006prediction">[8]</a> algorithm is motivated by genomics applications where <span class="math inline"><em>d</em> &gt; <em>n</em></span>, but is applicable to our current setting as well, and by the fact that, in a predictive setting, variance of the features <span class="math inline">\(\boldsymbol{x}\) is only interesting if correlated with <span class="math inline"><em>y</em></span>. The inner-workings of the algorithm are relatively simple: the correlation between each feature <span class="math inline">\(x_j\) and <span class="math inline"><em>y</em></span> is computed. Only the features for which this correlation exceeds a user-defined threshold are retained, and the first few principal components of these features are calculated and used to predict <span class="math inline"><em>y</em></span>.</p>
                                        <p>There is a close link between <span data-acronym-label="pls" data-acronym-form="singular+short">PLS</span> and <span data-acronym-label="spc" data-acronym-form="singular+short">SPC</span> that is thoroughly explained in <a href="#friedman2001elements" class="citation" data-cites="friedman2001elements">[19]</a> Section 18.6.2 p. 680. For numerical experiments of Section <a href="#sec:num_exp" data-reference-type="ref" data-reference="sec:num_exp">1.6</a>, we will use, among others, the <span data-acronym-label="pls" data-acronym-form="singular+short">PLS</span> approach.</p>
                                    
                                    <p>Although these methods make good use of <span class="math inline"><em>y</em></span> in constructing sub-populations on which the practitioner would construct separate scorecards, the resulting segments would be, as described in the <span data-acronym-label="mca" data-acronym-form="singular+short">MCA</span> Section, visually separated clouds of points on the graph of the two first principal components. This paradigm has two major drawbacks: first, as explained in the preceding section, the separation boundary is complex and multivariate (as the two first principal components will most likely involve all features). Second, to make a complete tree as on Figure <a href="#fig:arbre" data-reference-type="ref" data-reference="fig:arbre">[1]</a>, these procedures would have to be repeated “recursively” which yields the need for a stopping criterion and an objective splitting criterion in place of the rather subjective visual separation. Direct approaches of estimating such trees are reviewed in the next section.</p>
                                    <h3 id="subsec:direct">Direct approaches: logistic regression trees</h3>
                                    <h5 id="section-5"></h5>
                                    <p>The first research work focusing on a similar problem than the present one seems to be <span data-acronym-label="lotus" data-acronym-form="singular+short">LOTUS</span> <a href="#chan2004lotus" class="citation" data-cites="chan2004lotus">[9]</a>, where logistic regression trees are constructed so as to select features to split the data on the tree’s nodes which break the linearity assumption of logistic regression. The original article states an application case similar to this one, namely the insurance market.</p>
                                    <p>Their motivation is that logistic regression has a fixed parameter space, defined by the number of input features, whereas trees adapt their flexibility (<em>i.e.</em> depth) to the sample size <span class="math inline"><em>n</em></span>; however, trees perform well for classification (<em>i.e.</em> their label estimates <span class="math inline"><em>ŷ</em></span> can achieve low classification error) but poorly in assessing the probability of the event (<em>i.e.</em> the estimate <span class="math inline">\(\hat{p}(y | \boldsymbol{x})\) is the proportion of the event <span class="math inline"><em>y</em></span> among observations <span class="math inline">\(\boldsymbol{x}\) at each leaf which is arguably not very informative) as it is piecewise constant; if the true decision boundary separating the two classes of <span class="math inline"><em>y</em></span> given <span class="math inline">\(\boldsymbol{x}\) is linear, they need an infinite depth to estimate it as well as logistic regression. Thus, they search for trees which leaves are logistic regression with a few continuous features and which intermediate nodes split the population based on categorical or continuous features which relationship to the log-odd ratio of <span class="math inline"><em>y</em></span> is not linear (<em>i.e.</em> features that would perform poorly in a logistic regression).</p>
                                        <p>They propose a feature selection method for node splits that is claimed to be “bias-free”: the number of partitions of <span class="math inline">\(l_j\) labelled factor levels into <span class="math inline"><em>m</em><sub><em>j</em></sub></span> unlabelled categories (which would here be the tree split criterion and define its sub-populations) is huge which yields overfitting; thus, their approach relies on a <span class="math inline"><em>χ</em><sup>2</sup></span> test which degrees of freedom is linked to the number of potential rearrangements of <span class="math inline">\(l_j\) levels into 2 bins to avoid wrongfully selecting categorical features that have lots of levels. Their optimized criterion is the sum of the log-likelihoods of the logistic regression on the tree’s leaves. Of course, this leads also to overfitting which requires the tree to be pruned (as is classical for classification trees) using a method closely related to the one developed in the classical CART <a href="#cart84" class="citation" data-cites="cart84">[10]</a> algorithm. Lastly, their proposed method is not directly applicable to missing values: these observations are not used during training (in the <em>Credit Scoring</em> industry, there would most likely be at least one missing value for each observation) and during test, their missing values are imputed by the mean or median.</p>
                                    <p>To sum up, although their motivation is similar to the present problem, <span data-acronym-label="lotus" data-acronym-form="singular+short">LOTUS</span> is not directly usable since only continuous features are used as predictive features in the logistic regression of the tree’s leaves, it does not handle missing values gracefully, and there are currently no implementation available in <span class="sans-serif">R</span> or Python.</p>
                                    <h5 id="section-6"></h5>
                                    <p>The second approach very close to our industrial problem is named <span data-acronym-label="lmt" data-acronym-form="singular+short">LMT</span> <a href="#landwehr2005logistic" class="citation" data-cites="landwehr2005logistic">[11]</a>. As for <span data-acronym-label="lotus" data-acronym-form="singular+short">LOTUS</span>, the result is a tree of logistic regression at its leaves and the motivation is very similar. Their introductory example, reproduced here with permission on Figure <a href="#fig:lmt" data-reference-type="ref" data-reference="fig:lmt">[9]</a> is enlightening: a quadratic bivariate boundary cannot be well approximated by trees or logistic regression alone, but a combination of both achieves good performance and interpretation.</p>
                                    <p>Their approach differs however drastically from <span data-acronym-label="lotus" data-acronym-form="singular+short">LOTUS</span> in that they rely on a particular boosting approach derived from the LogitBoost algorithm <a href="#friedman2000additive" class="citation" data-cites="friedman2000additive">[12]</a> to adjust the logistic regression, and an adaptation of the classical C4.5 <a href="#quinlan2014c4" class="citation" data-cites="quinlan2014c4">[13]</a> algorithm to grow the tree. The two central ideas behind their usage of the LogitBoost algorithm are simple: it allows to perform feature selection <em>via</em> a stagewise-like process where one feature enters the model at each step and to recursively “refine” the logistic regression by boosting the logistic regression fitted at a node’s parent. Indeed, a first logistic regression is fitted at the tree’s root via LogitBoost using all observations in <span class="math inline">\(\mathcal{T}_{\text{f}}\), which is further boosted separatly at its subsequent children nodes on sub-populations, say <span class="math inline">\(((\boldsymbol{\mathbf{x}}^1,\boldsymbol{\mathbf{y}}^1), (\boldsymbol{\mathbf{x}}^2,\boldsymbol{\mathbf{y}}^2))\) and so on. This most probably induces less parameter estimation variance in each leaf since they partly benefit from samples not in their leaf but used to fit the parents’ logistic regression. One if its main advantages compared to other approaches is that it is fast. Here again, the resulting tree must be pruned and either a tactic similar to the classical tree algorithm CART, or cross-validation, or the AIC criterion (in a refinement of the method proposed in <a href="#sumner2005speeding" class="citation" data-cites="sumner2005speeding">[14]</a>) are used.</p>
                                    <p>However, categorical features are dummy / one-hot encoded so that only a few factor levels might be selected at each leaf, which amounts to merging the not selected levels into a reference value. Conversely, when used as a split feature, each level yields a distinct branch. Moreover, missing values are imputed by the mean or mode.</p>
                                    <p>Its original implementation is in Java (Weka) but the <span class="sans-serif">R</span> package provides interfaces and wrappers to it. When applied directly to the Automobile dataset, as <span data-acronym-label="lmt" data-acronym-form="singular+short">LMT</span> does not handle missing values, a first preprocessing step is to select only complete observations: there remains only approx. <span class="math inline">4,000</span> observations (among <span class="math inline">50,000</span>) and no segmentation is performed. Due to the use of the LogitBoost algorithm, only a few features are selected: one continuous feature and three particular levels of three different categorical features, yielding a rather low performance of <span class="math inline">44.3</span> Gini points (compared to the current performance of <span class="math inline">55.7</span>) which is nevertheless impressive given the few training observations and features used. To help the <span data-acronym-label="lmt" data-acronym-form="singular+short">LMT</span> algorithm further, features with the highest rate of missing values are deleted; we now have <span class="math inline"><em>d</em> = 21</span> and <span class="math inline"><em>n</em> = 20, 000</span>. Finally, in a third experiment, missing values of categorical features are encoded as a particular level and continuous features’ missing values are imputed by their mean, all observations and features can now be used. In these two last experiments, the LogitBoost algorithm seems to fail since no segmentation is performed and only the age of the client is retained, yielding a low performance (<span class="math inline">30</span> Gini points).</p>
                                    
                                    
                                    <figure>
                                        <img src="figures/chapitre6/lmt_generation.png" alt="[fig:lmt1] Quadratic data generation process: the true boundary depends on the square of input features." width="100%"/><figcaption><span id="fig:lmt" label="fig:lmt1">[9a]</span> Quadratic data generation process: the true boundary depends on the square of input features.</figcaption>
                                    </figure>
                                    
                                    
                                    <figure>
                                        <img src="figures/chapitre6/lmt_tree_1.png" alt="[fig:lmt2] The first split of a classification tree is way too simplistic."  width="100%"/><figcaption><span id="fig:lmt" label="fig:lmt2">[9b]</span> The first split of a classification tree is way too simplistic.</figcaption>
                                    </figure>
                                    
                                    
                                    <figure>
                                        <img src="figures/chapitre6/lmt_tree_2.png" alt="[fig:lmt3] The second split is more helpful."  width="100%"/><figcaption><span id="fig:lmt" label="fig:lmt3">[9c]</span> The second split is more helpful.</figcaption>
                                    </figure>
                                    
                                    
                                    <figure>
                                        <img src="figures/chapitre6/lmt_tree_3.png" alt="[fig:lmt4] The subsequent splits yield overfitting: these nodes shall be pruned."  width="100%"/><figcaption><span id="fig:lmt" label="fig:lmt4">[9d]</span> The subsequent splits yield overfitting: these nodes shall be pruned.</figcaption>
                                    </figure>
                                    
                                    
                                    <figure>
                                        <img src="figures/chapitre6/lmt_logistic.png" alt="[fig:lmt5] A lr tree with only two leaves (and consequently two lr) yields the best result."  width="100%"/><figcaption><span id="fig:lmt" label="fig:lmt5">[9e]</span> A logistic regression tree with only two leaves (and consequently two logistic regression) yields the best result.</figcaption>
                                    </figure>
                                    
                                    
                                    <h5 id="par:mob"></h5>
                                    <p>Lastly, a third approach closely related to our problem is <span data-acronym-label="mob" data-acronym-form="singular+short">MOB</span> <a href="#zeileis2008model" class="citation" data-cites="zeileis2008model">[15]</a> which is an adaptation of a better-known paper <a href="#hothorn2006unbiased" class="citation" data-cites="hothorn2006unbiased">[16]</a> to parametric models in the leaves of a recursively partitioned dataset (hence the name).</p>
                                    <p>Their algorithm consists in fitting the chosen model (in our case, logistic regression) for all observations in <span class="math inline">\(\mathcal{T}_{\text{f}}\) at the current node and decide to split these into subsets based on a correlation measure (several such measures are proposed) of the residuals of the current model and splitting features <span class="math inline"><strong>V</strong> = (<em>V</em><sub>1</sub>, …, <em>V</em><sub><em>p</em></sub>)</span> where <span class="math inline">\(V_j \in \mathbb{R}\) or <span class="math inline">\(\mathbb{N}_{o_j}\), <span class="math inline">1 ≤ <em>j</em> ≤ <em>p</em></span>, which are not necessarily included in <span class="math inline">\(\boldsymbol{\mathbf{x}}\) (they are specified by the user). The procedure is repeated until no significant “correlation” is detected. The C4.5 algorithm, in presence of a binary outcome, orders the levels of categorical features by their proportion of events <span class="math inline"><em>y</em></span> and split as if the feature were ordinal (it can be shown that it is optimal, see <a href="#friedman2001elements" class="citation" data-cites="friedman2001elements">[19]</a> Section 9.2.4). Similarly to <span data-acronym-label="lotus" data-acronym-form="singular+short">LOTUS</span> and contrary to C4.5, <span data-acronym-label="mob" data-acronym-form="singular+short">MOB</span> performs, for example for binary splits and when confronted to categorical features <span class="math inline"><em>j</em></span> having <span class="math inline">\(l_j\) levels, <span class="math inline">\(2^{l_j}\) tests. Moreover, there is no mention of an eventual treatment of missing values. Finally, the number of segments per split is searched exhaustively.</p>
                                    <p>Its implementation is available through the <span class="sans-serif">R</span> packages and which will be used in numerical experiments of Section <a href="#sec:num_exp" data-reference-type="ref" data-reference="sec:num_exp">1.6</a>. It worked well on small toy data, however, on real data, even with complete cases (<span class="math inline"><em>n</em> = 4, 000</span>) and by arbitrarily selecting <span class="math inline"><em>d</em> = 4</span> features, computation took a very long time. With bigger datasets, I got “file size”-related errors.</p>
                                    
                                    <p>To sum up, these direct approaches are far more promising than unsupervised and supervised generative approaches of Sections <a href="#subsec:adhoc" data-reference-type="ref" data-reference="subsec:adhoc">1.1.2</a> and <a href="#subsec:sup_gen" data-reference-type="ref" data-reference="subsec:sup_gen">1.2.1</a> respectively, in that they produce directly the sought tree-structure of Figure <a href="#fig:arbre" data-reference-type="ref" data-reference="fig:arbre">[1]</a> (apart, of course, from quantization <span class="math inline">\(\boldsymbol{q}^1, \dots, \boldsymbol{q}^7\) and interactions <span class="math inline">\(\boldsymbol{\delta}^1, \dots, \boldsymbol{\delta}^7\)). However, their treatment of missing values and categorical features are not satisfactory: classical <em>Credit Scoring</em> data would require preprocessing steps such as imputation or quantization (or at least merging numerous factor levels) which might greatly influence the resulting segmentation as emphasized in Section <a href="#subsec:context" data-reference-type="ref" data-reference="subsec:context">1.1.1</a>. Moreover, quantization has to be segment-specific: on a theoretical note, it participates in reducing model bias; on a practical note, it does not make much sense to use the same quantization of wages on segments of applicants to a leasing for a Ferrari or for a smartphone.</p>
                                    <p>In the next section, we formalize the problem as a model selection problem, similarly to the three approaches presented here, with our own notations introduced in the previous section.</p>
                                    <h2 id="sec:model_selec_tree">Logistic regression trees as a combinatorial model selection problem</h2>
                                    <p>We assume there are <span class="math inline">\(\text{K}^\star\) “clusters” which form the leaves of a tree similar to Figure <a href="#fig:arbre" data-reference-type="ref" data-reference="fig:arbre">[1]</a> and which assigning latent random feature is denoted by <span class="math inline">\(\text{C}^\star\) (lower-case for observations). The other notations employed inspire from the preceding posts: the superscript notation is used to insist on the fact that available features <span class="math inline">\(\boldsymbol{x}^{c^\star}\) differ potentially in each of the scorecards. For <span class="math inline">\(c^\star \in \mathbb{N}_{\text{K}^\star}\), <span class="math inline">\((\boldsymbol{\mathbf{x}}^{c^\star}, \boldsymbol{\mathbf{y}}^{c^\star})\) denotes the subset of observations of <span class="math inline">\((\boldsymbol{\mathbf{x}}_{\text{f}}, \boldsymbol{\mathbf{y}}_{\text{f}})\) for which <span class="math inline">\(\text{C}^\star = c^\star\), such that <span class="math inline">\(\cup_{c^\star=1}^{\text{K}^\star} (\boldsymbol{\mathbf{x}}^{c^\star}, \boldsymbol{\mathbf{y}}^{c^\star}) = (\boldsymbol{\mathbf{x}}_{\text{f}}, \boldsymbol{\mathbf{y}}_{\text{f}})\) and for <span class="math inline"><em>c</em><sup>⋆</sup>, <em>c</em><sup>′⋆</sup></span>, <span class="math inline">\((\boldsymbol{\mathbf{x}}^{c^\star}, \boldsymbol{\mathbf{y}}^{c^\star}) \cap (\boldsymbol{\mathbf{x}}^{c^{'\star}}, \boldsymbol{\mathbf{y}}^{c^{'\star}}) = \varnothing\). It follows that quantizations <span class="math inline">\(\boldsymbol{q}^{c^\star}\) and interactions <span class="math inline">\(\boldsymbol{\delta}^{c^\star}\), discussed in earlier posts are also different. Consequently, the logistic regression coefficients <span class="math inline">\(\boldsymbol{\theta}^{\star,c^\star}\) are also obviously different. In this section, we drop the quantization and interactions requirement, such that: <br /><span class="math display">$$\label{eq:lr_tree}
                                        \forall \boldsymbol{x}, y, \exists c^\star \in \mathbb{N}_{\text{K}^\star}, \boldsymbol{\theta}^{\star,c^\star} \in \Theta^{\star,c^\star}, p(y | \boldsymbol{x}) = p_{\boldsymbol{\theta}^{\star,c^\star}}(y | \boldsymbol{x}).$$ </span><br /> The membership of an observation <span class="math inline">\(\boldsymbol{x}\) to a segment <span class="math inline">\(\text{c}\) is given by a tree. We restrict to binary trees for simplicity, such that a segment <span class="math inline">\(\text{c}\) with depth <span class="math inline">𝒟(<em>c</em>)</span> has <span class="math inline"><em>r</em> = 1, …, 𝒟(<em>c</em>)</span> parents successively denoted by <span class="math inline">𝒫<em>a</em><sup><em>r</em></sup>(<em>c</em>)</span>. At these parent nodes, a binary rule is taken. This rule is univariate: it depends on only one feature <span class="math inline">\(x_{\sigma(r,c)}\) where <span class="math inline"><em>σ</em>(<em>r</em>, <em>c</em>)</span> denotes the anti-rank of the feature used in rule <span class="math inline"><em>r</em></span> for segment <span class="math inline">\(\text{c}\). Being a binary rule, the membership of <span class="math inline">\(x_{\sigma(r,c)}\) is tested between <span class="math inline"><em>C</em><sub>𝒫<em>a</em><sup><em>r</em></sup>(<em>c</em>), 1</sub></span> and <span class="math inline"><em>C</em><sub>𝒫<em>a</em><sup><em>r</em></sup>(<em>c</em>), 2</sub></span> such that <span class="math inline">\(C_{\mathcal{P}a^r(c),1} \cup C_{\mathcal{P}a^r(c),2} =\mathbb{R}\) for continuous features (half-spaces), or <span class="math inline">\(\mathbb{N}_{l_{\sigma(r,c)}}\) for categorical features respectively. This membership is denoted by <span class="math inline"><em>λ</em>(<em>r</em>, <em>c</em>)</span> such that <span class="math inline">\(x_{\sigma(r,c)} \in C_{\mathcal{P}a^r(c),\lambda(r,c)}\). With all these newly introduced notations, the probability of a segment <span class="math inline">\(\text{c}\) given covariates <span class="math inline">\(\boldsymbol{x}\) can be expressed as: <br /><span class="math display">$$\label{eq:tree}
                                            p( c | \boldsymbol{x}) = \prod_{r = 1}^{\mathcal{D}(c)} {1}_{C_{\mathcal{P}a^r(c),\lambda(r,c)}} (x_{\sigma(r,c)}).$$ </span><br /> An example is given on Figure <a href="#fig_arbre_ex_notations" data-reference-type="ref" data-reference="fig_arbre_ex_notations">[10]</a>.</p>

                                        <figure>
                                            <img id="#fig_arbre_ex_notations" src="figures/chapitre6/fig_arbre_ex_notations.png" alt="" width="100%"/>[10]
                                        </figure>

                                        <p>The above mentioned algorithms <span data-acronym-label="lotus" data-acronym-form="singular+short">LOTUS</span>, <span data-acronym-label="lmt" data-acronym-form="singular+short">LMT</span> and <span data-acronym-label="mob" data-acronym-form="singular+short">MOB</span> optimized the sum of the segments’ log-likelihoods, then needed pruning since it leads to obvious overfitting: infinite log-likelihood is achievable by putting each sample into its own segment, provided there is at least one continuous feature and no identical examples with different labels, or combinations of categorical features’ levels that separate classes perfectly.</p>
                                        <p>Another approach can be taken by considering the segment <span class="math inline">\(\text{c}\) as a latent random feature: <br /><span class="math display">$$\begin{aligned}
                                            p(\boldsymbol{\mathbf{x}}_{\text{f}},\boldsymbol{\mathbf{y}}_{\text{f}}) &amp; =  \sum_{c=1}^{\text{K}^\star} p(\boldsymbol{\mathbf{y}}_{\text{f}} | \boldsymbol{\mathbf{x}}_{\text{f}}, c) p(c | \boldsymbol{\mathbf{x}}_{\text{f}}) p(\boldsymbol{\mathbf{x}}_{\text{f}}) &amp; \text{($p(c | \boldsymbol{x})$ is non-zero only for $c = c^\star$)} \nonumber \\
                                            &amp; = \prod_{c^\star=1}^{\text{K}^\star} p(\boldsymbol{\mathbf{y}}^{c^\star} | \boldsymbol{\mathbf{x}}^{c^\star}, c^\star) p(\boldsymbol{\mathbf{x}}_{\text{f}}) \nonumber \\
                                            &amp; = \prod_{c^\star=1}^{\text{K}^\star} \int_{\Theta^{\star,c^\star}} p_{\boldsymbol{\theta}^{\star, c^\star}}(\boldsymbol{\mathbf{y}}^{c^\star} | \boldsymbol{\mathbf{x}}^{c^\star}) p(\boldsymbol{\theta}^{\star, c^\star} | c^\star) d\boldsymbol{\theta}^{\star, c^\star} p(\boldsymbol{\mathbf{x}}_{\text{f}}).
                                                \end{aligned}$$ </span><br /> Thus, we have: <br /><span class="math display">$$\begin{aligned}
                                                \ln p(\boldsymbol{\mathbf{x}}_{\text{f}},\boldsymbol{\mathbf{y}}_{\text{f}}) &amp; =  \sum_{c^\star = 1}^{K^\star} \int_{\Theta^{\star,c^\star}} \ln p_{\boldsymbol{\theta}^{\star, c^\star}}(\boldsymbol{\mathbf{y}}^{c^\star} | \boldsymbol{\mathbf{x}}^{c^\star}) p(\boldsymbol{\theta}^{\star, c^\star} | c^\star) d\boldsymbol{\theta}^{\star, c^\star} + \ln p(\boldsymbol{\mathbf{x}}_{\text{f}}) \\
                                                &amp; \approx - \sum_{c^\star=1}^{K^\star} \text{BIC}(\theta^{\star,c^\star})/2 + O(K^\star) + \ln p(\boldsymbol{\mathbf{x}}_{\text{f}}).\end{aligned}$$ </span><br /> Since in our application, the number of sample <span class="math inline"><em>n</em> ≈ 10<sup>6</sup></span> is large and the number of desired segments <span class="math inline"><em>K</em><sup>⋆</sup> ≈ 10</span> is low, we use the following criterion to select a segmentation: <br /><span class="math display">$$\label{eq:BICc}
                                                    (\text{K}^\star, c^\star) = \arg\!\min_{\text{K},c} \sum_{c=1}^{\text{K}} \text{BIC}(\hat{\boldsymbol{\theta}}^c).$$ </span><br /> As was thoroughly explained for quantizations and interactions in earlier posts, it is unclear how many parameters should be accounted for in this BIC criterion since the tree (see Equation of <span class="math inline">\(p(c | \boldsymbol{x})\)</span> above) has “parameters”, in the sense that it selects a splitting feature and a splitting criterion, which have to be estimated (this is somewhat reflected in the Equation of <span class="math inline">\(\ln p(\boldsymbol{\mathbf{x}}_{\text{f}},\boldsymbol{\mathbf{y}}_{\text{f}})\)</span> above by the <span class="math inline">\(p(\boldsymbol{\theta}^c | c)\) term); some are continuous (when the split is done on a continuous feature), some are discrete (when it concerns a categorical feature). As discussed in Section <a href="#par:consistency" data-reference-type="ref" data-reference="par:consistency">[par:consistency]</a>, discrete parameters are usually not counted, but here, following the C4.5 approach of considering the levels of categorical features as ordered (w.r.t. the proportion of events <span class="math inline"><em>y</em></span> associated to them - see Section <a href="#subsec:sup_gen" data-reference-type="ref" data-reference="subsec:sup_gen">1.2.1</a>), a split on categorical features can count as one continuous parameter. However, when there are more than two classes <span class="math inline">\(\text{c}\) (typically, a financial institution of moderate to big size would have <span class="math inline">\(\text{K} = 4\) to <span class="math inline">30</span> scorecards), this “ordering” simplification about the search for discrete parameters does not apply. We still stick with the BIC criterion above as it will show good empirical properties in Section <a href="#sec:num_exp" data-reference-type="ref" data-reference="sec:num_exp">1.6</a>.</p>
                                                <p>In the next section, we propose to relax the constraint of the tree structure, exactly as was done for quantizations in earlier posts, by using a continuous approximation of this discrete problem (and thus highly difficult to optimize directly).</p>
                                                <h2 id="a-mixture-and-latent-feature-based-relaxation">A mixture and latent feature-based relaxation</h2>
                                                <p>The difficulty in optimizing the sum of the BIC criteria directly lies in the discrete nature of <span class="math inline">\(\text{c}\) given <span class="math inline">\(\boldsymbol{x}\), illustrated by the profusion of indicator functions in the Equation of the tree, which is very similar to the problems of quantization and interaction screening. In both cases, highly-combinatorial discrete problems were relaxed, by approaching door functions by softmax and relying on MCMC methods. A somewhat similar approach can be taken here to design a “smooth” approximation of <span class="math inline">\(p(c | \boldsymbol{x})\) which will be denoted by <span class="math inline">\(p_{\boldsymbol{\beta}}(c | \boldsymbol{x})\): the classical probability estimate of decision trees consisting in the proportion of training examples in each class in all leaves.</p>
                                                <h3 id="subsec:relax_tree">The proposed relaxation: tree structure and piecewise constant membership probability</h3>
                                                <p>As emphasized in Section <a href="#subsec:sup_gen" data-reference-type="ref" data-reference="subsec:sup_gen">1.2.1</a>, classification trees aim at predicting <span class="math inline">\(\text{c}\) by making their leaves as “pure” as possible (hence the use of the term “impurity measure” to designate their optimized criterion), <em>i.e.</em> where one class strongly dominates the others by being the labels of most observations that fall into it. However, as for logistic regression, they can be viewed as probabilistic classifiers by substituting their classical majority vote by the proportion of each class in each leaf: <br /><span class="math display">$$\label{eq:tree_leaf}
                                                    p_{\boldsymbol{\beta}}(c | \boldsymbol{x}) = \frac{|\mathbf{c}^{\mathcal{L}(\boldsymbol{x}_{\text{f}})}|}{|\boldsymbol{\mathbf{x}}^{\mathcal{L}(\boldsymbol{x})}|},
                                                    %p_{\boldsymbol{\beta}}(c | \boldsymbol{x}) = p(c | \mathcal{L}(\boldsymbol{x})),$$ </span><br /> where <span class="math inline">\(\mathcal{L}(\boldsymbol{x})\) denotes the leaf where <span class="math inline">\(\boldsymbol{x}\) falls, <span class="math inline">\(|\mathbf{c}^{\mathcal{L}(\boldsymbol{x})}|\) the number of training examples in <span class="math inline">\(\boldsymbol{\mathbf{x}}\) of class <span class="math inline"><em>c</em></span>, the number of training examples in <span class="math inline">\(\boldsymbol{\mathbf{x}}\) falling in leaf <span class="math inline">\(\mathcal{L}(\boldsymbol{x}_{\text{f}})\), and <span class="math inline"><em>β</em></span> is sloppily used to denote all parameters involved in classical classification tree methods such as CART and C4.5. Indeed, in this soft assignment, <span class="math inline">\(\mathcal{L}(\boldsymbol{x})\) and its segment <span class="math inline">\(\text{c}\) are not identifiable anymore. An example of such behaviour is given on Figure <a href="#fig:titanic_tree" data-reference-type="ref" data-reference="fig:titanic_tree">[11]</a> where there are two classes: “survived” and “not survived” for Titanic passengers given their age, sex and passenger class. The proportion of each class in each leaf is given in parentheses.</p>
                                                
                                                <figure>
                                                    <img id="#fig:titanic_tree" src="figures/chapitre6/fig_titanic.png" alt="" width="100%"/>[11]
                                                </figure>

                                                <p>This “soft” assignment will be useful to design an algorithm that does not greedily evaluate all possible segmentations of the form of a tree and its subsequent logistic regressions. A softmax could have been used, but would have yielded a major drawback: the assignment decisions would have been multivariate, thus losing the interpretability of the tree structure. Using this new parametrization, we get a mixture model: <br /><span class="math display">$$\begin{aligned}
                                                    p(y | \boldsymbol{x}) &amp; = \sum_{c = 1}^{\text{K}} p_{\boldsymbol{\theta}^c}(y | \boldsymbol{x}, c) p_{\boldsymbol{\beta}}(c | \boldsymbol{x}),\end{aligned}$$ </span><br /> where feature <span class="math inline">\(\text{c}\) is latent and which makes immediately think of a straightforward estimation strategy: the <span data-acronym-label="em" data-acronym-form="singular+short">EM</span> algorithm. Indeed, it can be easily remarked that: <br /><span class="math display">$$\begin{aligned}
                                                        p(c | \boldsymbol{x}, y) &amp; \propto p_{\boldsymbol{\theta}^c}(y | \boldsymbol{x}, c) p_{\boldsymbol{\beta}}(c | \boldsymbol{x}),\end{aligned}$$ </span><br /> which will be at the basis of the <span data-acronym-label="em" data-acronym-form="singular+short">EM</span>’s fuzzy assignment among segments, detailed in the next section.</p>
                                                    <h3 id="a-classical-em-estimation-strategy">A classical <span data-acronym-label="em" data-acronym-form="singular+short">EM</span> estimation strategy</h3>
                                                    <p>Following the preceding section, we would like to maximize the following likelihood, both in terms of the segments and their resulting logistic regressions: <br /><span class="math display">$$\ell(\boldsymbol{\beta},\text{K},(\boldsymbol{\theta}^c)_1^{\text{K}} ; \boldsymbol{\mathbf{x}}_{\text{f}}, \boldsymbol{\mathbf{y}}_{\text{f}}) = \sum_{c=1}^{\text{K}} \sum_{i=1}^n \ln p_{\boldsymbol{\theta}^c}(y_i | \boldsymbol{x}_i, c) p_{\boldsymbol{\beta}}(c | \boldsymbol{x}_i).$$ </span><br /> The <span data-acronym-label="em" data-acronym-form="singular+short">EM</span> algorithm <a href="#dempster1977maximum" class="citation" data-cites="dempster1977maximum">[20]</a> is an iterative method that can be used to estimate the <em>maximum a posteriori</em> of <span class="math inline">\(p(c | \boldsymbol{x}, y)\), since <span class="math inline">\(\text{c}\) is latent, and alternates between the expectation (E-)step, which computes the relative membership of the observations into each segment, and a maximization (M-)step, which computes the <span data-acronym-label="mle" data-acronym-form="singular+short">mle</span> of the parameters of the log-likelihoods of each segment’s logistic regression and the tree structure. These new logistic regression and tree estimates are then used to determine the distribution of the latent variables in the next E-step. Considering the number of segments <span class="math inline">\(\text{K}\) fixed, the E- and M-steps of the <span data-acronym-label="em" data-acronym-form="singular+short">EM</span> can be derived as follows.</p>
                                                    <h5 id="e-step">E-step</h5>
                                                    <p>At iteration <span class="math inline">(<em>s</em> + 1)</span>, the membership of an observation <span class="math inline"><em>i</em></span> to segment <span class="math inline">\(\text{c}\) can be computed as: <br /><span class="math display">$$t_{i,c}^{(s+1)} = \frac{p_{\boldsymbol{\theta}^{c(s)}}(y_i | \boldsymbol{x}_i) p_{\boldsymbol{\beta}^{(s)}}(c | \boldsymbol{x}_i) }{ \sum_{c'=1}^{\text{K}} p_{\boldsymbol{\theta}^{c{'}{(s)}}}(y_i | \boldsymbol{x}_i) p_{\boldsymbol{\beta}^{(s)}}(c{'} | \boldsymbol{x}_i) }.$$ </span><br /> For notational convenience, we denote the matrix of partial membership of all observations to all segments as <span class="math inline">\(\mathbf{t} = (t_{i,c})_{1\leq i \leq n, 1 \leq c \leq \text{K}}\).</p>
                                                    <h5 id="m1-step">M1-step</h5>
                                                    <p>The previous E-step allows to derive the new <span data-acronym-label="mle" data-acronym-form="singular+short">mle</span> of the logistic regression parameters of each segment <span class="math inline">\(\text{c}\) as: <br /><span class="math display">$$\begin{aligned}
                                                        \boldsymbol{\theta}^{c(s+1)} &amp; = \arg\!\max_{\boldsymbol{\theta}^c} \mathbb{E}[\ell(\boldsymbol{\beta}, \text{K}, (\boldsymbol{\theta}^{c{'}})_1^{\text{K}}; \boldsymbol{\mathbf{x}}_{\text{f}}, \boldsymbol{\mathbf{y}}_{\text{f}}, \mathbf{t}^{(s+1)}) | (\boldsymbol{\theta}^{(c(s)})_1^{\text{K}}, \boldsymbol{\beta}^{(s)}, \text{K}] \\
                                                        &amp; = \arg\!\max_{\boldsymbol{\theta}} \sum_{i=1}^n t_{i,c}^{(s+1)} \ln p_{\boldsymbol{\theta}^c}(y_i | \boldsymbol{x}_i).\end{aligned}$$ </span><br /></p>
                                                    <h5 id="m2-step">M2-step</h5>
                                                    <p>Similarly, a new tree structure can be derived by the new <span data-acronym-label="mle" data-acronym-form="singular+short">mle</span> of its parameter <span class="math inline">\(\boldsymbol{\beta}\): <br /><span class="math display">$$\begin{aligned}
                                                        \boldsymbol{\beta}^{(s+1)} &amp; = \arg\!\max_{\boldsymbol{\beta}} \mathbb{E}[\ell(\boldsymbol{\beta}, \text{K}, (\boldsymbol{\theta}^c)_1^{\text{K}}; \boldsymbol{\mathbf{x}}_{\text{f}}, \boldsymbol{\mathbf{y}}_{\text{f}}, \mathbf{t}^{(s+1)}) | \boldsymbol{\theta}^{(c(s)}, \boldsymbol{\beta}^{(s)}, \text{K}] \\
                                                        &amp; = \arg\!\max_{\boldsymbol{\beta}} \sum_{i=1}^n \sum_{c=1}^{\text{K}} t_{i,c}^{(s+1)} \ln p_{\boldsymbol{\beta}}( c | \boldsymbol{x}_i),\end{aligned}$$ </span><br /> where <span class="math inline">\(p_{\boldsymbol{\beta}}( c | \boldsymbol{x}_i)\) is estimated by relative frequency in each leaf, such that <span class="math inline">\(p_{\boldsymbol{\beta}}(c | \boldsymbol{x}) = \frac{|\mathbf{c}^{\mathcal{L}(\boldsymbol{x})}|}{|\boldsymbol{\mathbf{x}}^{\mathcal{L}(\boldsymbol{x})}|}\). Unfortunately, tree induction methods like CART or C4.5 do not follow a maximum likelihood approach, so that they rather try to minimize a so-called impurity measure, the Gini index or the entropy, respectively. However, since it is hoped that segments <span class="math inline"><em>c</em><sup>⋆</sup></span> are “peaks” of the distribution <span class="math inline">\(p_{\boldsymbol{\beta}}( c | \boldsymbol{x})\), just as it was supposed that the best quantization <span class="math inline">\(\boldsymbol{\mathfrak{\boldsymbol{q}}}^\star\) dominated its posterior <span data-acronym-label="pdf" data-acronym-form="singular+short">pdf</span> in the <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm, we assume the log-likelihood can be approximated by the entropy: <br /><span class="math display">$$\boldsymbol{\beta}^{(s+1)} \approx \arg\!\max_{\boldsymbol{\beta}} \sum_{i=1}^n \sum_{c=1}^{\text{K}} t_{i,c}^{(s+1)} \underbrace{p_{\boldsymbol{\beta}}( c | \boldsymbol{x}_i)}_{\begin{cases} \approx 1 \text{ for } c = c^\star, \\ 0 \text{ otherwise.} \end{cases}} \ln p_{\boldsymbol{\beta}}( c | \boldsymbol{x}_i).$$ </span><br /> This last formulation allows to obtain <span class="math inline">\(\boldsymbol{\beta}^{(s)}\) from a simple application of the C4.5 algorithm, with observations properly weighted by <span class="math inline"><em>t</em><sub><em>i</em>, <em>c</em></sub></span>.</p>
                                                    <p>However, this approach suffers from two main drawbacks: first, all observations are used in all logistic regression <span class="math inline">\(p_{\boldsymbol{\theta}^c}\) which might be problematic with real data since there will be “blocks” of available features (<em>e.g.</em> vehicle information); second, all possible values of <span class="math inline">\(\text{K}\) must be iterated through since the <span data-acronym-label="em" data-acronym-form="singular+short">EM</span> algorithm does not allow for the disappearance of a segment <span class="math inline">\(\text{c}\) contrary to the <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> approach developed hereafter.</p>
                                                    <h3 id="subsec:sem">An <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> estimation strategy</h3>
                                                    <p>In a similar fashion as the MCMC approaches developed in earlier posts where a “clever” quantization (resp. interaction matrix) was drawn and evaluated at each step, refining it for the subsequent steps, a straightforward way of building logistic regression trees is to propose a tree structure, fit logistic regression at its leaves, and evaluate the goodness-of-fit using the sum of the BIC criteria of the resulting logistic regressions. This is somehow the way <span data-acronym-label="lmt" data-acronym-form="singular+short">LMT</span> works: a tree structure is proposed based on C4.5, logistic regression are fitted using the LogitBoost algorithm, and the tree is pruned back using a goodness-of-fit criterion.</p>
                                                    <p>Similarly to the quantization and the interaction screening problems, doing so for all possible tree structures is intractable, so that a way of generating “good” candidates can be designed by relying on an <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm, which we call <em>glmtree</em>. The E-step of the previous section is thus replaced by a Stochastic (S-) step which has some consequences on the M-steps.</p>
                                                    <h5 id="s-step">S-step</h5>
                                                    <p>The “soft” assignment of the <span data-acronym-label="em" data-acronym-form="singular+short">EM</span> algorithm of the previous section is hereby replaced by a “hard” stochastic assignment such that: <br /><span class="math display">$$c_i^{(s+1)} \sim p_{\boldsymbol{\theta}^{\cdot(s)}}(y_i | \boldsymbol{x}_i) p_{\boldsymbol{\beta}^{(s)}}(\cdot | \boldsymbol{x}_i).$$ </span><br /></p>
                                                    <h5 id="m1-step-1">M1-step</h5>
                                                    <p>Thanks to the previous step, the segments are now “hardly” assigned such that the logistic regression are estimated using only observations affected to their segment: <br /><span class="math display">$$\begin{aligned}
                                                        \boldsymbol{\theta}^{c(s+1)} &amp; = \arg\!\max_{\boldsymbol{\theta}^c} \ell(\boldsymbol{\theta};\boldsymbol{\mathbf{x}}^{c(s+1)},\boldsymbol{\mathbf{y}}^{c(s+1)}) \\
                                                        &amp; = \arg\!\max_{\boldsymbol{\theta}^c} \sum_{i=1}^n {1}_{c}(c_i^{s+1)}) \ln p_{\boldsymbol{\theta}^c}(y_i | \boldsymbol{x}_i, c).\end{aligned}$$ </span><br /></p>
                                                    <h5 id="m2-step-1">M2-step</h5>
                                                    <p>Similarly, a new tree structure is given by: <br /><span class="math display">$$\begin{aligned}
                                                        \boldsymbol{\beta}^{(s)} &amp; = \arg\!\max_{\boldsymbol{\beta}} \ln p_{\boldsymbol{\beta}}(c_i | \boldsymbol{x}_i) \\
                                                        &amp; = \arg\!\max_{\boldsymbol{\beta}} \ell(\boldsymbol{\beta}; \boldsymbol{\mathbf{x}}, \mathbf{\boldsymbol{c}}).\end{aligned}$$ </span><br /> This last expression is again approximated by C4.5’s impurity measure: the entropy. Without more theoretical and empirical work, it is unclear which of the <span data-acronym-label="em" data-acronym-form="singular+short">EM</span> and <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> approaches will perform best. However, as mentioned earlier, this <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm calls for an easy integration with the quantization and interaction screening methods proposed in earlier posts.</p>
                                                    <h3 id="subsec:hard_seg">Choosing an appropriate number of “hard” segments</h3>
                                                    <h5 id="going-back-to-hard-segments">Going back to “hard” segments</h5>
                                                    <p>The motivation of Section <a href="#subsec:relax_tree" data-reference-type="ref" data-reference="subsec:relax_tree">1.4.1</a> was to propose a relaxation of the tree structure so that an iterative estimation, be it an <span data-acronym-label="em" data-acronym-form="singular+short">EM</span> or an <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm, could be carried out. In an earlier post, a similar relaxation was proposed for quantization, which lead us to propose a “soft” quantization <span class="math inline">\(\boldsymbol{q}_{\boldsymbol{\alpha}}(\cdot)\) or <span class="math inline">\(p_{\boldsymbol{\alpha}}(\boldsymbol{\mathfrak{\boldsymbol{q}}}_j | \cdot)\) for the neural network and the <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> approaches respectively. These relaxations allowed quantized features to be “partly” in all intervals or groups for continuous or categorical features respectively. Thus, to get back to the original quantization problem, a <em>maximum a posteriori</em> scheme was introduced in an earlier post to deduce “hard” quantizations from this relaxation. In our tree setting, a similar approach has to be taken: this soft segmentation can be interpreted as a mixture of logistic regression which implies that all applicants are scored by all scorecards which is arguably not interpretable. An assignment of each applicant <span class="math inline"><em>i</em></span> to a single scorecard, <em>i.e.</em> to a leaf of the segmentation tree, is easily done again by a <em>maximum a posteriori</em> step such that: <br /><span class="math display">$$\label{eq:max_seg}
                                                        \hat{c}_i^{(s)} = \arg\!\max_c p_{\boldsymbol{\beta}^{(s)}}(c | \boldsymbol{x}_i).$$ </span><br /></p>
                                                    <h5 id="segmentation-candidates">Segmentation candidates</h5>
                                                    <p>Similarly to the neural network architecture introduced in San earlier post, the <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm which proposed quantization candidates and the Metropolis-Hastings algorithm for pairwise interaction screening, both introduced in earlier posts, the <span data-acronym-label="em" data-acronym-form="singular+short">EM</span> and <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> strategies introduced in the two previous sections for segmentation are merely “segments providers”. Indeed, through the iterations <span class="math inline">1</span> to <span class="math inline"><em>S</em></span>, as argued in the preceding paragraph, segmentations <span class="math inline">\(\hat{\mathbf{c}}^{(1)}, \dots \hat{\mathbf{c}}^{(S)}\) are proposed through a <em>maximum a posteriori</em> rule parallel to these algorithms. These candidates are then reintroduced to our original criterion (sum of BIC criteria) and the best performing segmentation is found according to: <br /><span class="math display">$$\label{eq:BICtree}
                                                        s^\star = \arg\!\min_s \text{BIC}(\hat{\boldsymbol{\theta}}^{c^{(s)}}).$$ </span><br />
                                                    </p>
                                                    <h5 id="exploring-a-fewer-number-of-segments">Exploring a fewer number of segments</h5>
                                                    <p>In the preceding sections, the number of segments <span class="math inline">\(\text{K}\) was assumed to be fixed. However, the <em>maximum a posteriori</em> scheme introduced in this section allows, similarly to the one used to go from “soft” (<span class="math inline">\(\boldsymbol{q}_{\boldsymbol{\alpha}}(\cdot)\) or <span class="math inline">\(p_{\boldsymbol{\alpha}}(\boldsymbol{\mathfrak{\boldsymbol{q}}}_j | \cdot)\)) to “hard” (<span class="math inline">\(\boldsymbol{q}(\cdot)\)) quantizations, to explore a number of segments potentially way lower than <span class="math inline">\(\text{K}\): for a fixed segment <span class="math inline">\(\text{c}\), if there is no observation <span class="math inline"><em>i</em></span> such that <span class="math inline">\(p_{\boldsymbol{\beta}}(c | \boldsymbol{x}_i)\) &gt; <span class="math inline">\(p_{\boldsymbol{\beta}}(c' | \boldsymbol{x}_i)\) for <span class="math inline"><em>c</em>′ ≠ <em>c</em></span>, than the segment is empty, which is equivalent to producing a segmentation in <span class="math inline">\(\text{K}-1\) segments. Supplemental to this thresholding effect, the use of an <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm makes it possible to enforce this phenomenon: as <span class="math inline">\(\text{c}\) is drawn in the S-step, and as was argued for quantizations with an <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm, there is a non-zero probability of not drawing a particular segment <span class="math inline">\(\text{c}\) at a given step <span class="math inline">(<em>s</em>)</span>. When run long enough, the chain will stop with <span class="math inline">\(\text{K} = 1\), just like the <em>glmdisc</em>-SEM algorithm could be run until all features are quantized in one level. This can be seen as a strength since it does not require to loop on the number of segments <span class="math inline">\(\text{K}\) which would be required for an <span data-acronym-label="em" data-acronym-form="singular+short">EM</span> algorithm, which is why focus is given to the <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm in what follows.</p>
                                                    <h2 id="sec:adding_quant">Extension to quantization and interactions</h2>
                                                    <p>The <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> estimation strategy proposed in the previous section has one clear advantage: it could easily be used in conjunction with the <em>glmdisc</em>-SEM algorithm proposed in earlier posts for quantization and interaction screening. Following the preceding sections, we would like to maximize the following likelihood, both in terms of segments, the quantizations in each segment and the resulting logistic regressions: <br /><span class="math display">$$\ell((\boldsymbol{\alpha}^c)_1^{\text{K}}, \boldsymbol{\beta},\text{K},(\boldsymbol{\theta}^c)_1^{\text{K}} ; \boldsymbol{\mathbf{x}}_{\text{f}}, \boldsymbol{\mathbf{y}}_{\text{f}}) = \sum_{c=1}^{\text{K}} \sum_{i=1}^n \ln p_{\boldsymbol{\theta}^c}(y_i | \boldsymbol{\mathfrak{\boldsymbol{q}}}_i, c) p_{\boldsymbol{\beta}}(c | \boldsymbol{x}_i) p_{\boldsymbol{\alpha}^c}(\boldsymbol{\mathfrak{\boldsymbol{q}}}_i | \boldsymbol{x}_i).$$ </span><br /></p>
                                                    <p>The following modifications to the two <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithms (for the quantization and segmentation problems) previously proposed would have to be performed:</p>
                                                    <h5 id="s1-step">S1-step</h5>
                                                    <p>The segment is drawn, for an observation <span class="math inline"><em>i</em></span> such that <span class="math inline">\(\boldsymbol{x}_i\) belongs to segment <span class="math inline">\(\text{c}\), according to: <br /><span class="math display">$$c_i^{(s+1)} \sim p_{\boldsymbol{\theta}^{\cdot(s)}}(y_i | \boldsymbol{\mathfrak{\boldsymbol{q}}}^{\cdot(s)},\boldsymbol{\delta}^{\cdot(s)}) p_{\boldsymbol{\beta}^{(s)}}(\cdot | \boldsymbol{x}_i).$$ </span><br /></p>
                                                    <h5 id="s2-step">S2-step</h5>
                                                    <p>The <em>glmdisc</em>-SEM performs the subsequent S-steps. The quantization is drawn according to: <br /><span class="math display">$$\boldsymbol{\mathfrak{\boldsymbol{q}}}_{i,j}^{c(s+1)} \sim p_{}(y_i | \boldsymbol{\mathfrak{\boldsymbol{q}}}_{i,-\{j\}}, \cdot, \boldsymbol{\delta}^{c(s)}) p_{\boldsymbol{\alpha}_j^{c(s)}}(\cdot | x_{i,j}).$$ </span><br /></p>
                                                    <h5 id="s3-step">S3-step</h5>
                                                    <p>The interaction matrix is drawn following the Metropolis-Hastings approach developed in earlier posts and denoted for simplicity as MH here: <br /><span class="math display">$$\boldsymbol{\delta}^{c(s+1)} \sim \text{MH}(\boldsymbol{\delta}^{c(s)}, \boldsymbol{\mathbf{q}}^{c(s+1)}, \boldsymbol{\mathbf{y}}^{c(s+1)}).$$ </span><br /></p>
                                                    <h5 id="m1-step-2">M1-step</h5>
                                                    <p>The logistic regression parameters are obtained in each segment by using the appropriate quantization, interaction matrix and observations: <br /><span class="math display">$$\boldsymbol{\theta}^{c(s+1)} = \arg\!\max_{\boldsymbol{\theta}} \ell(\boldsymbol{\theta};\boldsymbol{\mathbf{x}}^{c(s+1)},\boldsymbol{\mathbf{y}}^{c(s+1)}, \boldsymbol{\delta}^{c(s+1)}).$$ </span><br /></p>
                                                    <h5 id="m2-step-2">M2-step</h5>
                                                    <p>In each segment and for each predictive feature in this particular segment, polytomous logistic links are fitted between the “soft” quantization and the raw feature: <br /><span class="math display">$$\boldsymbol{\alpha}_j^{c(s+1)} = \arg\!\max_{\boldsymbol{\alpha}_j} \ell(\boldsymbol{\alpha}_j;\boldsymbol{\mathbf{x}}_j^{c(s+1)},\boldsymbol{\mathbf{q}}_j^{c(s+1)}).$$ </span><br /></p>
                                                    <h5 id="m3-step">M3-step</h5>
                                                    <p>The tree-structure is obtained again via the C4.5 algorithm as an approximation of: <br /><span class="math display">$$\boldsymbol{\beta}^{(s+1)} = \arg\!\max_{\boldsymbol{\beta}} \ell(\boldsymbol{\beta}, \boldsymbol{\mathbf{x}}, \mathbf{\boldsymbol{c}}^{(s+1)}).$$ </span><br /> Parallel to this <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm, “hard” quantizations are obtained by performing a <em>maximum a posteriori</em> operation on the quantization probability <span class="math inline">\(p_{\boldsymbol{\alpha}}\)</span>: <br /><span class="math display">$$\hat{q}_{j,h}^{c(s)}(\cdot) = 1 \text{ if } h = \arg\!\max_{1 \leq h' \leq m_j} p_{{\boldsymbol{\alpha}}_{j,h'}^{(c(s)}}(\boldsymbol{e}_{h'}^{m_j} | \cdot), 0 \text{ otherwise.}$$ </span><br /> As proposed in Section <a href="#subsec:hard_seg" data-reference-type="ref" data-reference="subsec:hard_seg">1.4.4</a>, “hard” segments are obtained <em>via</em> a <em>maximum a posteriori</em> operation on the segmentation probability <span class="math inline">\(p_{\boldsymbol{\beta}}\)</span>. The best logistic regression tree is thereafter chosen via the sum of the BIC criteria.</p>
                                                    <p>Although this extension seems straightforward, it is relatively computationally expensive since at each step (s), <span class="math inline">\(\text{K}\) Metropolis-Hastings steps have to be performed and a tree, <span class="math inline">\(\text{K}\) logistic regression and <span class="math inline">\(\text{K} \times d\) polytomous logistic regressions are fitted. With a relatively small number of segments, <em>i.e.</em> 4 to 30 as proposed earlier, it seems nevertheless feasible but it will require more work. In particular, since classification tree methods with more than 2 labels are computationally intensive when presented with categorical features with many levels (see Section <a href="#subsec:direct" data-reference-type="ref" data-reference="subsec:direct">1.2.2</a>), a straightforward workaround is to consider the quantized features as ordered.</p>
                                                    <h2 id="sec:num_exp">Numerical experiments</h2>
                                                    <p>This post is based on more recent work which consequently limits the exhaustiveness of the numerical experiments. The next section aims at comparing the proposed approach to other methods on simulated data from the proposed model, and in particular the failing situations discussed in Section <a href="#subsec:fail" data-reference-type="ref" data-reference="subsec:fail">1.1.3</a>.</p>
                                                    <h3 id="subsec:num_sim">Empirical consistency on simulated data</h3>
                                                    <p>The first set of numerical experiments are dedicated to verifying empirically the consistency of the proposed approach. To do so, we simulate the failing situations presented in Section <a href="#subsec:fail" data-reference-type="ref" data-reference="subsec:fail">1.1.3</a>.</p>
                                                    <ol type="a">
                                                        <li><p>Two covariates <span class="math inline">\((x_1,x_2)\) are independently simulated from an equally probable mixture of <span class="math inline">𝒩(3, 1)</span>, <span class="math inline">𝒩(6, 1)</span> and <span class="math inline">𝒩(9, 1)</span> and the log odd ratio of <span class="math inline"><em>y</em></span> is given by <span class="math inline">\(\theta_0 + \theta_1 x_1 + \theta_2 x_2\) where <span class="math inline"><em>θ</em><sub>0</sub> = 3</span>, <span class="math inline"><em>θ</em><sub>1</sub> = 0.5</span> and <span class="math inline"><em>θ</em><sub>2</sub> =  − 1</span>. This data generating mechanism is illustrated in Figure <a href="#fig:simu_pas" data-reference-type="ref" data-reference="fig:simu_pas">[12]</a>. Results of various clustering methods developed in this post are given in Table <a href="#tab:num_exp_tree_pas" data-reference-type="ref" data-reference="tab:num_exp_tree_pas">[1]</a>.</p></li>
                                                        <li><p>Two covariates <span class="math inline">\((x_1,x_2)\) are simulated from <span class="math inline">𝒰(0, 1)</span> and a third categorical covariate <span class="math inline">\(x_3\) with 6 uniformly drawn levels. For levels <span class="math inline">1</span> and <span class="math inline">2</span> of feature <span class="math inline">\(x_3\), the log odd ratio of <span class="math inline"><em>y</em></span> is given by <span class="math inline">\(\theta_1 x_1 + \theta_2 x_2\) where <span class="math inline"><em>θ</em><sub>1</sub> =  − 1</span> and <span class="math inline"><em>θ</em><sub>2</sub> = 0.5</span>. For levels <span class="math inline">3</span> and <span class="math inline">4</span>, we have <span class="math inline"><em>θ</em><sub>1</sub> =  − 0.5</span> and <span class="math inline"><em>θ</em><sub>2</sub> = 1.5</span> and finally for levels <span class="math inline">5</span> and <span class="math inline">6</span>, we set <span class="math inline"><em>θ</em><sub>1</sub> = 1</span> and <span class="math inline"><em>θ</em><sub>2</sub> =  − 0.5</span>. This data generating mechanism is illustrated in Figure <a href="#fig:simu" data-reference-type="ref" data-reference="fig:simu">[13]</a>. Results of various clustering methods developed in this post are given in Table <a href="#tab:num_exp_tree" data-reference-type="ref" data-reference="tab:num_exp_tree">[2]</a>.</p></li>
                                                    </ol>
                                                    
                                                    <figure>
                                                        <img src="figures/chapitre6/fig_simu_pas.png" alt="" width="100%"/><figcaption><span id="fig:simu_pas" label="fig:simu_pas">[12]</span></figcaption>
                                                    </figure>
                                                    

                                                    <p>For both experiments, the <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm is initialized randomly with <span class="math inline">\(\text{K} = 5\) segments. In experiment (a), the proposed approach selects effectively no partitions. The <em>maximum a posteriori</em> scheme is able, to make segments “vanish” and explore segmentations with less than <span class="math inline">\(\text{K}\) segments. In experiment (b), the proposed approach is able to recover the tree structure. Consequently, the proposed algorithm yields the best performance in both settings. As for <span data-acronym-label="famd" data-acronym-form="singular+short">FAMD</span> and <span data-acronym-label="pls" data-acronym-form="singular+short">PLS</span> which resulting projections for experiment (a) are displayed on Figure <a href="#fig:simu_a_famd" data-reference-type="ref" data-reference="fig:simu_a_famd">[14]</a> and <a href="#fig:simu_a_pls" data-reference-type="ref" data-reference="fig:simu_a_pls">[15]</a> respectively, they form <span class="math inline">3</span> clusters and consequently the <span class="math inline">3</span> resulting logistic regression suffer from a higher estimation variance loosely reflected in their inferior performance in Table <a href="#tab:num_exp_tree_pas" data-reference-type="ref" data-reference="tab:num_exp_tree_pas">[1]</a>. <span data-acronym-label="lmt" data-acronym-form="singular+short">LMT</span> recovers the truth by producing a single logistic regression but not <span data-acronym-label="mob" data-acronym-form="singular+short">MOB</span> (see Figure <a href="#fig:simu_a_mob" data-reference-type="ref" data-reference="fig:simu_a_mob">[17a]</a>) which splits the data into <span class="math inline">2</span> segments. On experiment (b), <span data-acronym-label="famd" data-acronym-form="singular+short">FAMD</span> produces worse results than a single logistic regression and the benefit of using the target <span class="math inline"><em>y</em></span> is clear from the result of <span data-acronym-label="pls" data-acronym-form="singular+short">PLS</span> (see Table <a href="#tab:num_exp_tree" data-reference-type="ref" data-reference="tab:num_exp_tree">[2]</a>). <span data-acronym-label="mob" data-acronym-form="singular+short">MOB</span> also recovers the true structure (see Figure <a href="#fig:simu_b_mob" data-reference-type="ref" data-reference="fig:simu_b_mob">[17b]</a>) but not <span data-acronym-label="lmt" data-acronym-form="singular+short">LMT</span> which first splitting node is a continuous feature not involved in the data generating mechanism of the segments as displayed on Figure <a href="#fig:simu_b_lmt" data-reference-type="ref" data-reference="fig:simu_b_lmt">[16]</a>. For both experiments, it would be useful to report confidence intervals and or bar plots to derive an empirical consistency of the proposed approach.</p>
                                                    
                                                    
                                                    <figure>
                                                        <img src="figures/chapitre6/graph_simu_1.jpg" alt="Representation of y w.r.t. (x_1,x_2) for x_3 \in \{1,2\}." id="fig:simu" /><figcaption>[13a] Representation of <span class="math inline"><em>y</em></span> w.r.t. <span class="math inline">\((x_1,x_2)\) for <span class="math inline">\(x_3 \in \{1,2\}\).<span label="fig:simu1"></span></figcaption>
                                                    </figure>
                                                    <figure>
                                                        <img src="figures/chapitre6/graph_simu_2.jpg" alt="Representation of y w.r.t. (x_1,x_2) for x_3 \in \{3,4\}." id="fig:simu" width="100%"/><figcaption>[13b] Representation of <span class="math inline"><em>y</em></span> w.r.t. <span class="math inline">\((x_1,x_2)\) for <span class="math inline">\(x_3 \in \{3,4\}\).<span label="fig:simu2"></span></figcaption>
                                                    </figure>
                                                    <figure>
                                                        <img src="figures/chapitre6/graph_simu_3.jpg" alt="Representation of y w.r.t. (x_1,x_2) for x_3 \in \{5,6\}." id="fig:simu" width="100%"/><figcaption>[13c] Representation of <span class="math inline"><em>y</em></span> w.r.t. <span class="math inline">\((x_1,x_2)\) for <span class="math inline">\(x_3 \in \{5,6\}\).<span label="fig:simu3"></span></figcaption>
                                                    </figure>
                                                    
                                                    <figure>
                                                        <img src="figures/chapitre6/fig_famd_simu.png" alt="Representation of y w.r.t. (x_1,x_2) for x_3 \in \{5,6\}." id="fig:simu_a_famd" width="100%"/><figcaption>[14] </figcaption>
                                                    </figure>
                                                    
                                                    <figure>
                                                        <img src="figures/chapitre6/fig_pls_simu.png" alt="Representation of y w.r.t. (x_1,x_2) for x_3 \in \{5,6\}." id="fig:simu_a_pls" width="100%"/><figcaption>[15] </figcaption>
                                                    </figure>


                                                    <figure>
                                                        <img id="fig:simu_b_lmt" src="figures/chapitre6/graphLMT.png" alt="" width="100%"/><figcaption><span data-acronym-label="lmt" data-acronym-form="singular+short">[16] LMT</span> tree resulting from simulated data from (b).</figcaption>
                                                    </figure>
                                                    
                                                    <figure>
                                                        <img id="fig:simu_a_mob" src="figures/chapitre6/fig_simu_mob_a.png" alt="" width="100%"/><figcaption><span data-acronym-label="lmt" data-acronym-form="singular+short">[17a] MOB</span> tree resulting from simulated data from (a).</figcaption>
                                                    </figure>
                                                    
                                                    <figure>
                                                        <img id="fig:simu_b_mob" src="figures/chapitre6/fig_simu_mob_b.png" alt="" width="100%"/><figcaption><span data-acronym-label="lmt" data-acronym-form="singular+short">[17b] MOB</span> tree resulting from simulated data from (b).</figcaption>
                                                    </figure>
                                                    
                                                    <table>
                                                        <caption><span id="tab:num_exp_tree_pas" label="tab:num_exp_tree_pas">[1]</span> Comparison of several clustering approaches w.r.t. the subsequent predictive performance in experiment (a).</caption>
                                                        <thead>
                                                            <tr class="header">
                                                                <th style="text-align: left;"></th>
                                                                <th style="text-align: left;">Oracle = ALLR</th>
                                                                <th style="text-align: left;"><em>glmtree</em>-SEM</th>
                                                                <th style="text-align: left;"><span data-acronym-label="famd" data-acronym-form="singular+short">FAMD</span></th>
                                                                <th style="text-align: left;"><span data-acronym-label="pls" data-acronym-form="singular+short">PLS</span></th>
                                                                <th style="text-align: left;"><span data-acronym-label="lmt" data-acronym-form="singular+short">LMT</span></th>
                                                                <th style="text-align: left;"><span data-acronym-label="mob" data-acronym-form="singular+short">MOB</span></th>
                                                            </tr>
                                                        </thead>
                                                        <tbody>
                                                            <tr class="odd">
                                                                <td style="text-align: left;">Gini</td>
                                                                <td style="text-align: left;">69.7</td>
                                                                <td style="text-align: left;"><strong>69.7</strong></td>
                                                                <td style="text-align: left;">65.3</td>
                                                                <td style="text-align: left;">47.0</td>
                                                                <td style="text-align: left;"><strong>69.7</strong></td>
                                                                <td style="text-align: left;">64.8</td>
                                                            </tr>
                                                        </tbody>
                                                    </table>
                                                    
                                                    <table>
                                                        <caption><span id="tab:num_exp_tree" label="tab:num_exp_tree">[2]</span> Comparison of several clustering approaches w.r.t. the subsequent predictive performance in experiment (b).</caption>
                                                        <thead>
                                                            <tr class="header">
                                                                <th style="text-align: left;"></th>
                                                                <th style="text-align: left;">Oracle</th>
                                                                <th style="text-align: left;">ALLR</th>
                                                                <th style="text-align: left;"><em>glmtree</em>-SEM</th>
                                                                <th style="text-align: left;"><span data-acronym-label="famd" data-acronym-form="singular+short">FAMD</span></th>
                                                                <th style="text-align: left;"><span data-acronym-label="pls" data-acronym-form="singular+short">PLS</span></th>
                                                                <th style="text-align: left;"><span data-acronym-label="lmt" data-acronym-form="singular+short">LMT</span></th>
                                                                <th style="text-align: left;"><span data-acronym-label="mob" data-acronym-form="singular+short">MOB</span></th>
                                                            </tr>
                                                        </thead>
                                                        <tbody>
                                                            <tr class="odd">
                                                                <td style="text-align: left;">Gini</td>
                                                                <td style="text-align: left;">69.7</td>
                                                                <td style="text-align: left;">25.8</td>
                                                                <td style="text-align: left;"><strong>69.7</strong></td>
                                                                <td style="text-align: left;">17.7</td>
                                                                <td style="text-align: left;">48.4</td>
                                                                <td style="text-align: left;">65.8</td>
                                                                <td style="text-align: left;"><strong>69.7</strong></td>
                                                            </tr>
                                                        </tbody>
                                                    </table>
                                                    <h3 id="benchmark-on-credit-scoring-data">Benchmark on <em>Credit Scoring</em> data</h3>
                                                    <h4 id="the-running-example-the-automobile-dataset">The running example: the Automobile dataset</h4>
                                                    <p>Recall from Sections <a href="#subsec:adhoc" data-reference-type="ref" data-reference="subsec:adhoc">1.1.2</a> and <a href="#subsec:sup_gen" data-reference-type="ref" data-reference="subsec:sup_gen">1.2.1</a> that <span data-acronym-label="pca" data-acronym-form="singular+short">PCA</span>, <span data-acronym-label="mca" data-acronym-form="singular+short">MCA</span>, <span data-acronym-label="famd" data-acronym-form="singular+short">FAMD</span> and <span data-acronym-label="pls" data-acronym-form="singular+short">PLS</span> revealed no segments on this dataset and from Section <a href="#subsec:direct" data-reference-type="ref" data-reference="subsec:direct">1.2.2</a> that <span data-acronym-label="lmt" data-acronym-form="singular+short">LMT</span> produced disappointing results and <span data-acronym-label="mob" data-acronym-form="singular+short">MOB</span> could not be tested. A logistic regression learnt on 70 % of the sample yields an overall performance of <span class="math inline">57.9</span> Gini points.</p>
                                                    <p>By applying <em>glmtree</em>-SEM to the Automobile dataset, we get <span class="math inline">\(\hat{\text{K}} = 2\) segments defined by the value of the car being over <span class="math inline">10, 000</span> Euros, yielding an overall performance of <span class="math inline">58.7</span> Gini points. This difference might not seem significant, but in the <em>Credit Scoring</em> industry, such small improvements might result in the selection of a few more good applicants (resp. a few less bad applicants) whose car loans are of high amount. It is thus a high stake for financial institutions.</p>
                                                    <h4 id="one-year-of-financed-applications">One year of financed applications</h4>
                                                    <p>A subset of all applications, representative of approx. <span class="math inline">30</span> portfolios with different scorecards, has been extracted for the purpose of the present benchmark with <span class="math inline"><em>n</em> = 900, 000</span> observations and <span class="math inline"><em>d</em> = 18</span> among which <span class="math inline">12</span> continuous features and <span class="math inline">6</span> categorical features with <span class="math inline">6</span> to <span class="math inline">100</span> levels (most features are similar to the Automobile dataset). The missing values have been preprocessed such that no continuous features have missing values and the categorical features have a separate and meaningful “missing” level. Also for simplification purposes, no quantization or interaction screening is performed so that the <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm is conducted as presented in Section <a href="#subsec:sem" data-reference-type="ref" data-reference="subsec:sem">1.4.3</a>.</p>
                                                    <p>Generative approaches (<span data-acronym-label="famd" data-acronym-form="singular+short">FAMD</span> and <span data-acronym-label="pls" data-acronym-form="singular+short">PLS</span>) are not used due to their subjectivity (visual separation) and the fact that they are used by practitioners to provide “local” segments (<em>e.g.</em> for the Automobile market). Hence for such a large dataset, they would have to be applied “recursively” (applying <span data-acronym-label="famd" data-acronym-form="singular+short">FAMD</span> / <span data-acronym-label="pls" data-acronym-form="singular+short">PLS</span> on each of the resulting visually separated segments). For computational reasons that became apparent in applying <span data-acronym-label="lmt" data-acronym-form="singular+short">LMT</span> and <span data-acronym-label="mob" data-acronym-form="singular+short">MOB</span> to the Automobile dataset, these methods cannot cope either with this larger dataset.</p>
                                                    <p>Consequently, <em>glmtree</em>-SEM is only compared to the current performance. The combined scorecards have an overall performance of approximately 46 Gini points but they are not on the same “scale” since they were developed at different times. I rely on the Platt scaling method developed in <a href="#platt1999probabilistic" class="citation" data-cites="platt1999probabilistic">[17]</a> and <a href="#zadrozny2002transforming" class="citation" data-cites="zadrozny2002transforming">[18]</a> and used in common <em>machine learning</em> libraries such as Scikit-learn, to put all of them on the same scale by fitting a logistic regression between the observed labels <span class="math inline">\(\boldsymbol{\mathbf{y}}\) and the scores outputted by each scorecard. After this procedure, overall performance jumps to approximately 54.6 Gini points.</p>
                                                    <p>Another possible benchmark with the current segmentation is to learn additive linear logistic regression (hereafter ALLR) for each existing segment. This approach leads to an overall Gini fo 52. As our proposed algorithm <em>glmtree</em>-SEM will be applied without quantization or interaction screening, this result is a “fairer” baseline than the preceding one since they are both in the same model space: additive linear logistic regression trees.</p>
                                                    <p>The <em>glmtree</em>-SEM applied to this big dataset yields only <span class="math inline">\(\hat{\text{K}} = 12\) segments for an overall performance of 50.2 Gini points. This is satisfactory in two aspects: the performance is relatively close to the existing segmentation while using 3 times less segments / logistic regressions. We can hope for even better interpretability and performance by performing quantization and interaction screening. Results are summarized in Table <a href="#tab:res_tot_trees" data-reference-type="ref" data-reference="tab:res_tot_trees">[3]</a>.</p>
                                                    
                                                    <table>
                                                        <caption><span id="tab:res_tot_trees" label="tab:res_tot_trees">[3]</span> Comparison of the existing segmentation and the proposed approach <em>glmtree</em>-SEM.</caption>
                                                        <tbody>
                                                            <tr class="odd">
                                                                <td style="text-align: left;">Current performance <em>via</em> Platt scaling</td>
                                                                <td style="text-align: left;"><em>glmtree</em>-SEM; K=10</td>
                                                                <td style="text-align: left;">ALLR</td>
                                                            </tr>
                                                            <tr class="even">
                                                                <td style="text-align: left;">54.6</td>
                                                                <td style="text-align: left;">52.0</td>
                                                                <td style="text-align: left;">50.2</td>
                                                            </tr>
                                                        </tbody>
                                                    </table>
                                                    <h2 id="conclusion">Conclusion</h2>
                                                    <p>This post aimed at formalizing an old problem in <em>Credit Scoring</em>, providing a literature review as well as a research idea for future work. As is often the case, practitioners have had good intuitions to deal with practical and theoretical requirements, such as performing clustering techniques, choosing segments empirically from the resulting visualization and fitting logistic regression on these.</p>
                                                    <p>However, situations can easily be imagined where such practices can fail, which is why other existing methods, that take into account the predictive task, were exemplified. Nevertheless, as in the best case scenario, practitioners would like to have an all-in-one tool that works with missing values and eventually performs quantization and interaction screening while guaranteeing the best predictive performance by embedding the learning of a segmentation in the predictive task of learning its logistic regression, a new method is proposed, based on an <span data-acronym-label="sem" data-acronym-form="singular+short">SEM</span> algorithm, that was adapted to be usable with the <em>glmdisc</em> method developed in earlier posts.</p>
                                                    <p>On simulated data, it shows very promising results that aims at demonstrating the consistency of the approach. On real data from <span data-acronym-label="CACF" data-acronym-form="singular+short">CACF</span>, other methods yielded disappointing results while <em>glmtree</em>-SEM was able to compete with the current performance which required months of manual adjustments. By adding the quantization and interaction screening ability to this algorithm, as described in Section <a href="#sec:adding_quant" data-reference-type="ref" data-reference="sec:adding_quant">1.5</a>, we could easily imagine beating this <em>ad hoc</em> segmentation by a significant margin.</p>
                                                    
                                                    <div id="HCB_comment_box"><a href="http://www.htmlcommentbox.com">Comment Box</a> is loading comments...</div>
                                                    <link rel="stylesheet" type="text/css" href="//www.htmlcommentbox.com/static/skins/bootstrap/twitter-bootstrap.css?v=0" />
                                                    <script type="text/javascript" id="hcb"> /*<!--*/ if(!window.hcb_user){hcb_user={};} (function(){var s=document.createElement("script"), l=hcb_user.PAGE || (""+window.location).replace(/'/g,"%27"), h="//www.htmlcommentbox.com";s.setAttribute("type","text/javascript");s.setAttribute("src", h+"/jread?page="+encodeURIComponent(l).replace("+","%2B")+"&mod=%241%24wq1rdBcg%247122Rf3pq5RdU5hlkjJ4L1"+"&opts=16862&num=50&ts=1486459652719");if (typeof s!="undefined") document.getElementsByTagName("head")[0].appendChild(s);})(); /*-->*/ </script>
                                                    <!-- end www.htmlcommentbox.com -->
                                                    </div>

                                                    
                                                    
                                                    <section class="footnotes">
                                                        <hr />
                                                        <ol>
                                                            <li id="fn1"><p>©2009 IEEE<a href="#fnref1" class="footnote-back">↩</a></p></li>
                                                        </ol>
                                                    </section>
                                                    
                                                
<table>
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="jordan1994hierarchical">1</a>]
        </td>
        <td class="bibtexitem">
            Michael&nbsp;I Jordan and Robert&nbsp;A Jacobs.
            Hierarchical mixtures of experts and the em algorithm.
            <em>Neural computation</em>, 6(2):181-214, 1994.
            [&nbsp;<a href="chapitre6_bib.html#jordan1994hierarchical">bib</a>&nbsp;]
            
        </td>
    </tr>
    
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="opitz1999popular">2</a>]
        </td>
        <td class="bibtexitem">
            David Opitz and Richard Maclin.
            Popular ensemble methods: An empirical study.
            <em>Journal of artificial intelligence research</em>, 11:169-198, 1999.
            [&nbsp;<a href="chapitre6_bib.html#opitz1999popular">bib</a>&nbsp;]
            
        </td>
    </tr>
    
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="pages2014multiple">3</a>]
        </td>
        <td class="bibtexitem">
            J&eacute;r&ocirc;me Pag&egrave;s.
            <em>Multiple factor analysis by example using R</em>.
            Chapman and Hall/CRC, 2014.
            [&nbsp;<a href="chapitre6_bib.html#pages2014multiple">bib</a>&nbsp;]
            
        </td>
    </tr>
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="JSSv025i01">4</a>]
        </td>
        <td class="bibtexitem">
            Sébastien Lê, Julie Josse, and François Husson.
            Factominer: An r package for multivariate analysis.
            <em>Journal of Statistical Software, Articles</em>, 25(1):1-18, 2008.
            [&nbsp;<a href="chapitre6_bib.html#JSSv025i01">bib</a>&nbsp;|
            <a href="http://dx.doi.org/10.18637/jss.v025.i01">DOI</a>&nbsp;|
            <a href="https://www.jstatsoft.org/v025/i01">http</a>&nbsp;]
            <blockquote><font size="-1">
                In this article, we present FactoMineR an R package dedicated to multivariate data analysis. The main features of this package is the possibility to take into account different types of variables (quantitative or categorical), different types of structure on the data (a partition on the variables, a hierarchy on the variables, a partition on the individuals) and finally supplementary information (supplementary individuals and variables). Moreover, the dimensions issued from the different exploratory data analyses can be automatically described by quantitative and/or categorical variables. Numerous graphics are also available with various options. Finally, a graphical user interface is implemented within the Rcmdr environment in order to propose an user friendly package.
            </font></blockquote>
            <p><blockquote><font size="-1">
                Keywords:
            </font></blockquote>
            
        </td>
    </tr>
    
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="lebart1995statistique">5</a>]
        </td>
        <td class="bibtexitem">
            Ludovic Lebart, Alain Morineau, and Marie Piron.
            <em>Statistique exploratoire multidimensionnelle</em>, volume&nbsp;3.
            Dunod Paris, 1995.
            [&nbsp;<a href="chapitre6_bib.html#lebart1995statistique">bib</a>&nbsp;]
            
        </td>
    </tr>
    
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="wold1984collinearity">6</a>]
        </td>
        <td class="bibtexitem">
            Svante Wold, Arnold Ruhe, Herman Wold, and WJ&nbsp;Dunn, III.
            The collinearity problem in linear regression. the partial least
            squares (pls) approach to generalized inverses.
            <em>SIAM Journal on Scientific and Statistical Computing</em>,
            5(3):735-743, 1984.
            [&nbsp;<a href="chapitre6_bib.html#wold1984collinearity">bib</a>&nbsp;]
            
        </td>
    </tr>
    
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="schwartz2009human">7</a>]
        </td>
        <td class="bibtexitem">
            William&nbsp;Robson Schwartz, Aniruddha Kembhavi, David Harwood, and Larry&nbsp;S Davis.
            Human detection using partial least squares analysis.
            In <em>2009 IEEE 12th International Conference on Computer Vision
                (ICCV)</em>, pages 24-31. IEEE, 2009.
            [&nbsp;<a href="chapitre6_bib.html#schwartz2009human">bib</a>&nbsp;]
            
        </td>
    </tr>
    
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="bair2006prediction">8</a>]
        </td>
        <td class="bibtexitem">
            Eric Bair, Trevor Hastie, Debashis Paul, and Robert Tibshirani.
            Prediction by supervised principal components.
            <em>Journal of the American Statistical Association</em>,
            101(473):119-137, 2006.
            [&nbsp;<a href="chapitre6_bib.html#bair2006prediction">bib</a>&nbsp;]
            
        </td>
    </tr>
    
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="chan2004lotus">9</a>]
        </td>
        <td class="bibtexitem">
            Kin-Yee Chan and Wei-Yin Loh.
            Lotus: An algorithm for building accurate and comprehensible logistic
            regression trees.
            <em>Journal of Computational and Graphical Statistics</em>,
            13(4):826-852, 2004.
            [&nbsp;<a href="chapitre6_bib.html#chan2004lotus">bib</a>&nbsp;]
            
        </td>
    </tr>
    
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="cart84">10</a>]
        </td>
        <td class="bibtexitem">
            Leo Breiman, J.&nbsp;H. Friedman, R.&nbsp;A. Olshen, and C.&nbsp;J. Stone.
            <em>Classification and Regression Trees</em>.
            Statistics/Probability Series. Wadsworth Publishing Company, Belmont,
            California, U.S.A., 1984.
            [&nbsp;<a href="chapitre6_bib.html#cart84">bib</a>&nbsp;]
            
        </td>
    </tr>
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="landwehr2005logistic">11</a>]
        </td>
        <td class="bibtexitem">
            Niels Landwehr, Mark Hall, and Eibe Frank.
            Logistic model trees.
            <em>Machine learning</em>, 59(1-2):161-205, 2005.
            [&nbsp;<a href="chapitre6_bib.html#landwehr2005logistic">bib</a>&nbsp;]
            
        </td>
    </tr>
    
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="friedman2000additive">12</a>]
        </td>
        <td class="bibtexitem">
            Jerome Friedman, Trevor Hastie, Robert Tibshirani, et&nbsp;al.
            Additive logistic regression: a statistical view of boosting (with
            discussion and a rejoinder by the authors).
            <em>The annals of statistics</em>, 28(2):337-407, 2000.
            [&nbsp;<a href="chapitre6_bib.html#friedman2000additive">bib</a>&nbsp;]
            
        </td>
    </tr>
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="quinlan2014c4">13</a>]
        </td>
        <td class="bibtexitem">
            J&nbsp;Ross Quinlan.
            <em>C4. 5: programs for machine learning</em>.
            Elsevier, 2014.
            [&nbsp;<a href="chapitre6_bib.html#quinlan2014c4">bib</a>&nbsp;]
            
        </td>
    </tr>
    
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="sumner2005speeding">14</a>]
        </td>
        <td class="bibtexitem">
            Marc Sumner, Eibe Frank, and Mark Hall.
            Speeding up logistic model tree induction.
            In <em>European Conference on Principles of Data Mining and
                Knowledge Discovery</em>, pages 675-683. Springer, 2005.
            [&nbsp;<a href="chapitre6_bib.html#sumner2005speeding">bib</a>&nbsp;]
            
        </td>
    </tr>
    
    
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="zeileis2008model">15</a>]
        </td>
        <td class="bibtexitem">
            Achim Zeileis, Torsten Hothorn, and Kurt Hornik.
            Model-based recursive partitioning.
            <em>Journal of Computational and Graphical Statistics</em>,
            17(2):492-514, 2008.
            [&nbsp;<a href="chapitre6_bib.html#zeileis2008model">bib</a>&nbsp;]
            
        </td>
    </tr>
    
    
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="hothorn2006unbiased">16</a>]
        </td>
        <td class="bibtexitem">
            Torsten Hothorn, Kurt Hornik, and Achim Zeileis.
            Unbiased recursive partitioning: A conditional inference framework.
            <em>Journal of Computational and Graphical statistics</em>,
            15(3):651-674, 2006.
            [&nbsp;<a href="chapitre6_bib.html#hothorn2006unbiased">bib</a>&nbsp;]
            
        </td>
    </tr>
    
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="platt1999probabilistic">17</a>]
        </td>
        <td class="bibtexitem">
            John Platt et&nbsp;al.
            Probabilistic outputs for support vector machines and comparisons to
            regularized likelihood methods.
            <em>Advances in large margin classifiers</em>, 10(3):61-74, 1999.
            [&nbsp;<a href="chapitre6_bib.html#platt1999probabilistic">bib</a>&nbsp;]
            
        </td>
    </tr>
    
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="zadrozny2002transforming">18</a>]
        </td>
        <td class="bibtexitem">
            Bianca Zadrozny and Charles Elkan.
            Transforming classifier scores into accurate multiclass probability
            estimates.
            In <em>Proceedings of the eighth ACM SIGKDD international conference
                on Knowledge discovery and data mining</em>, pages 694-699. ACM, 2002.
            [&nbsp;<a href="chapitre6_bib.html#zadrozny2002transforming">bib</a>&nbsp;]
            
        </td>
    </tr>
    
    
    <tr valign="top">
        <td align="right" class="bibtexnumber">
            [<a name="friedman2001elements">19</a>]
        </td>
        <td class="bibtexitem">
Hastie, T., Tibshirani, R., Friedman, J., & Franklin, J.
The elements of statistical learning: data mining, inference and prediction
            In <em>The Mathematical Intelligencer</em>, 2005.
        </td>
    </tr>

    <tr valign="top">
    <td align="right" class="bibtexnumber">
    [<a name="dempster1977maximum">20</a>]
    </td>
    <td class="bibtexitem">
Dempster, A. P., Laird, N. M., & Rubin, D. B.
Maximum likelihood from incomplete data via the EM algorithm.
In <em>Journal of the Royal Statistical Society: Series B (Methodological)</em>, 39(1), 1-22. 1977.
    </td>
    </tr>

</table>


								</div>
							</section>

							<section id="footer">
						<div class="container">
							<ul class="copyright">
								<li>&copy; Adrien Ehrhardt. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollzer.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>
